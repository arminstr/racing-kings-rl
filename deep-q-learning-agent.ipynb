{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning - Deep Q-Learning for Racing Kings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "!pip install numpy\n",
    "!pip install tensorflow==2.3.0\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install chess"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (1.18.5)\n",
      "Requirement already satisfied: tensorflow==2.3.0 in ./env/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.39.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.5.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.17.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (44.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: keras in ./env/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in ./env/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./env/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.8/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in ./env/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from h5py->keras) (1.16.0)\n",
      "Requirement already satisfied: keras-rl2 in ./env/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in ./env/lib/python3.8/site-packages (from keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.36.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.39.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.3.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.13.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.17.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (44.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./env/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.1.1)\n",
      "Requirement already satisfied: chess in ./env/lib/python3.8/site-packages (1.6.1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from racing_kings_env_armin import RacingKingsEnvironment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Test of the Evironment\n",
    "To ensure proper executability of the environment a few episodes of gameplay are executed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "episodes = 10\n",
    "env = RacingKingsEnvironment()\n",
    "for episode in range(1, episodes+1):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    sum_of_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, information = env.step(action)\n",
    "        sum_of_rewards+=reward\n",
    "    print('Episode:{} Sum of Rewards:{} Information:{}'.format(episode, sum_of_rewards, information))\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:2 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:3 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:4 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:5 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:6 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:7 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:8 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:9 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "Episode:10 Sum of Rewards:0 Information:{'msg': 'Action is not a valid move'}\n",
      "closing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Convolutional Neural Network (CNN)\n",
    "For better compatibility with the task of Racing Kings a Custom CNN is created using Keras.\n",
    "The kernel Size (convolution window size), is chosen according to the publication that was given with the task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    # Init function for the Custom CNN Model\n",
    "    def __init__(   self, \n",
    "                    shape_states, \n",
    "                    shape_actions ):\n",
    "        # Call the inherited init function\n",
    "        super(CustomModel, self).__init__()\n",
    "        # define input layer\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=shape_states)\n",
    "        # create array for hidden layers\n",
    "        self.hidden_layers = []\n",
    "        # init the hidden convolutional layers\n",
    "        for layer_template in [256, 256]:\n",
    "            self.hidden_layers.append( \n",
    "                tf.keras.layers.Conv2D( layer_template, \n",
    "                                        kernel_size=(3,3), \n",
    "                                        activation='relu', \n",
    "                                        kernel_initializer='RandomNormal'))\n",
    "        # flatten layer for compatibility with Dense output of shape_actions size\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        # output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "                shape_actions, activation='linear', kernel_initializer='RandomNormal')\n",
    "    # tensorflow compatible function used to assemble the model, outputting the model\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        flatten = self.flatten_layer(z)\n",
    "        output = self.output_layer(flatten)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN class\n",
    "For easier creation of Target and Training model a custom class is created. It stores the hyperparameter gamma which represents the weight of future rewards."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(   self, \n",
    "                    shape_states, \n",
    "                    shape_actions, \n",
    "                    gamma, \n",
    "                    max_number_experiences, \n",
    "                    min_number_experiences, \n",
    "                    batch_size, \n",
    "                    lr ):\n",
    "        self.shape_actions = shape_actions\n",
    "        self.shape_states = shape_states\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = CustomModel(shape_states, shape_actions)\n",
    "        self.experience = { 'states': [], \n",
    "                            'actions': [], \n",
    "                            'rewards': [], \n",
    "                            'states_next': [], \n",
    "                            'done': []}\n",
    "        self.max_number_experiences = max_number_experiences\n",
    "        self.min_number_experiences = min_number_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if inputs.shape == self.shape_states:\n",
    "            inputs = np.expand_dims(inputs, axis = 0)\n",
    "        prediction = self.model(inputs.astype('float32'))\n",
    "        return prediction\n",
    "\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['states']) < self.min_number_experiences:\n",
    "            return 0\n",
    "        # randomly chooses an integer in range 0 to num experiences states, with batchsize samples\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['states']), size=self.batch_size)\n",
    "        # next copy batch size experiences to according array\n",
    "        states = np.asarray([ self.experience['states'][i] for i in ids ])\n",
    "        actions = np.asarray([ self.experience['actions'][i] for i in ids ])\n",
    "        rewards = np.asarray([ self.experience['rewards'][i] for i in ids ])\n",
    "        states_next = np.asarray([ self.experience['states_next'][i] for i in ids ])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        # predicts the next values based on the next states\n",
    "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
    "        \n",
    "        # gets the reward where done is true \n",
    "        # and the reward + self.gamma * predicted_value) where done is false\n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "        \n",
    "        # select action avlues based on the predictions for the states and actions\n",
    "        with tf.GradientTape() as tensorflow_gradient_tape:\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states) * tf.one_hot(actions, self.shape_actions), axis=1)\n",
    "            # calculates the loss based on a reduced mean square\n",
    "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        # then calculate and apply the gradients\n",
    "        gradients = tensorflow_gradient_tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    # does an prediction or pics a random action based on epsilon (exploration)\n",
    "    def get_action(self, states, epsilon):\n",
    "        # gets a random action if the random value between 0 and 1 is smaller exploration value epsilon\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.shape_actions)\n",
    "        # do a normal prediction \n",
    "        else:\n",
    "            return np.argmax(self.predict(states))\n",
    "\n",
    "    # adds an experience to the buffer\n",
    "    def add_experience(self, experience_to_add):\n",
    "        # if the array is to long pop one elemnt for each \n",
    "        if len(self.experience['states']) >= self.max_number_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        # append the items of the experience\n",
    "        for key, value in experience_to_add.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    # copies the trainable variables from one network to this network\n",
    "    def copy_trainable_variables(self, network_to_copy_from):\n",
    "        own_trainable_vars = self.model.trainable_variables\n",
    "        other_trainable_vars = network_to_copy_from.model.trainable_variables\n",
    "        for vars1, vars2 in zip(own_trainable_vars, other_trainable_vars):\n",
    "            vars1.assign(vars2.numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# gameplay for the training loop\n",
    "def play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "        e = {'states': prev_observations, 'actions': action, 'rewards': reward, 'states_next': observations, 'done': done}\n",
    "        TrainNet.add_experience(e)\n",
    "        loss = TrainNet.train( TargetNet )\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_trainable_variables( TrainNet )\n",
    "    return rewards, mean(losses), iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# this training loop is explained in the documentation\n",
    "env = RacingKingsEnvironment()\n",
    "gamma = 0.99\n",
    "copy_step = 5\n",
    "max_number_of_experiences = 10000\n",
    "min_number_of_experiences = 100\n",
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'Training/Logs/CustomDQN-' + current_time + '-min_epsilon-025-decay-1e-5_no-neg-reward'\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "TrainNet = DeepQNetwork(    env.state_shape, \n",
    "                            env.action_shape, \n",
    "                            gamma, \n",
    "                            max_number_of_experiences, \n",
    "                            min_number_of_experiences, \n",
    "                            batch_size, \n",
    "                            lr)\n",
    "TargetNet = DeepQNetwork(   env.state_shape, \n",
    "                            env.action_shape, \n",
    "                            gamma, \n",
    "                            max_number_of_experiences, \n",
    "                            min_number_of_experiences, \n",
    "                            batch_size, \n",
    "                            lr)\n",
    "\n",
    "N = 10000\n",
    "StorageInterval = 500\n",
    "total_episode_rewards = np.empty(StorageInterval)\n",
    "total_episode_lengths = np.empty(StorageInterval)\n",
    "total_episode_losses = np.empty(StorageInterval)\n",
    "# starting epsilon: at the beginning of training 99 % of randomness are allowed\n",
    "epsilon = 0.99\n",
    "# sets the speed epsilon decreases to min epsilon\n",
    "decay = 1 - 1e-5\n",
    "# sets the end amount of randomness encounterd by to model on long term training to 1 %\n",
    "min_epsilon = 0.25\n",
    "for n in range(N):\n",
    "    epsilon = epsilon * decay\n",
    "    if epsilon < min_epsilon:\n",
    "        epsilon = min_epsilon\n",
    "    \n",
    "    reward, loss, step = play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "    total_episode_rewards[n%StorageInterval] = reward\n",
    "    total_episode_lengths[n%StorageInterval] = step\n",
    "    total_episode_losses[n%StorageInterval] = loss\n",
    "    if n % StorageInterval == 0 and n != 0:\n",
    "        avg_episode_rewards = np.mean(total_episode_rewards)\n",
    "        avg_episode_length = np.mean(total_episode_lengths)\n",
    "        avg_episode_losses = np.mean(total_episode_losses)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('mean_ep_length', avg_episode_length, step=n)\n",
    "            tf.summary.scalar('mean_reward', avg_episode_rewards, step=n)\n",
    "            tf.summary.scalar('loss', avg_episode_losses, step=n)\n",
    "            tf.summary.scalar('epsilon', epsilon, step=n)\n",
    "        print('episode:{} epsilon:{:.3} mean_reward:{:.3} mean_ep_length:{:.3} loss:{:.3}'\n",
    "                .format(n, float(epsilon), float(avg_episode_rewards), float(avg_episode_length), float(avg_episode_losses)))\n",
    "        total_episode_rewards = np.empty(StorageInterval)\n",
    "        total_episode_lengths = np.empty(StorageInterval)\n",
    "        total_episode_losses = np.empty(StorageInterval)\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Play some games with the freshly trained Model. You can load other models with the commands below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    # obs = env.reset()\n",
    "    env.board.reset()\n",
    "    obs, _, _, _ = env.step(None)\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode=\"human\")\n",
    "        action = TrainNet.get_action(obs, 0)\n",
    "        print(\">>{}\".format(env.action_index_to_uci(action)))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save and Reload the Model Weights\n",
    "\n",
    "If you want to try loading and executing a model, the 250k model with the relative path ***train_250k_net-025-1e-5_stp_rwd-0.h5f*** works quite well. And is also mentioned in the documentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TrainNet.model.save_weights('Training/SavedModels/train_10k_net-025-1e-5_stp_rwd-0.h5f', overwrite=True)\n",
    "TargetNet.model.save_weights('Training/SavedModels/target_10k_net-025-1e-5_stp_rwd-0.h5f', overwrite=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delete the current models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del TrainNet.model\n",
    "del TargetNet.model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "load the model from memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TrainNet.model = CustomModel(TrainNet.shape_states, TrainNet.shape_actions)\n",
    "TrainNet.model.load_weights('Training/SavedModels/train_250k_net-025-1e-5_stp_rwd-0.h5f')\n",
    "\n",
    "# comment out these lines for 250k since only train net was saved\n",
    "# TargetNet.model = CustomModel(TargetNet.shape_states, TargetNet.shape_actions)\n",
    "# TargetNet.model.load_weights('Training/SavedModels/target_250k_net-025-1e-5_stp_rwd-0.h5f')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Play Game Human vs AI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create environment from class RacingKings\n",
    "env = RacingKingsEnvironment()\n",
    "# Define necessary variables\n",
    "obs = None\n",
    "done = False\n",
    "score = 0\n",
    "# Reset chess board to ensure fresh start\n",
    "env.board.reset()    \n",
    "# Start game loop\n",
    "while not done:\n",
    "    #Activate render mode \n",
    "    env.render()\n",
    "    # Check if turn is white or black (Human plays white)\n",
    "    if env.who(env.board.turn) == 'White':\n",
    "        # Print out formatted list of possible moves at each step as a reminder for human player\n",
    "        print('Gueltige Züge:')\n",
    "        legal_moves = list(env.board.legal_moves)\n",
    "        legal_moves = map(lambda move: move.uci(), legal_moves)\n",
    "        print(*legal_moves, sep = \", \")\n",
    "        # Ask for human step via input of UCI move     \n",
    "        action = env.action_uci_to_index(input ('Ihr nächster Zug:'))\n",
    "        # Make step (Parameter 'True' to trigger game functionality in environment)\n",
    "        obs, reward, done, info = env.step(action,True)\n",
    "        score+=reward\n",
    "        \n",
    "    else:\n",
    "        # Create AI step using predict function\n",
    "        action = model.predict(obs)\n",
    "        # Make step (position 0 because predict function returns tuple)\n",
    "        obs, reward, done, info = env.step(action[0],True)\n",
    "        score+=reward\n",
    "# End game loop\n",
    "\n",
    "# Render last state of chess board after game ends        \n",
    "env.render()\n",
    "# Print out game info\n",
    "print('Score:{} Info:{}'.format(score, info))\n",
    "# Closing environment\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea16cf9687884ff056c50def91fea02e8f072861cbc27c56850047f3dd11ae6d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}