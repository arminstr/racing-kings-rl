{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model based Racing Kings Reinforcement Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install chess\n",
    "!pip install gym"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: chess in ./env/lib/python3.8/site-packages (1.5.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: gym in ./env/lib/python3.8/site-packages (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in ./env/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in ./env/lib/python3.8/site-packages (from gym) (8.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in ./env/lib/python3.8/site-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.8/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in ./env/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "from statistics import mean\n",
    "sys.path.append('/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/')\n",
    "from racing_kings_env import RacingKingsEnvironment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "episodes = 10\n",
    "env = RacingKingsEnvironment()\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:2 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:3 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:4 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:5 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:6 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:7 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:8 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:9 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "closing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the RL-Model\n",
    "\n",
    "Reference for the code below:\n",
    "https://github.com/VXU1230/Medium-Tutorials/blob/master/dqn/cart_pole.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, shape_states, hidden_layers_template, shape_actions):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=shape_states)\n",
    "        self.hidden_layers = []\n",
    "        for hlt in hidden_layers_template:\n",
    "            self.hidden_layers.append(tf.keras.layers.Conv2D(\n",
    "                hlt, kernel_size=(3,3), activation='relu', kernel_initializer='RandomNormal'))\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "                shape_actions, activation='linear', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        flatten = self.flatten_layer(z)\n",
    "        output = self.output_layer(flatten)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class DQN:\n",
    "    def __init__(self, shape_states, shape_actions, hidden_layers_template, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.shape_actions = shape_actions\n",
    "        self.shape_states = shape_states\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = CustomModel(shape_states, hidden_layers_template, shape_actions)\n",
    "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if inputs.shape == self.shape_states:\n",
    "            inputs = np.expand_dims(inputs, axis = 0)\n",
    "        prediction = self.model(inputs.astype('float32'))\n",
    "        return prediction\n",
    "\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "            return 0\n",
    "        # chooses an random integer in range low to high, with batch_size samples\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
    "        # sets states to an array of batch_size random experiences from the experience array\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        # same for actions\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        # same for rewards\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        # same for next states\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        # same for dones\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        # predicts the next values based on the next states\n",
    "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
    "        # gets the reward where done is true \n",
    "        # and the reward * self.gamma * predicted_value) where done is false\n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states) * tf.one_hot(actions, self.shape_actions), axis=1)\n",
    "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.shape_actions)\n",
    "        else:\n",
    "            return np.argmax(self.predict(states))\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss = TrainNet.train(TargetNet)\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, mean(losses), iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "env = RacingKingsEnvironment()\n",
    "gamma = 0.99\n",
    "copy_step = 25\n",
    "hidden_units = [256, 256]\n",
    "max_experiences = 10000\n",
    "min_experiences = 100\n",
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'Training/Logs/CustomDQN-' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "TrainNet = DQN(env.state_shape, env.action_shape, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(env.state_shape, env.action_shape, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "\n",
    "N = 100000\n",
    "total_episode_rewards = np.empty(N)\n",
    "total_episode_lengths = np.empty(N)\n",
    "total_episode_losses = np.empty(N)\n",
    "# starting epsilon: at the beginning of training 99 % of randomness are allowed\n",
    "epsilon = 0.99\n",
    "# sets the speed epsilon decreases to min epsilon\n",
    "decay = 1 - 1e-4\n",
    "# sets the end amount of randomness encounterd by to model on long term training to 1 %\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = epsilon * decay\n",
    "    if epsilon < min_epsilon:\n",
    "        epsilon = min_epsilon\n",
    "    \n",
    "    reward, loss, step = play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "    total_episode_rewards[n] = reward\n",
    "    total_episode_lengths[n] = step\n",
    "    total_episode_losses[n] = loss\n",
    "    if n % 100 == 0:\n",
    "        avg_episode_rewards = total_episode_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        avg_episode_length = total_episode_lengths[max(0, n - 100):(n + 1)].mean()\n",
    "        avg_episode_losses = total_episode_losses[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('avg episode length(100)', avg_episode_length, step=n)\n",
    "            tf.summary.scalar('avg reward(100)', avg_episode_rewards, step=n)\n",
    "            tf.summary.scalar('average loss)', avg_episode_losses, step=n)\n",
    "        print('episode:{} epsilon:{:.3} avg_reward(100):{:.3} avg_length(100):{:.3} avg_losses(100):{:.3}'\n",
    "                .format(n, float(epsilon), float(avg_episode_rewards), float(avg_episode_length), float(avg_episode_losses)))\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode:0 epsilon:0.989 avg_reward(100):-1.0 avg_length(100):1.0 avg_losses(100):0.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0a5ced698f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_racing_kings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtotal_episode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtotal_episode_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-122154c0b762>\u001b[0m in \u001b[0;36mplay_racing_kings\u001b[0;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprev_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/racing_kings_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# reward, done, info can't be included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/racing_kings_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# reset the board\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/racing_kings_env.py\u001b[0m in \u001b[0;36mstates\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHITE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_moves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0msquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard_square_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0midNum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages/chess/variant.py\u001b[0m in \u001b[0;36mgenerate_legal_moves\u001b[0;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBB_ALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBB_ALL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgives_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages/chess/__init__.py\u001b[0m in \u001b[0;36mgenerate_legal_moves\u001b[0;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[1;32m   3502\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3504\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_pseudo_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3505\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages/chess/__init__.py\u001b[0m in \u001b[0;36mgenerate_pseudo_legal_moves\u001b[0;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0mmoves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattacks_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_square\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mour_pieces\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mto_square\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscan_reversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_square\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;31m# Generate castling moves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages/chess/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, from_square, to_square, promotion, drop)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = TrainNet.get_action(env.states, 1)\n",
    "        print(\">>{}\".format(env.action_index_to_uci(action)))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>f5a8\n",
      "Episode:1 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>e7e4\n",
      "Episode:2 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>h7f1\n",
      "Episode:3 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>g7e5\n",
      "Episode:4 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>g8g2\n",
      "Episode:5 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>c1a5\n",
      "Episode:6 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>f6a7\n",
      "Episode:7 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>e4g8\n",
      "Episode:8 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>e8f3\n",
      "Episode:9 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      ">>c6c7\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Model Weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TrainNet.model.save_weights('Training/SavedModels/train_net.h5f', overwrite=True)\n",
    "TargetNet.model.save_weights('Training/SavedModels/target_net.h5f', overwrite=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "del TrainNet.model\n",
    "del TargetNet.model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "TrainNet.model = CustomModel(TrainNet.shape_states, [256, 256], TrainNet.shape_actions)\n",
    "TrainNet.model.load_weights('Training/SavedModels/train_net.h5f')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x142169a90>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "636cf5762cc7d752356f6c503ef4faeef5e3c22281f38effb63ed56f12463915"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}