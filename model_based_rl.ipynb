{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model based Racing Kings Reinforcement Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install numpy\n",
    "!pip install tensorflow==2.3.0\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install chess"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (1.18.5)\n",
      "Requirement already satisfied: tensorflow==2.3.0 in ./env/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.5.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.17.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.39.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.33.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (44.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.26.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./env/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: keras in ./env/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in ./env/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.8/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./env/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in ./env/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from h5py->keras) (1.16.0)\n",
      "Requirement already satisfied: keras-rl2 in ./env/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in ./env/lib/python3.8/site-packages (from keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.17.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.39.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.4.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.18.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.33.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (44.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.26.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.26.6)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./env/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.1.1)\n",
      "Requirement already satisfied: chess in ./env/lib/python3.8/site-packages (1.6.1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from racing_kings_env import RacingKingsEnvironment"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-27 16:36:31.520591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "episodes = 10\n",
    "env = RacingKingsEnvironment()\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:2 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:3 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:4 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:5 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:6 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:7 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:8 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:9 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "closing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the RL-Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, shape_states, hidden_layers_template, shape_actions):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=shape_states)\n",
    "        self.hidden_layers = []\n",
    "        for hlt in hidden_layers_template:\n",
    "            self.hidden_layers.append(tf.keras.layers.Conv2D(\n",
    "                hlt, kernel_size=(3,3), activation='relu', kernel_initializer='RandomNormal'))\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "                shape_actions, activation='linear', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        flatten = self.flatten_layer(z)\n",
    "        output = self.output_layer(flatten)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class DQN:\n",
    "    def __init__(self, shape_states, shape_actions, hidden_layers_template, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.shape_actions = shape_actions\n",
    "        self.shape_states = shape_states\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = CustomModel(shape_states, hidden_layers_template, shape_actions)\n",
    "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if inputs.shape == self.shape_states:\n",
    "            inputs = np.expand_dims(inputs, axis = 0)\n",
    "        prediction = self.model(inputs.astype('float32'))\n",
    "        return prediction\n",
    "\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "            return 0\n",
    "        # chooses an random integer in range low to high, with batch_size samples\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
    "        # sets states to an array of batch_size random experiences from the experience array\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        # same for actions\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        # same for rewards\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        # same for next states\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        # same for dones\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        # predicts the next values based on the next states\n",
    "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
    "        # gets the reward where done is true \n",
    "        # and the reward * self.gamma * predicted_value) where done is false\n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states) * tf.one_hot(actions, self.shape_actions), axis=1)\n",
    "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.shape_actions)\n",
    "        else:\n",
    "            return np.argmax(self.predict(states))\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss = TrainNet.train(TargetNet)\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, mean(losses), iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "env = RacingKingsEnvironment()\n",
    "gamma = 0.99\n",
    "copy_step = 5\n",
    "hidden_units = [256, 256]\n",
    "max_experiences = 10000\n",
    "min_experiences = 100\n",
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'Training/Logs/CustomDQN-' + current_time + '-min_epsilon-025-decay-1e-5_no-neg-reward'\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "TrainNet = DQN(env.state_shape, env.action_shape, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(env.state_shape, env.action_shape, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "\n",
    "N = 500000\n",
    "StorageInterval = 500\n",
    "total_episode_rewards = np.empty(StorageInterval)\n",
    "total_episode_lengths = np.empty(StorageInterval)\n",
    "total_episode_losses = np.empty(StorageInterval)\n",
    "# starting epsilon: at the beginning of training 99 % of randomness are allowed\n",
    "epsilon = 0.99\n",
    "# sets the speed epsilon decreases to min epsilon\n",
    "decay = 1 - 1e-5\n",
    "# sets the end amount of randomness encounterd by to model on long term training to 1 %\n",
    "min_epsilon = 0.25\n",
    "for n in range(N):\n",
    "    epsilon = epsilon * decay\n",
    "    if epsilon < min_epsilon:\n",
    "        epsilon = min_epsilon\n",
    "    \n",
    "    reward, loss, step = play_racing_kings(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "    total_episode_rewards[n%StorageInterval] = reward\n",
    "    total_episode_lengths[n%StorageInterval] = step\n",
    "    total_episode_losses[n%StorageInterval] = loss\n",
    "    if n % StorageInterval == 0 and n != 0:\n",
    "        avg_episode_rewards = np.mean(total_episode_rewards)\n",
    "        avg_episode_length = np.mean(total_episode_lengths)\n",
    "        avg_episode_losses = np.mean(total_episode_losses)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('mean_ep_length', avg_episode_length, step=n)\n",
    "            tf.summary.scalar('mean_reward', avg_episode_rewards, step=n)\n",
    "            tf.summary.scalar('loss', avg_episode_losses, step=n)\n",
    "            tf.summary.scalar('epsilon', epsilon, step=n)\n",
    "        print('episode:{} epsilon:{:.3} mean_reward:{:.3} mean_ep_length:{:.3} loss:{:.3}'\n",
    "                .format(n, float(epsilon), float(avg_episode_rewards), float(avg_episode_length), float(avg_episode_losses)))\n",
    "        total_episode_rewards = np.empty(StorageInterval)\n",
    "        total_episode_lengths = np.empty(StorageInterval)\n",
    "        total_episode_losses = np.empty(StorageInterval)\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-27 16:36:33.292065: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-27 16:36:33.331441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.331869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-07-27 16:36:33.331891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-07-27 16:36:33.333440: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-07-27 16:36:33.334740: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-27 16:36:33.335070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-27 16:36:33.336594: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-27 16:36:33.337435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-07-27 16:36:33.340920: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-27 16:36:33.341111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.341635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.342296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-07-27 16:36:33.350019: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3392145000 Hz\n",
      "2021-07-27 16:36:33.350455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x605df20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-27 16:36:33.350480: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-07-27 16:36:33.447789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.448292: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x60f5ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-27 16:36:33.448331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2021-07-27 16:36:33.448619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.449360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-07-27 16:36:33.449407: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-07-27 16:36:33.449441: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-07-27 16:36:33.449468: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-27 16:36:33.449494: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-27 16:36:33.449524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-27 16:36:33.449551: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-07-27 16:36:33.449578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-27 16:36:33.449688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.450436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.451138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-07-27 16:36:33.451194: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-07-27 16:36:33.847224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-27 16:36:33.847257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2021-07-27 16:36:33.847265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2021-07-27 16:36:33.847460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.847865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-27 16:36:33.848218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5193 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2021-07-27 16:36:38.383123: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-07-27 16:36:38.532006: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode:500 epsilon:0.985 mean_reward:-1.0 mean_ep_length:1.01 loss:0.091\n",
      "episode:1000 epsilon:0.98 mean_reward:-1.0 mean_ep_length:1.01 loss:0.043\n",
      "episode:1500 epsilon:0.975 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0416\n",
      "episode:2000 epsilon:0.97 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0357\n",
      "episode:2500 epsilon:0.966 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0355\n",
      "episode:3000 epsilon:0.961 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0391\n",
      "episode:3500 epsilon:0.956 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0325\n",
      "episode:4000 epsilon:0.951 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0322\n",
      "episode:4500 epsilon:0.946 mean_reward:-0.999 mean_ep_length:1.02 loss:0.0302\n",
      "episode:5000 epsilon:0.942 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0311\n",
      "episode:5500 epsilon:0.937 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0321\n",
      "episode:6000 epsilon:0.932 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0294\n",
      "episode:6500 epsilon:0.928 mean_reward:-1.0 mean_ep_length:1.0 loss:0.0273\n",
      "episode:7000 epsilon:0.923 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0297\n",
      "episode:7500 epsilon:0.918 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0261\n",
      "episode:8000 epsilon:0.914 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0304\n",
      "episode:8500 epsilon:0.909 mean_reward:-1.0 mean_ep_length:1.02 loss:0.032\n",
      "episode:9000 epsilon:0.905 mean_reward:-1.0 mean_ep_length:1.02 loss:0.028\n",
      "episode:9500 epsilon:0.9 mean_reward:-1.0 mean_ep_length:1.04 loss:0.031\n",
      "episode:10000 epsilon:0.896 mean_reward:-1.0 mean_ep_length:1.03 loss:0.034\n",
      "episode:10500 epsilon:0.891 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0351\n",
      "episode:11000 epsilon:0.887 mean_reward:-0.999 mean_ep_length:1.03 loss:0.0373\n",
      "episode:11500 epsilon:0.882 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0357\n",
      "episode:12000 epsilon:0.878 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0351\n",
      "episode:12500 epsilon:0.874 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0366\n",
      "episode:13000 epsilon:0.869 mean_reward:-1.0 mean_ep_length:1.02 loss:0.037\n",
      "episode:13500 epsilon:0.865 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0363\n",
      "episode:14000 epsilon:0.861 mean_reward:-0.999 mean_ep_length:1.01 loss:0.0367\n",
      "episode:14500 epsilon:0.856 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0324\n",
      "episode:15000 epsilon:0.852 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0361\n",
      "episode:15500 epsilon:0.848 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0384\n",
      "episode:16000 epsilon:0.844 mean_reward:-1.0 mean_ep_length:1.03 loss:0.041\n",
      "episode:16500 epsilon:0.839 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0409\n",
      "episode:17000 epsilon:0.835 mean_reward:-1.0 mean_ep_length:1.04 loss:0.05\n",
      "episode:17500 epsilon:0.831 mean_reward:-0.999 mean_ep_length:1.04 loss:0.0504\n",
      "episode:18000 epsilon:0.827 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0546\n",
      "episode:18500 epsilon:0.823 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0498\n",
      "episode:19000 epsilon:0.819 mean_reward:-1.0 mean_ep_length:1.04 loss:0.0492\n",
      "episode:19500 epsilon:0.815 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0492\n",
      "episode:20000 epsilon:0.811 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0475\n",
      "episode:20500 epsilon:0.806 mean_reward:-1.0 mean_ep_length:1.04 loss:0.0534\n",
      "episode:21000 epsilon:0.802 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0544\n",
      "episode:21500 epsilon:0.798 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0531\n",
      "episode:22000 epsilon:0.794 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0536\n",
      "episode:22500 epsilon:0.791 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0559\n",
      "episode:23000 epsilon:0.787 mean_reward:-1.0 mean_ep_length:1.04 loss:0.0594\n",
      "episode:23500 epsilon:0.783 mean_reward:-1.0 mean_ep_length:1.04 loss:0.0537\n",
      "episode:24000 epsilon:0.779 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0573\n",
      "episode:24500 epsilon:0.775 mean_reward:-0.999 mean_ep_length:1.06 loss:0.0602\n",
      "episode:25000 epsilon:0.771 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0666\n",
      "episode:25500 epsilon:0.767 mean_reward:-0.999 mean_ep_length:1.05 loss:0.0614\n",
      "episode:26000 epsilon:0.763 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0669\n",
      "episode:26500 epsilon:0.76 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0685\n",
      "episode:27000 epsilon:0.756 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0723\n",
      "episode:27500 epsilon:0.752 mean_reward:-0.999 mean_ep_length:1.08 loss:0.07\n",
      "episode:28000 epsilon:0.748 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0737\n",
      "episode:28500 epsilon:0.744 mean_reward:-1.0 mean_ep_length:1.07 loss:0.073\n",
      "episode:29000 epsilon:0.741 mean_reward:-1.0 mean_ep_length:1.05 loss:0.073\n",
      "episode:29500 epsilon:0.737 mean_reward:-0.999 mean_ep_length:1.08 loss:0.0775\n",
      "episode:30000 epsilon:0.733 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0783\n",
      "episode:30500 epsilon:0.73 mean_reward:-0.998 mean_ep_length:1.09 loss:0.0762\n",
      "episode:31000 epsilon:0.726 mean_reward:-0.999 mean_ep_length:1.11 loss:0.0767\n",
      "episode:31500 epsilon:0.722 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0795\n",
      "episode:32000 epsilon:0.719 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0932\n",
      "episode:32500 epsilon:0.715 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0884\n",
      "episode:33000 epsilon:0.712 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0973\n",
      "episode:33500 epsilon:0.708 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0955\n",
      "episode:34000 epsilon:0.705 mean_reward:-1.0 mean_ep_length:1.12 loss:0.0941\n",
      "episode:34500 epsilon:0.701 mean_reward:-1.0 mean_ep_length:1.13 loss:0.0971\n",
      "episode:35000 epsilon:0.698 mean_reward:-0.999 mean_ep_length:1.11 loss:0.105\n",
      "episode:35500 epsilon:0.694 mean_reward:-1.0 mean_ep_length:1.14 loss:0.105\n",
      "episode:36000 epsilon:0.691 mean_reward:-1.0 mean_ep_length:1.12 loss:0.106\n",
      "episode:36500 epsilon:0.687 mean_reward:-1.0 mean_ep_length:1.12 loss:0.113\n",
      "episode:37000 epsilon:0.684 mean_reward:-0.999 mean_ep_length:1.1 loss:0.112\n",
      "episode:37500 epsilon:0.68 mean_reward:-1.0 mean_ep_length:1.14 loss:0.111\n",
      "episode:38000 epsilon:0.677 mean_reward:-1.0 mean_ep_length:1.1 loss:0.113\n",
      "episode:38500 epsilon:0.674 mean_reward:-1.0 mean_ep_length:1.1 loss:0.109\n",
      "episode:39000 epsilon:0.67 mean_reward:-1.0 mean_ep_length:1.12 loss:0.114\n",
      "episode:39500 epsilon:0.667 mean_reward:-0.999 mean_ep_length:1.13 loss:0.0952\n",
      "episode:40000 epsilon:0.664 mean_reward:-1.0 mean_ep_length:1.13 loss:0.0481\n",
      "episode:40500 epsilon:0.66 mean_reward:-1.0 mean_ep_length:1.11 loss:0.0379\n",
      "episode:41000 epsilon:0.657 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0242\n",
      "episode:41500 epsilon:0.654 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0236\n",
      "episode:42000 epsilon:0.65 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0247\n",
      "episode:42500 epsilon:0.647 mean_reward:-0.999 mean_ep_length:1.1 loss:0.0264\n",
      "episode:43000 epsilon:0.644 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0246\n",
      "episode:43500 epsilon:0.641 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0251\n",
      "episode:44000 epsilon:0.638 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0252\n",
      "episode:44500 epsilon:0.634 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0229\n",
      "episode:45000 epsilon:0.631 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0257\n",
      "episode:45500 epsilon:0.628 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0242\n",
      "episode:46000 epsilon:0.625 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0222\n",
      "episode:46500 epsilon:0.622 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0169\n",
      "episode:47000 epsilon:0.619 mean_reward:-1.0 mean_ep_length:1.06 loss:0.01\n",
      "episode:47500 epsilon:0.616 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00791\n",
      "episode:48000 epsilon:0.613 mean_reward:-0.999 mean_ep_length:1.04 loss:0.00764\n",
      "episode:48500 epsilon:0.61 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00929\n",
      "episode:49000 epsilon:0.606 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00896\n",
      "episode:49500 epsilon:0.603 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00936\n",
      "episode:50000 epsilon:0.6 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00843\n",
      "episode:50500 epsilon:0.597 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00872\n",
      "episode:51000 epsilon:0.594 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00803\n",
      "episode:51500 epsilon:0.592 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00757\n",
      "episode:52000 epsilon:0.589 mean_reward:-0.999 mean_ep_length:1.03 loss:0.00595\n",
      "episode:52500 epsilon:0.586 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00548\n",
      "episode:53000 epsilon:0.583 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00592\n",
      "episode:53500 epsilon:0.58 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0057\n",
      "episode:54000 epsilon:0.577 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00562\n",
      "episode:54500 epsilon:0.574 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00554\n",
      "episode:55000 epsilon:0.571 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00527\n",
      "episode:55500 epsilon:0.568 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0047\n",
      "episode:56000 epsilon:0.565 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00536\n",
      "episode:56500 epsilon:0.563 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00511\n",
      "episode:57000 epsilon:0.56 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00494\n",
      "episode:57500 epsilon:0.557 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00455\n",
      "episode:58000 epsilon:0.554 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00488\n",
      "episode:58500 epsilon:0.552 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00453\n",
      "episode:59000 epsilon:0.549 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00455\n",
      "episode:59500 epsilon:0.546 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00415\n",
      "episode:60000 epsilon:0.543 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00388\n",
      "episode:60500 epsilon:0.541 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00393\n",
      "episode:61000 epsilon:0.538 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00379\n",
      "episode:61500 epsilon:0.535 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00371\n",
      "episode:62000 epsilon:0.533 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00345\n",
      "episode:62500 epsilon:0.53 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00355\n",
      "episode:63000 epsilon:0.527 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0034\n",
      "episode:63500 epsilon:0.525 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00355\n",
      "episode:64000 epsilon:0.522 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00346\n",
      "episode:64500 epsilon:0.519 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00314\n",
      "episode:65000 epsilon:0.517 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00328\n",
      "episode:65500 epsilon:0.514 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00285\n",
      "episode:66000 epsilon:0.512 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00318\n",
      "episode:66500 epsilon:0.509 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00271\n",
      "episode:67000 epsilon:0.507 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00271\n",
      "episode:67500 epsilon:0.504 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00291\n",
      "episode:68000 epsilon:0.502 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00294\n",
      "episode:68500 epsilon:0.499 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00308\n",
      "episode:69000 epsilon:0.497 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00318\n",
      "episode:69500 epsilon:0.494 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00276\n",
      "episode:70000 epsilon:0.492 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00279\n",
      "episode:70500 epsilon:0.489 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00305\n",
      "episode:71000 epsilon:0.487 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00276\n",
      "episode:71500 epsilon:0.484 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0029\n",
      "episode:72000 epsilon:0.482 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0025\n",
      "episode:72500 epsilon:0.479 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00262\n",
      "episode:73000 epsilon:0.477 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00236\n",
      "episode:73500 epsilon:0.475 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00243\n",
      "episode:74000 epsilon:0.472 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00234\n",
      "episode:74500 epsilon:0.47 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00234\n",
      "episode:75000 epsilon:0.468 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00234\n",
      "episode:75500 epsilon:0.465 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00231\n",
      "episode:76000 epsilon:0.463 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00228\n",
      "episode:76500 epsilon:0.461 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00233\n",
      "episode:77000 epsilon:0.458 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0023\n",
      "episode:77500 epsilon:0.456 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00222\n",
      "episode:78000 epsilon:0.454 mean_reward:-0.998 mean_ep_length:1.01 loss:0.00219\n",
      "episode:78500 epsilon:0.452 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00219\n",
      "episode:79000 epsilon:0.449 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00204\n",
      "episode:79500 epsilon:0.447 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00222\n",
      "episode:80000 epsilon:0.445 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00217\n",
      "episode:80500 epsilon:0.443 mean_reward:-0.999 mean_ep_length:1.02 loss:0.0021\n",
      "episode:81000 epsilon:0.44 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00237\n",
      "episode:81500 epsilon:0.438 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00228\n",
      "episode:82000 epsilon:0.436 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00232\n",
      "episode:82500 epsilon:0.434 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00235\n",
      "episode:83000 epsilon:0.432 mean_reward:-0.999 mean_ep_length:1.0 loss:0.00247\n",
      "episode:83500 epsilon:0.43 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0022\n",
      "episode:84000 epsilon:0.427 mean_reward:-0.998 mean_ep_length:1.01 loss:0.0025\n",
      "episode:84500 epsilon:0.425 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00225\n",
      "episode:85000 epsilon:0.423 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00229\n",
      "episode:85500 epsilon:0.421 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00218\n",
      "episode:86000 epsilon:0.419 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00213\n",
      "episode:86500 epsilon:0.417 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00215\n",
      "episode:87000 epsilon:0.415 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00236\n",
      "episode:87500 epsilon:0.413 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00227\n",
      "episode:88000 epsilon:0.411 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0022\n",
      "episode:88500 epsilon:0.409 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00242\n",
      "episode:89000 epsilon:0.407 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00222\n",
      "episode:89500 epsilon:0.405 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00222\n",
      "episode:90000 epsilon:0.402 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00213\n",
      "episode:90500 epsilon:0.4 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00239\n",
      "episode:91000 epsilon:0.398 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00208\n",
      "episode:91500 epsilon:0.397 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0021\n",
      "episode:92000 epsilon:0.395 mean_reward:-1.0 mean_ep_length:1.02 loss:0.002\n",
      "episode:92500 epsilon:0.393 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00195\n",
      "episode:93000 epsilon:0.391 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00219\n",
      "episode:93500 epsilon:0.389 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00204\n",
      "episode:94000 epsilon:0.387 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00196\n",
      "episode:94500 epsilon:0.385 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00199\n",
      "episode:95000 epsilon:0.383 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00238\n",
      "episode:95500 epsilon:0.381 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00227\n",
      "episode:96000 epsilon:0.379 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00227\n",
      "episode:96500 epsilon:0.377 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00244\n",
      "episode:97000 epsilon:0.375 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00188\n",
      "episode:97500 epsilon:0.373 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00219\n",
      "episode:98000 epsilon:0.372 mean_reward:-0.999 mean_ep_length:1.03 loss:0.00178\n",
      "episode:98500 epsilon:0.37 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00208\n",
      "episode:99000 epsilon:0.368 mean_reward:-1.0 mean_ep_length:1.01 loss:0.002\n",
      "episode:99500 epsilon:0.366 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00196\n",
      "episode:100000 epsilon:0.364 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00203\n",
      "episode:100500 epsilon:0.362 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00203\n",
      "episode:101000 epsilon:0.361 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0019\n",
      "episode:101500 epsilon:0.359 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00201\n",
      "episode:102000 epsilon:0.357 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00204\n",
      "episode:102500 epsilon:0.355 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00184\n",
      "episode:103000 epsilon:0.353 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00211\n",
      "episode:103500 epsilon:0.352 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00185\n",
      "episode:104000 epsilon:0.35 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00171\n",
      "episode:104500 epsilon:0.348 mean_reward:-0.999 mean_ep_length:1.0 loss:0.00163\n",
      "episode:105000 epsilon:0.346 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00171\n",
      "episode:105500 epsilon:0.345 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00158\n",
      "episode:106000 epsilon:0.343 mean_reward:-0.999 mean_ep_length:1.01 loss:0.0016\n",
      "episode:106500 epsilon:0.341 mean_reward:-1.0 mean_ep_length:1.02 loss:0.0017\n",
      "episode:107000 epsilon:0.34 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00175\n",
      "episode:107500 epsilon:0.338 mean_reward:-0.999 mean_ep_length:1.01 loss:0.0017\n",
      "episode:108000 epsilon:0.336 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00169\n",
      "episode:108500 epsilon:0.335 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00178\n",
      "episode:109000 epsilon:0.333 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00171\n",
      "episode:109500 epsilon:0.331 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00164\n",
      "episode:110000 epsilon:0.33 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00175\n",
      "episode:110500 epsilon:0.328 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00166\n",
      "episode:111000 epsilon:0.326 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00161\n",
      "episode:111500 epsilon:0.325 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00163\n",
      "episode:112000 epsilon:0.323 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00165\n",
      "episode:112500 epsilon:0.321 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00151\n",
      "episode:113000 epsilon:0.32 mean_reward:-0.998 mean_ep_length:1.02 loss:0.00168\n",
      "episode:113500 epsilon:0.318 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00173\n",
      "episode:114000 epsilon:0.317 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00164\n",
      "episode:114500 epsilon:0.315 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00176\n",
      "episode:115000 epsilon:0.313 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00194\n",
      "episode:115500 epsilon:0.312 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00187\n",
      "episode:116000 epsilon:0.31 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00149\n",
      "episode:116500 epsilon:0.309 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00166\n",
      "episode:117000 epsilon:0.307 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00178\n",
      "episode:117500 epsilon:0.306 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00176\n",
      "episode:118000 epsilon:0.304 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00155\n",
      "episode:118500 epsilon:0.303 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00169\n",
      "episode:119000 epsilon:0.301 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00148\n",
      "episode:119500 epsilon:0.3 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00151\n",
      "episode:120000 epsilon:0.298 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00157\n",
      "episode:120500 epsilon:0.297 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00155\n",
      "episode:121000 epsilon:0.295 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00175\n",
      "episode:121500 epsilon:0.294 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00149\n",
      "episode:122000 epsilon:0.292 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00166\n",
      "episode:122500 epsilon:0.291 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00159\n",
      "episode:123000 epsilon:0.289 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00167\n",
      "episode:123500 epsilon:0.288 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00143\n",
      "episode:124000 epsilon:0.286 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00159\n",
      "episode:124500 epsilon:0.285 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00149\n",
      "episode:125000 epsilon:0.284 mean_reward:-0.999 mean_ep_length:1.0 loss:0.00166\n",
      "episode:125500 epsilon:0.282 mean_reward:-1.0 mean_ep_length:1.0 loss:0.00141\n",
      "episode:126000 epsilon:0.281 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00151\n",
      "episode:126500 epsilon:0.279 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00164\n",
      "episode:127000 epsilon:0.278 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00154\n",
      "episode:127500 epsilon:0.277 mean_reward:-0.998 mean_ep_length:1.01 loss:0.00152\n",
      "episode:128000 epsilon:0.275 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00154\n",
      "episode:128500 epsilon:0.274 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00168\n",
      "episode:129000 epsilon:0.273 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00156\n",
      "episode:129500 epsilon:0.271 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00158\n",
      "episode:130000 epsilon:0.27 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00164\n",
      "episode:130500 epsilon:0.268 mean_reward:-0.999 mean_ep_length:1.01 loss:0.0017\n",
      "episode:131000 epsilon:0.267 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00152\n",
      "episode:131500 epsilon:0.266 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00167\n",
      "episode:132000 epsilon:0.264 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00171\n",
      "episode:132500 epsilon:0.263 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00179\n",
      "episode:133000 epsilon:0.262 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00197\n",
      "episode:133500 epsilon:0.261 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0018\n",
      "episode:134000 epsilon:0.259 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00191\n",
      "episode:134500 epsilon:0.258 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00173\n",
      "episode:135000 epsilon:0.257 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00187\n",
      "episode:135500 epsilon:0.255 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00195\n",
      "episode:136000 epsilon:0.254 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00176\n",
      "episode:136500 epsilon:0.253 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00177\n",
      "episode:137000 epsilon:0.252 mean_reward:-1.0 mean_ep_length:1.0 loss:0.0019\n",
      "episode:137500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.0 loss:0.00182\n",
      "episode:138000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00167\n",
      "episode:138500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00174\n",
      "episode:139000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00161\n",
      "episode:139500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00185\n",
      "episode:140000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.03 loss:0.00151\n",
      "episode:140500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00158\n",
      "episode:141000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00191\n",
      "episode:141500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.0014\n",
      "episode:142000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.0 loss:0.0015\n",
      "episode:142500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00167\n",
      "episode:143000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00147\n",
      "episode:143500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00147\n",
      "episode:144000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00139\n",
      "episode:144500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00149\n",
      "episode:145000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00137\n",
      "episode:145500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00145\n",
      "episode:146000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00149\n",
      "episode:146500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00148\n",
      "episode:147000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00126\n",
      "episode:147500 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.01 loss:0.00153\n",
      "episode:148000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00141\n",
      "episode:148500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00155\n",
      "episode:149000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00171\n",
      "episode:149500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00143\n",
      "episode:150000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00154\n",
      "episode:150500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00131\n",
      "episode:151000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.01 loss:0.00137\n",
      "episode:151500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00133\n",
      "episode:152000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00145\n",
      "episode:152500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00156\n",
      "episode:153000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00154\n",
      "episode:153500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00144\n",
      "episode:154000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00167\n",
      "episode:154500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00155\n",
      "episode:155000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00186\n",
      "episode:155500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00137\n",
      "episode:156000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00164\n",
      "episode:156500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00173\n",
      "episode:157000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00179\n",
      "episode:157500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00172\n",
      "episode:158000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.01 loss:0.00164\n",
      "episode:158500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00164\n",
      "episode:159000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0017\n",
      "episode:159500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00169\n",
      "episode:160000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00166\n",
      "episode:160500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00184\n",
      "episode:161000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00191\n",
      "episode:161500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00221\n",
      "episode:162000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00186\n",
      "episode:162500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00185\n",
      "episode:163000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00196\n",
      "episode:163500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00212\n",
      "episode:164000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.002\n",
      "episode:164500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00213\n",
      "episode:165000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00209\n",
      "episode:165500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00203\n",
      "episode:166000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.02 loss:0.00208\n",
      "episode:166500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.04 loss:0.0023\n",
      "episode:167000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00196\n",
      "episode:167500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.04 loss:0.00236\n",
      "episode:168000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00209\n",
      "episode:168500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.0025\n",
      "episode:169000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00243\n",
      "episode:169500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00245\n",
      "episode:170000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00284\n",
      "episode:170500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.05 loss:0.00265\n",
      "episode:171000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00266\n",
      "episode:171500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00264\n",
      "episode:172000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0029\n",
      "episode:172500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00289\n",
      "episode:173000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00282\n",
      "episode:173500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00278\n",
      "episode:174000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00271\n",
      "episode:174500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00288\n",
      "episode:175000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0028\n",
      "episode:175500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00299\n",
      "episode:176000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.04 loss:0.00316\n",
      "episode:176500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00306\n",
      "episode:177000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00313\n",
      "episode:177500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0033\n",
      "episode:178000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00337\n",
      "episode:178500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00355\n",
      "episode:179000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00353\n",
      "episode:179500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00332\n",
      "episode:180000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00338\n",
      "episode:180500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.0032\n",
      "episode:181000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00327\n",
      "episode:181500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00308\n",
      "episode:182000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0033\n",
      "episode:182500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.03 loss:0.00313\n",
      "episode:183000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0035\n",
      "episode:183500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.03 loss:0.00336\n",
      "episode:184000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0034\n",
      "episode:184500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00333\n",
      "episode:185000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00338\n",
      "episode:185500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00316\n",
      "episode:186000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00336\n",
      "episode:186500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00339\n",
      "episode:187000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00356\n",
      "episode:187500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0035\n",
      "episode:188000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00355\n",
      "episode:188500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00346\n",
      "episode:189000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00354\n",
      "episode:189500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0037\n",
      "episode:190000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.004\n",
      "episode:190500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00366\n",
      "episode:191000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00411\n",
      "episode:191500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00401\n",
      "episode:192000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00384\n",
      "episode:192500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00387\n",
      "episode:193000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00422\n",
      "episode:193500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00428\n",
      "episode:194000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00427\n",
      "episode:194500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00424\n",
      "episode:195000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00441\n",
      "episode:195500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00479\n",
      "episode:196000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00445\n",
      "episode:196500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0045\n",
      "episode:197000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.0046\n",
      "episode:197500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00447\n",
      "episode:198000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00483\n",
      "episode:198500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00449\n",
      "episode:199000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00471\n",
      "episode:199500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00434\n",
      "episode:200000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00436\n",
      "episode:200500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.02 loss:0.00461\n",
      "episode:201000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00437\n",
      "episode:201500 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.06 loss:0.00445\n",
      "episode:202000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00417\n",
      "episode:202500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00443\n",
      "episode:203000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00429\n",
      "episode:203500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00442\n",
      "episode:204000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0043\n",
      "episode:204500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00437\n",
      "episode:205000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00399\n",
      "episode:205500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0044\n",
      "episode:206000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00417\n",
      "episode:206500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.04 loss:0.00427\n",
      "episode:207000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00403\n",
      "episode:207500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00437\n",
      "episode:208000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00424\n",
      "episode:208500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00433\n",
      "episode:209000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00435\n",
      "episode:209500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00425\n",
      "episode:210000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00415\n",
      "episode:210500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00454\n",
      "episode:211000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.0042\n",
      "episode:211500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00417\n",
      "episode:212000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00402\n",
      "episode:212500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00386\n",
      "episode:213000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00435\n",
      "episode:213500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00395\n",
      "episode:214000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.004\n",
      "episode:214500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00422\n",
      "episode:215000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.05 loss:0.00426\n",
      "episode:215500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00393\n",
      "episode:216000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0038\n",
      "episode:216500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00411\n",
      "episode:217000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00422\n",
      "episode:217500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00393\n",
      "episode:218000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00419\n",
      "episode:218500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00393\n",
      "episode:219000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.05 loss:0.00389\n",
      "episode:219500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0042\n",
      "episode:220000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00438\n",
      "episode:220500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00449\n",
      "episode:221000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00437\n",
      "episode:221500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00477\n",
      "episode:222000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00448\n",
      "episode:222500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00467\n",
      "episode:223000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00473\n",
      "episode:223500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0045\n",
      "episode:224000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.05 loss:0.00444\n",
      "episode:224500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00436\n",
      "episode:225000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00488\n",
      "episode:225500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00445\n",
      "episode:226000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00488\n",
      "episode:226500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00536\n",
      "episode:227000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00477\n",
      "episode:227500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00486\n",
      "episode:228000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.005\n",
      "episode:228500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00549\n",
      "episode:229000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00506\n",
      "episode:229500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00519\n",
      "episode:230000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00522\n",
      "episode:230500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00524\n",
      "episode:231000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0055\n",
      "episode:231500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00533\n",
      "episode:232000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00547\n",
      "episode:232500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00593\n",
      "episode:233000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00545\n",
      "episode:233500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00539\n",
      "episode:234000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00524\n",
      "episode:234500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00502\n",
      "episode:235000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00523\n",
      "episode:235500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0052\n",
      "episode:236000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00521\n",
      "episode:236500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00509\n",
      "episode:237000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00511\n",
      "episode:237500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00506\n",
      "episode:238000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00477\n",
      "episode:238500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00446\n",
      "episode:239000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00477\n",
      "episode:239500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00458\n",
      "episode:240000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00493\n",
      "episode:240500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00468\n",
      "episode:241000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0044\n",
      "episode:241500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00455\n",
      "episode:242000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00446\n",
      "episode:242500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00448\n",
      "episode:243000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00433\n",
      "episode:243500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.14 loss:0.00447\n",
      "episode:244000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00494\n",
      "episode:244500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00469\n",
      "episode:245000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00445\n",
      "episode:245500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00488\n",
      "episode:246000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00527\n",
      "episode:246500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00525\n",
      "episode:247000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00522\n",
      "episode:247500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00515\n",
      "episode:248000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00503\n",
      "episode:248500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00506\n",
      "episode:249000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00541\n",
      "episode:249500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00531\n",
      "episode:250000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00544\n",
      "episode:250500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00539\n",
      "episode:251000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0054\n",
      "episode:251500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00519\n",
      "episode:252000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0055\n",
      "episode:252500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00516\n",
      "episode:253000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00527\n",
      "episode:253500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00543\n",
      "episode:254000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00527\n",
      "episode:254500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00533\n",
      "episode:255000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00504\n",
      "episode:255500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00508\n",
      "episode:256000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00522\n",
      "episode:256500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00515\n",
      "episode:257000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00509\n",
      "episode:257500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00531\n",
      "episode:258000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00514\n",
      "episode:258500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00526\n",
      "episode:259000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00503\n",
      "episode:259500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00475\n",
      "episode:260000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.04 loss:0.00469\n",
      "episode:260500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00489\n",
      "episode:261000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00485\n",
      "episode:261500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0046\n",
      "episode:262000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00481\n",
      "episode:262500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.0044\n",
      "episode:263000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00436\n",
      "episode:263500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00458\n",
      "episode:264000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00479\n",
      "episode:264500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00479\n",
      "episode:265000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00415\n",
      "episode:265500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00449\n",
      "episode:266000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00479\n",
      "episode:266500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00482\n",
      "episode:267000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00478\n",
      "episode:267500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.00459\n",
      "episode:268000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00479\n",
      "episode:268500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00514\n",
      "episode:269000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00525\n",
      "episode:269500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00535\n",
      "episode:270000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00528\n",
      "episode:270500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00554\n",
      "episode:271000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00554\n",
      "episode:271500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00532\n",
      "episode:272000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00556\n",
      "episode:272500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00511\n",
      "episode:273000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00534\n",
      "episode:273500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00552\n",
      "episode:274000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00533\n",
      "episode:274500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00554\n",
      "episode:275000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00521\n",
      "episode:275500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00562\n",
      "episode:276000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.13 loss:0.00529\n",
      "episode:276500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.0056\n",
      "episode:277000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00565\n",
      "episode:277500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00548\n",
      "episode:278000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00541\n",
      "episode:278500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00517\n",
      "episode:279000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0051\n",
      "episode:279500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00508\n",
      "episode:280000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00558\n",
      "episode:280500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00522\n",
      "episode:281000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00509\n",
      "episode:281500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00538\n",
      "episode:282000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00551\n",
      "episode:282500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00538\n",
      "episode:283000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00538\n",
      "episode:283500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00542\n",
      "episode:284000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00557\n",
      "episode:284500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0054\n",
      "episode:285000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00521\n",
      "episode:285500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00559\n",
      "episode:286000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00524\n",
      "episode:286500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00521\n",
      "episode:287000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00533\n",
      "episode:287500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00491\n",
      "episode:288000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00538\n",
      "episode:288500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00564\n",
      "episode:289000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00575\n",
      "episode:289500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00535\n",
      "episode:290000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0052\n",
      "episode:290500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00534\n",
      "episode:291000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00497\n",
      "episode:291500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00535\n",
      "episode:292000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0053\n",
      "episode:292500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00515\n",
      "episode:293000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00497\n",
      "episode:293500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00509\n",
      "episode:294000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00496\n",
      "episode:294500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00539\n",
      "episode:295000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00505\n",
      "episode:295500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00513\n",
      "episode:296000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00513\n",
      "episode:296500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0053\n",
      "episode:297000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00522\n",
      "episode:297500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00544\n",
      "episode:298000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00511\n",
      "episode:298500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00503\n",
      "episode:299000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00511\n",
      "episode:299500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00546\n",
      "episode:300000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00515\n",
      "episode:300500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00519\n",
      "episode:301000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.0052\n",
      "episode:301500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00575\n",
      "episode:302000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00544\n",
      "episode:302500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00558\n",
      "episode:303000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.0055\n",
      "episode:303500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00521\n",
      "episode:304000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00503\n",
      "episode:304500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00512\n",
      "episode:305000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00503\n",
      "episode:305500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00493\n",
      "episode:306000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00519\n",
      "episode:306500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00526\n",
      "episode:307000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00521\n",
      "episode:307500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00539\n",
      "episode:308000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.0053\n",
      "episode:308500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00535\n",
      "episode:309000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00529\n",
      "episode:309500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0056\n",
      "episode:310000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00573\n",
      "episode:310500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00555\n",
      "episode:311000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00557\n",
      "episode:311500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00542\n",
      "episode:312000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00582\n",
      "episode:312500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00578\n",
      "episode:313000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00581\n",
      "episode:313500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00567\n",
      "episode:314000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00591\n",
      "episode:314500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00573\n",
      "episode:315000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00567\n",
      "episode:315500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00551\n",
      "episode:316000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00552\n",
      "episode:316500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00548\n",
      "episode:317000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00583\n",
      "episode:317500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00591\n",
      "episode:318000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00553\n",
      "episode:318500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00539\n",
      "episode:319000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00576\n",
      "episode:319500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00551\n",
      "episode:320000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00571\n",
      "episode:320500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00518\n",
      "episode:321000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00538\n",
      "episode:321500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00541\n",
      "episode:322000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00533\n",
      "episode:322500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00525\n",
      "episode:323000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00516\n",
      "episode:323500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0057\n",
      "episode:324000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00552\n",
      "episode:324500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00535\n",
      "episode:325000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.00567\n",
      "episode:325500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00559\n",
      "episode:326000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0054\n",
      "episode:326500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0054\n",
      "episode:327000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00551\n",
      "episode:327500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00554\n",
      "episode:328000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00509\n",
      "episode:328500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00521\n",
      "episode:329000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0051\n",
      "episode:329500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00516\n",
      "episode:330000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00537\n",
      "episode:330500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00516\n",
      "episode:331000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0052\n",
      "episode:331500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00513\n",
      "episode:332000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00526\n",
      "episode:332500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00545\n",
      "episode:333000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00528\n",
      "episode:333500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00534\n",
      "episode:334000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0053\n",
      "episode:334500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00518\n",
      "episode:335000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.00532\n",
      "episode:335500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00541\n",
      "episode:336000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00551\n",
      "episode:336500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0053\n",
      "episode:337000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00513\n",
      "episode:337500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0055\n",
      "episode:338000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0056\n",
      "episode:338500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00586\n",
      "episode:339000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00557\n",
      "episode:339500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00598\n",
      "episode:340000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00625\n",
      "episode:340500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00608\n",
      "episode:341000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00585\n",
      "episode:341500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00603\n",
      "episode:342000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00585\n",
      "episode:342500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00573\n",
      "episode:343000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00569\n",
      "episode:343500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00572\n",
      "episode:344000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00596\n",
      "episode:344500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00558\n",
      "episode:345000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00552\n",
      "episode:345500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00524\n",
      "episode:346000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0051\n",
      "episode:346500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00516\n",
      "episode:347000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0054\n",
      "episode:347500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0049\n",
      "episode:348000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00491\n",
      "episode:348500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00477\n",
      "episode:349000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00461\n",
      "episode:349500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00455\n",
      "episode:350000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0046\n",
      "episode:350500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00446\n",
      "episode:351000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00481\n",
      "episode:351500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00459\n",
      "episode:352000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00459\n",
      "episode:352500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00462\n",
      "episode:353000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00473\n",
      "episode:353500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.005\n",
      "episode:354000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00446\n",
      "episode:354500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00485\n",
      "episode:355000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00506\n",
      "episode:355500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00499\n",
      "episode:356000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.09 loss:0.00474\n",
      "episode:356500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00465\n",
      "episode:357000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.07 loss:0.00472\n",
      "episode:357500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00498\n",
      "episode:358000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.04 loss:0.00487\n",
      "episode:358500 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.07 loss:0.00497\n",
      "episode:359000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00501\n",
      "episode:359500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0051\n",
      "episode:360000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00497\n",
      "episode:360500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00462\n",
      "episode:361000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00464\n",
      "episode:361500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00489\n",
      "episode:362000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00474\n",
      "episode:362500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0048\n",
      "episode:363000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00468\n",
      "episode:363500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00467\n",
      "episode:364000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00485\n",
      "episode:364500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00486\n",
      "episode:365000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00483\n",
      "episode:365500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00503\n",
      "episode:366000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00504\n",
      "episode:366500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00484\n",
      "episode:367000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00518\n",
      "episode:367500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00551\n",
      "episode:368000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00516\n",
      "episode:368500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.0053\n",
      "episode:369000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00557\n",
      "episode:369500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00543\n",
      "episode:370000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00549\n",
      "episode:370500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.00524\n",
      "episode:371000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00599\n",
      "episode:371500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.0056\n",
      "episode:372000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00583\n",
      "episode:372500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00566\n",
      "episode:373000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00559\n",
      "episode:373500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00562\n",
      "episode:374000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00588\n",
      "episode:374500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00544\n",
      "episode:375000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00571\n",
      "episode:375500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00559\n",
      "episode:376000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00517\n",
      "episode:376500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00537\n",
      "episode:377000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00548\n",
      "episode:377500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00541\n",
      "episode:378000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00557\n",
      "episode:378500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.0055\n",
      "episode:379000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00602\n",
      "episode:379500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00604\n",
      "episode:380000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00578\n",
      "episode:380500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00569\n",
      "episode:381000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00568\n",
      "episode:381500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00569\n",
      "episode:382000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00575\n",
      "episode:382500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00598\n",
      "episode:383000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00542\n",
      "episode:383500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00641\n",
      "episode:384000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00587\n",
      "episode:384500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0062\n",
      "episode:385000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00638\n",
      "episode:385500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00582\n",
      "episode:386000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0057\n",
      "episode:386500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00617\n",
      "episode:387000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00581\n",
      "episode:387500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00539\n",
      "episode:388000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00568\n",
      "episode:388500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00581\n",
      "episode:389000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0059\n",
      "episode:389500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0057\n",
      "episode:390000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.006\n",
      "episode:390500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00576\n",
      "episode:391000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0055\n",
      "episode:391500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00546\n",
      "episode:392000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00566\n",
      "episode:392500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00538\n",
      "episode:393000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00528\n",
      "episode:393500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00527\n",
      "episode:394000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00493\n",
      "episode:394500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00526\n",
      "episode:395000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00514\n",
      "episode:395500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0054\n",
      "episode:396000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00521\n",
      "episode:396500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00497\n",
      "episode:397000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00542\n",
      "episode:397500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.005\n",
      "episode:398000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00577\n",
      "episode:398500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00545\n",
      "episode:399000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00514\n",
      "episode:399500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00514\n",
      "episode:400000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00509\n",
      "episode:400500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00557\n",
      "episode:401000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.07 loss:0.00538\n",
      "episode:401500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00507\n",
      "episode:402000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00533\n",
      "episode:402500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00524\n",
      "episode:403000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00521\n",
      "episode:403500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00531\n",
      "episode:404000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00534\n",
      "episode:404500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00572\n",
      "episode:405000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00572\n",
      "episode:405500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0054\n",
      "episode:406000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00558\n",
      "episode:406500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00585\n",
      "episode:407000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00589\n",
      "episode:407500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00623\n",
      "episode:408000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.006\n",
      "episode:408500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00597\n",
      "episode:409000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00589\n",
      "episode:409500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00585\n",
      "episode:410000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00568\n",
      "episode:410500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00602\n",
      "episode:411000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00607\n",
      "episode:411500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00585\n",
      "episode:412000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00592\n",
      "episode:412500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00584\n",
      "episode:413000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0058\n",
      "episode:413500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00582\n",
      "episode:414000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00579\n",
      "episode:414500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00578\n",
      "episode:415000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00566\n",
      "episode:415500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00525\n",
      "episode:416000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0058\n",
      "episode:416500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00536\n",
      "episode:417000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0051\n",
      "episode:417500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.11 loss:0.00536\n",
      "episode:418000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00521\n",
      "episode:418500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0054\n",
      "episode:419000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00528\n",
      "episode:419500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00526\n",
      "episode:420000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00515\n",
      "episode:420500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00515\n",
      "episode:421000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00561\n",
      "episode:421500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00533\n",
      "episode:422000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00548\n",
      "episode:422500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0054\n",
      "episode:423000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00555\n",
      "episode:423500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00562\n",
      "episode:424000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00521\n",
      "episode:424500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00555\n",
      "episode:425000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00548\n",
      "episode:425500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00545\n",
      "episode:426000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00591\n",
      "episode:426500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00554\n",
      "episode:427000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00547\n",
      "episode:427500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00616\n",
      "episode:428000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00606\n",
      "episode:428500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00533\n",
      "episode:429000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00567\n",
      "episode:429500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00556\n",
      "episode:430000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.13 loss:0.00613\n",
      "episode:430500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00637\n",
      "episode:431000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0057\n",
      "episode:431500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00588\n",
      "episode:432000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00609\n",
      "episode:432500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00597\n",
      "episode:433000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00573\n",
      "episode:433500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00614\n",
      "episode:434000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00599\n",
      "episode:434500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00567\n",
      "episode:435000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00573\n",
      "episode:435500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00545\n",
      "episode:436000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.0055\n",
      "episode:436500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00552\n",
      "episode:437000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00545\n",
      "episode:437500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00552\n",
      "episode:438000 epsilon:0.25 mean_reward:-0.998 mean_ep_length:1.1 loss:0.00557\n",
      "episode:438500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00578\n",
      "episode:439000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00554\n",
      "episode:439500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00541\n",
      "episode:440000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00541\n",
      "episode:440500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00564\n",
      "episode:441000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00535\n",
      "episode:441500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00518\n",
      "episode:442000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00548\n",
      "episode:442500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.0051\n",
      "episode:443000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00556\n",
      "episode:443500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00569\n",
      "episode:444000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00543\n",
      "episode:444500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00543\n",
      "episode:445000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00554\n",
      "episode:445500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00578\n",
      "episode:446000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0058\n",
      "episode:446500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0058\n",
      "episode:447000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00568\n",
      "episode:447500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.12 loss:0.00565\n",
      "episode:448000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0055\n",
      "episode:448500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00561\n",
      "episode:449000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00582\n",
      "episode:449500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00549\n",
      "episode:450000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00568\n",
      "episode:450500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00564\n",
      "episode:451000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00559\n",
      "episode:451500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0054\n",
      "episode:452000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0053\n",
      "episode:452500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00547\n",
      "episode:453000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00548\n",
      "episode:453500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00568\n",
      "episode:454000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00554\n",
      "episode:454500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00565\n",
      "episode:455000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.09 loss:0.00564\n",
      "episode:455500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00586\n",
      "episode:456000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00562\n",
      "episode:456500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00547\n",
      "episode:457000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0058\n",
      "episode:457500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00569\n",
      "episode:458000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00597\n",
      "episode:458500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00572\n",
      "episode:459000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00593\n",
      "episode:459500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00589\n",
      "episode:460000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00653\n",
      "episode:460500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00607\n",
      "episode:461000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00642\n",
      "episode:461500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00668\n",
      "episode:462000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00645\n",
      "episode:462500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00624\n",
      "episode:463000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00608\n",
      "episode:463500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00589\n",
      "episode:464000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00612\n",
      "episode:464500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00579\n",
      "episode:465000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00644\n",
      "episode:465500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00636\n",
      "episode:466000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00658\n",
      "episode:466500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00637\n",
      "episode:467000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00599\n",
      "episode:467500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00601\n",
      "episode:468000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00626\n",
      "episode:468500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00597\n",
      "episode:469000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00593\n",
      "episode:469500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00581\n",
      "episode:470000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00601\n",
      "episode:470500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.05 loss:0.00558\n",
      "episode:471000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00565\n",
      "episode:471500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00565\n",
      "episode:472000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.0059\n",
      "episode:472500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00552\n",
      "episode:473000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.00572\n",
      "episode:473500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00564\n",
      "episode:474000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.13 loss:0.00551\n",
      "episode:474500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00529\n",
      "episode:475000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00544\n",
      "episode:475500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00515\n",
      "episode:476000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00583\n",
      "episode:476500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00522\n",
      "episode:477000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00516\n",
      "episode:477500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00597\n",
      "episode:478000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00574\n",
      "episode:478500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.11 loss:0.00556\n",
      "episode:479000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.0058\n",
      "episode:479500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.0055\n",
      "episode:480000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00557\n",
      "episode:480500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0057\n",
      "episode:481000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00583\n",
      "episode:481500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00571\n",
      "episode:482000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00589\n",
      "episode:482500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00594\n",
      "episode:483000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00586\n",
      "episode:483500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00562\n",
      "episode:484000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00559\n",
      "episode:484500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00557\n",
      "episode:485000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00589\n",
      "episode:485500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.06 loss:0.00554\n",
      "episode:486000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00579\n",
      "episode:486500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00532\n",
      "episode:487000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0054\n",
      "episode:487500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00525\n",
      "episode:488000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00531\n",
      "episode:488500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00515\n",
      "episode:489000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00511\n",
      "episode:489500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00511\n",
      "episode:490000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00509\n",
      "episode:490500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.05 loss:0.00492\n",
      "episode:491000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00485\n",
      "episode:491500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00483\n",
      "episode:492000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.0051\n",
      "episode:492500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.1 loss:0.00513\n",
      "episode:493000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00484\n",
      "episode:493500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.08 loss:0.0053\n",
      "episode:494000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00495\n",
      "episode:494500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00473\n",
      "episode:495000 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.1 loss:0.00536\n",
      "episode:495500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.06 loss:0.00498\n",
      "episode:496000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.0052\n",
      "episode:496500 epsilon:0.25 mean_reward:-0.999 mean_ep_length:1.07 loss:0.00532\n",
      "episode:497000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00548\n",
      "episode:497500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00533\n",
      "episode:498000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00544\n",
      "episode:498500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.09 loss:0.00563\n",
      "episode:499000 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.08 loss:0.00552\n",
      "episode:499500 epsilon:0.25 mean_reward:-1.0 mean_ep_length:1.07 loss:0.00597\n",
      "closing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    # obs = env.reset()\n",
    "    env.board.reset()\n",
    "    obs, _, _, _ = env.step(None)\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode=\"human\")\n",
    "        action = TrainNet.get_action(obs, 0)\n",
    "        print(\">>{}\".format(env.action_index_to_uci(action)))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<b>Move 8 White:</b><br/><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><defs><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark lastmove b4\" stroke=\"none\" fill=\"#aaa23b\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light lastmove b5\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(60, 330)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 330)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(150, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(285, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(330, 330)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(15, 285)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(60, 285)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 285)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(195, 285)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(330, 285)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(150, 240)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(105, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(150, 195)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(60, 150)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(285, 15)\" /></svg>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>a5f4\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save and Reload the Model Weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "TrainNet.model.save_weights('Training/SavedModels/train_100k_net-025-1e-5_no-neg-reward.h5f', overwrite=True)\n",
    "TargetNet.model.save_weights('Training/SavedModels/target_100k_net-025-1e-5_no-neg-reward.h5f', overwrite=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delete the current models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "del TrainNet.model\n",
    "del TargetNet.model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).output_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).output_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).hidden_layers.0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).hidden_layers.0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).hidden_layers.1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).hidden_layers.1.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "load the model from memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "TrainNet.model = CustomModel(TrainNet.shape_states, [256, 256], TrainNet.shape_actions)\n",
    "TrainNet.model.load_weights('Training/SavedModels/train_100k_net-025-1e-5_no-neg-reward.h5f')\n",
    "TargetNet.model = CustomModel(TargetNet.shape_states, [256, 256], TargetNet.shape_actions)\n",
    "TargetNet.model.load_weights('Training/SavedModels/target_100k_net-025-1e-5_no-neg-reward.h5f')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9ea02fb430>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea16cf9687884ff056c50def91fea02e8f072861cbc27c56850047f3dd11ae6d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}