{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4997ee",
   "metadata": {},
   "source": [
    "# Reinforcement Learning mit Python - Running Kings Schach KI\n",
    "## Aufgabenstellung\n",
    "\n",
    "Erstellen  Sie  Modelle,  welche  das  Spiel  Racing  Kings erlernen.  Am  Ende  sollte  ein  Spiel  gegen  den erstellten  Algorithmus  möglich  sein.  Testen  Sie  als  Gegner  einenAlgorithmus,  welcher  Zufallszüge ausführt,  und  bewerten  Sie  ihren  Algorithmus. Nutzen  Sie  die  chess-Bibliothek  (https://python-chess.readthedocs.io/en/latest/)\n",
    "\n",
    "Bonus: Vergleichen Sie einen modellfreien und einen modellbasierten Ansatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72898e6",
   "metadata": {},
   "source": [
    "Diese Anleitung wurde verwendet um zügig mit der verwendung der chess bibliothek starten zu können.\n",
    "https://jupyter.brynmawr.edu/services/public/dblank/CS371%20Cognitive%20Science/2016-Fall/Programming%20a%20Chess%20Player.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beeb057",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33e9b18",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (1.18.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: tensorflow==2.3.0 in ./env/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.17.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.38.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: keras in ./env/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./env/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.8/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in ./env/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in ./env/lib/python3.8/site-packages (from keras) (2.10.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from h5py->keras) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: keras-rl2 in ./env/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in ./env/lib/python3.8/site-packages (from keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.36.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.38.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.17.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.4.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.32.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: chess in ./env/lib/python3.8/site-packages (1.5.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pydot in ./env/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in ./env/lib/python3.8/site-packages (from pydot) (2.4.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: gym in ./env/lib/python3.8/site-packages (0.18.3)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in ./env/lib/python3.8/site-packages (from gym) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in ./env/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in ./env/lib/python3.8/site-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in ./env/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.8/site-packages (from gym) (1.4.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: stable_baselines3 in ./env/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.8/site-packages (from stable_baselines3) (1.3.0)\n",
      "Requirement already satisfied: gym>=0.17 in ./env/lib/python3.8/site-packages (from stable_baselines3) (0.18.3)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (from stable_baselines3) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in ./env/lib/python3.8/site-packages (from stable_baselines3) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in ./env/lib/python3.8/site-packages (from stable_baselines3) (1.9.0)\n",
      "Requirement already satisfied: cloudpickle in ./env/lib/python3.8/site-packages (from stable_baselines3) (1.6.0)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3) (1.4.1)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in ./env/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3) (8.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in ./env/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3) (1.5.15)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.8/site-packages (from torch>=1.4.0->stable_baselines3) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./env/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (2.8.1)\n",
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->stable_baselines3) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./env/lib/python3.8/site-packages (from pandas->stable_baselines3) (2021.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ip (/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/armin/CloudStation/06_hochschule/02_MAPR/04_vorlesungen/06_einführung_in_die_ki_mit_python/code/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install tensorflow==2.3.0\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install chess\n",
    "!pip install pydot\n",
    "!pip install gym\n",
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51a2de",
   "metadata": {},
   "source": [
    "# Create chess environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1965ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_board(board, use_svg):\n",
    "    if use_svg:\n",
    "        return board._repr_svg_()\n",
    "    else:\n",
    "        return \"<pre>\" + str(board) + \"</pre>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1f075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who(player):\n",
    "    return \"White\" if player == chess.WHITE else \"Black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "605e81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.variant\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "\n",
    "N_DISCRETE_ACTIONS = 4096\n",
    "\n",
    "# from Learning_Chess pdf\n",
    "class RacingKingsEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(RacingKingsEnvironment, self).__init__()\n",
    "        self.board = chess.variant.RacingKingsBoard()\n",
    "        self.reward = 0\n",
    "        self.action_space = Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(14, 8, 8), dtype=np.uint8)\n",
    "    def board_square_to_index(self, name):\n",
    "        return (int(name[1])-1) * 8 + (ord(name[0])-97) \n",
    "    def action_index_to_uci(self, index):\n",
    "        index_from = index//64\n",
    "        index_to = index%64\n",
    "        name = chr(index_from%8 + 97) + str(index_from//8 +1) + chr(index_to%8 + 97) + str(index_to//8 + 1)\n",
    "        return name\n",
    "    def action_uci_to_index(self, uci):\n",
    "        index_from = (int(uci[1])-1) * 8 + (ord(uci[0])-97) \n",
    "        index_to = (int(uci[3])-1) * 8 + (ord(uci[2])-97) \n",
    "        return index_from*64 + index_to\n",
    "    @property\n",
    "    def actions(self):\n",
    "        moves = list(self.board.legal_moves)\n",
    "        moves_string = []\n",
    "        for move in moves:\n",
    "            moves_string.append(move.uci())\n",
    "        boardActions = np.zeros(N_DISCRETE_ACTIONS, dtype=np.int8)\n",
    "        for move in moves_string:\n",
    "            boardActions[self.board_square_to_index(move[0:2])*64 + self.board_square_to_index(move[2:4])] = 1\n",
    "        return boardActions\n",
    "    @property\n",
    "    def states(self):\n",
    "        boardState = np.zeros((14, 8, 8), dtype=np.int8)\n",
    "        for piece in chess.PIECE_TYPES:\n",
    "            for square in self.board.pieces(piece, chess.WHITE):\n",
    "                idNum = square//8\n",
    "                idAlph = square%8\n",
    "                boardState[piece - 1][7 - idNum][idAlph] = 1\n",
    "            for square in self.board.pieces(piece, chess.BLACK):\n",
    "                idNum = square//8\n",
    "                idAlph = square%8\n",
    "                boardState[piece + 5][7 - idNum][idAlph] = 1        \n",
    "        \n",
    "            aux = self.board.turn\n",
    "            self.board.turn = chess.WHITE\n",
    "            for move in list(self.board.legal_moves):\n",
    "                square = self.board_square_to_index(move.uci())\n",
    "                idNum = square//8\n",
    "                idAlph = square%8\n",
    "                boardState[12][7 - idNum][idAlph] = 1\n",
    "            self.board.turn = chess.BLACK\n",
    "            for move in list(self.board.legal_moves):\n",
    "                square = self.board_square_to_index(move.uci())\n",
    "                idNum = square//8\n",
    "                idAlph = square%8\n",
    "                boardState[13][7 - idNum][idAlph] = 1\n",
    "            self.board.turn = aux\n",
    "        return boardState\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        step_reward = 0\n",
    "        info = {}\n",
    "        \n",
    "        # check if it is not your turn -> then make a random move\n",
    "        # when uncommenting this code the ai will play moves for both players\n",
    "        if self.board.turn == chess.WHITE:\n",
    "            try:\n",
    "                self.board.push(random.choice(list(self.board.legal_moves)))\n",
    "                step_reward += 0\n",
    "                info = {\"msg\":\"White Did a valid move\"}\n",
    "            except:\n",
    "                info = {\"msg\":\"Passed an already finished board\"} \n",
    "                done = True\n",
    "           \n",
    "\n",
    "        if not self.board.is_game_over(claim_draw=True):\n",
    "            if action is not None:\n",
    "                try:\n",
    "                    self.board.push_uci(self.action_index_to_uci(action))\n",
    "                    step_reward += 0.1\n",
    "                    info = {\"msg\":\"Did a valid move\"}\n",
    "                except:\n",
    "                    step_reward -= 1\n",
    "                    info = {\"msg\":\"Action is not a valid move\"} \n",
    "                    done = True\n",
    "                if self.board.is_variant_end():\n",
    "                    if who(not self.board.turn) == \"Black\":\n",
    "                        step_reward += 100\n",
    "                        info = {\"msg\":\"AI won the game!\"} \n",
    "                        done = True\n",
    "                    else:\n",
    "                        step_reward -= 100\n",
    "                        info = {\"msg\":\"Opponent won the game!\"} \n",
    "                        done = True\n",
    "                    info = {\"msg\":\"racing kings: \" + who(not self.board.turn) + \" wins!\"}\n",
    "        else:\n",
    "            step_reward -=10\n",
    "            done = True\n",
    "            info = {\"msg\":\"game over\"}\n",
    "\n",
    "        self.reward += step_reward\n",
    "        \n",
    "        return self.states, step_reward, done, info\n",
    "    def reset(self):\n",
    "        # reset the board\n",
    "        self.board.reset()\n",
    "        # play random amount of actions\n",
    "        for i in range((random.randint(0, 30)*2)):\n",
    "            try:\n",
    "                move = random.choice(list(self.board.legal_moves))\n",
    "                self.board.push(move)\n",
    "            except:\n",
    "                self.board.reset()\n",
    "                \n",
    "        self.reward = 0.0\n",
    "        return self.step(None)[0]  # reward, done, info can't be included\n",
    "    def render(self, mode=\"human\", pause=0.2):\n",
    "        name = who(self.board.turn)\n",
    "        use_svg = (mode == \"human\")\n",
    "        board_stop = display_board(self.board, use_svg)\n",
    "        html = \"<b>Move %s %s:</b><br/>%s\" % (\n",
    "                    len(self.board.move_stack), name, board_stop)\n",
    "        if mode is not None:\n",
    "                if mode == \"human\":\n",
    "                    clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                if mode == \"human\":\n",
    "                    time.sleep(pause)\n",
    "    def close (self):\n",
    "        print(\"closing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01267760",
   "metadata": {},
   "source": [
    "# Test Chess environment randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf0b4349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:2 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:3 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:4 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:5 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:6 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:7 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:8 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:9 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "closing\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "env = RacingKingsEnvironment()\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2ae01",
   "metadata": {},
   "source": [
    "# Training Model with stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98e796a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "from stable_baselines3.common.callbacks import EvalCallback \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9ade570",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff2c7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = RacingKingsEnvironment()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfc48574",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_save_path = os.path.join('Training', 'SavedModels', 'Best_Racing_Kings')\n",
    "eval_callback = EvalCallback(env, eval_freq=2000, best_model_save_path = best_save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06806a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_31\n",
      "Eval num_timesteps=2000, episode_reward=-0.70 +/- 0.37\n",
      "Episode length: 4.00 +/- 3.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 50   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 40   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=0.12 +/- 1.37\n",
      "Episode length: 12.20 +/- 13.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.2       |\n",
      "|    mean_reward          | 0.12       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08392431 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.599     |\n",
      "|    explained_variance   | 0.194      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    value_loss           | 0.0734     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 48   |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 84   |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.66 +/- 0.28\n",
      "Episode length: 4.40 +/- 2.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | -0.66       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050214585 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.0613      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.335       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 43   |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 140  |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.54 +/- 0.43\n",
      "Episode length: 5.60 +/- 4.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.6        |\n",
      "|    mean_reward          | -0.54      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 8000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07547584 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.575     |\n",
      "|    explained_variance   | -0.0843    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00834   |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    value_loss           | 0.0922     |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 44   |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 183  |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.58 +/- 0.37\n",
      "Episode length: 5.20 +/- 3.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08385411 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.508     |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 4930       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 226   |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.46 +/- 0.55\n",
      "Episode length: 6.40 +/- 5.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09098716 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.559     |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 270   |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-0.66 +/- 0.44\n",
      "Episode length: 4.40 +/- 4.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | -0.66      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06906611 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.563     |\n",
      "|    explained_variance   | 0.171      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 4950       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.268      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 309   |\n",
      "|    total_timesteps | 14336 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=0.34 +/- 1.28\n",
      "Episode length: 14.40 +/- 12.78\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 14.4       |\n",
      "|    mean_reward          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08095586 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.56      |\n",
      "|    explained_variance   | 0.0866     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    value_loss           | 0.262      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 354   |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.68 +/- 0.25\n",
      "Episode length: 4.20 +/- 2.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 18000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07565628 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.52      |\n",
      "|    explained_variance   | 0.135      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00248   |\n",
      "|    n_updates            | 4970       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    value_loss           | 0.252      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 402   |\n",
      "|    total_timesteps | 18432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-0.88 +/- 0.12\n",
      "Episode length: 2.20 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08856416 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.656     |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0235    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 45    |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 445   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-0.36 +/- 0.72\n",
      "Episode length: 7.40 +/- 7.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08561969 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.601     |\n",
      "|    explained_variance   | 0.178      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00171    |\n",
      "|    n_updates            | 4990       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 487   |\n",
      "|    total_timesteps | 22528 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 6.60 +/- 10.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | -0.44       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074689865 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | 0.0831      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.542       |\n",
      "|    n_updates            | 5000        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    value_loss           | 0.455       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 529   |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-0.46 +/- 0.60\n",
      "Episode length: 6.40 +/- 5.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06794501 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.603     |\n",
      "|    explained_variance   | 0.161      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0483    |\n",
      "|    n_updates            | 5010       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    value_loss           | 0.395      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 569   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-0.24 +/- 0.74\n",
      "Episode length: 8.60 +/- 7.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.6        |\n",
      "|    mean_reward          | -0.24      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06691249 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 0.0241     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 0.483      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 609   |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=0.02 +/- 1.23\n",
      "Episode length: 11.20 +/- 12.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.2       |\n",
      "|    mean_reward          | 0.02       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08331606 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.69      |\n",
      "|    explained_variance   | 0.309      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0237    |\n",
      "|    n_updates            | 5030       |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 46    |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 653   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-0.70 +/- 0.19\n",
      "Episode length: 4.00 +/- 1.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09490877 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.763     |\n",
      "|    explained_variance   | -0.231     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 695   |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-0.60 +/- 0.26\n",
      "Episode length: 5.00 +/- 2.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 34000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06998316 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | 0.239      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 5050       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.227      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 735   |\n",
      "|    total_timesteps | 34816 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.66 +/- 0.28\n",
      "Episode length: 4.40 +/- 2.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | -0.66      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06287682 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.655     |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0313    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 776   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-0.84 +/- 0.16\n",
      "Episode length: 2.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065038055 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | -0.0307     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 5070        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 819   |\n",
      "|    total_timesteps | 38912 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.76 +/- 0.15\n",
      "Episode length: 3.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12905832 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.745     |\n",
      "|    explained_variance   | -0.0173    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 859   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=0.20 +/- 1.18\n",
      "Episode length: 13.00 +/- 11.82\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 13        |\n",
      "|    mean_reward          | 0.2       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 42000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0791993 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.671    |\n",
      "|    explained_variance   | 0.128     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0336   |\n",
      "|    n_updates            | 5090      |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 899   |\n",
      "|    total_timesteps | 43008 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-0.16 +/- 0.76\n",
      "Episode length: 9.40 +/- 7.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086343765 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 5100        |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 940   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-0.88 +/- 0.19\n",
      "Episode length: 2.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 46000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09054738 |\n",
      "|    clip_fraction        | 0.252      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.669     |\n",
      "|    explained_variance   | -0.0909    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00144    |\n",
      "|    n_updates            | 5110       |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 982   |\n",
      "|    total_timesteps | 47104 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-0.68 +/- 0.35\n",
      "Episode length: 4.20 +/- 3.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | -0.68       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067468524 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.697      |\n",
      "|    explained_variance   | -0.0466     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 5120        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    value_loss           | 0.338       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 1023  |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.84 +/- 0.05\n",
      "Episode length: 2.60 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 2.6       |\n",
      "|    mean_reward          | -0.84     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0729801 |\n",
      "|    clip_fraction        | 0.242     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.682    |\n",
      "|    explained_variance   | 0.189     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 5130      |\n",
      "|    policy_gradient_loss | -0.0398   |\n",
      "|    value_loss           | 0.21      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 1062  |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=0.94 +/- 2.14\n",
      "Episode length: 20.40 +/- 21.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | 0.94        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053522736 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.0141      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 5140        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.693       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 1103  |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.64 +/- 0.37\n",
      "Episode length: 4.60 +/- 3.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 54000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06949483 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.694     |\n",
      "|    explained_variance   | -0.0396    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0143    |\n",
      "|    n_updates            | 5150       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    value_loss           | 0.272      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 1142  |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-0.50 +/- 0.81\n",
      "Episode length: 6.00 +/- 8.07\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 56000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05543275 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.705     |\n",
      "|    explained_variance   | -0.0448    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0527     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    value_loss           | 0.352      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 1182  |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=0.98 +/- 3.31\n",
      "Episode length: 20.80 +/- 33.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.8       |\n",
      "|    mean_reward          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 58000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06597545 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.691     |\n",
      "|    explained_variance   | -0.026     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.147      |\n",
      "|    n_updates            | 5170       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.622      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 1227  |\n",
      "|    total_timesteps | 59392 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104639895 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | -0.0964     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 5180        |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 1265  |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-0.56 +/- 0.34\n",
      "Episode length: 5.40 +/- 3.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 62000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08570939 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.764     |\n",
      "|    explained_variance   | 0.152      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 5190       |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 1310  |\n",
      "|    total_timesteps | 63488 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=0.24 +/- 1.74\n",
      "Episode length: 13.40 +/- 17.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | 0.24        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075657964 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.046       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 5200        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 0.309       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 1353  |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-0.50 +/- 0.76\n",
      "Episode length: 6.00 +/- 7.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07585886 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.867     |\n",
      "|    explained_variance   | 0.205      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0328    |\n",
      "|    n_updates            | 5210       |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 1397  |\n",
      "|    total_timesteps | 67584 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-0.60 +/- 0.24\n",
      "Episode length: 5.00 +/- 2.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 68000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06875014 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.9       |\n",
      "|    explained_variance   | 0.0251     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0113     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    value_loss           | 0.316      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 1438  |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=0.70 +/- 1.96\n",
      "Episode length: 18.00 +/- 19.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 18        |\n",
      "|    mean_reward          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 70000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0712982 |\n",
      "|    clip_fraction        | 0.245     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.813    |\n",
      "|    explained_variance   | 0.135     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0501   |\n",
      "|    n_updates            | 5230      |\n",
      "|    policy_gradient_loss | -0.0365   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 1482  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-0.88 +/- 0.10\n",
      "Episode length: 2.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.2         |\n",
      "|    mean_reward          | -0.88       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074813426 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.933      |\n",
      "|    explained_variance   | -0.0404     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000302    |\n",
      "|    n_updates            | 5240        |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 0.301       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 1527  |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-0.08 +/- 1.11\n",
      "Episode length: 10.20 +/- 11.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -0.08       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055462584 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.888      |\n",
      "|    explained_variance   | 0.0604      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.458       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 1568  |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-0.42 +/- 0.79\n",
      "Episode length: 6.80 +/- 7.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08077634 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.768     |\n",
      "|    explained_variance   | -0.0284    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 1610  |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=0.34 +/- 2.15\n",
      "Episode length: 14.40 +/- 21.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 14.4       |\n",
      "|    mean_reward          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 78000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07089241 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.86      |\n",
      "|    explained_variance   | 0.127      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.138      |\n",
      "|    n_updates            | 5270       |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.392      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 1653  |\n",
      "|    total_timesteps | 79872 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.50 +/- 0.49\n",
      "Episode length: 6.00 +/- 4.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08352874 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.019      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0068    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    value_loss           | 0.243      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 1695  |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-0.56 +/- 0.55\n",
      "Episode length: 5.40 +/- 5.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | -0.56       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071525134 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 5290        |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 1740  |\n",
      "|    total_timesteps | 83968 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-0.74 +/- 0.42\n",
      "Episode length: 3.60 +/- 4.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | -0.74       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069380544 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 5300        |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-0.22 +/- 1.03\n",
      "Episode length: 8.80 +/- 10.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -0.22    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 86000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 1782  |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-0.08 +/- 1.64\n",
      "Episode length: 10.20 +/- 16.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -0.08       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 88000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071436316 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 1824  |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.90 +/- 0.13\n",
      "Episode length: 2.00 +/- 1.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 90000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07030664 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.75      |\n",
      "|    explained_variance   | -0.0281    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0403     |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.387      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 1871  |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-0.08 +/- 1.31\n",
      "Episode length: 10.20 +/- 13.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 92000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07985588 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.832     |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00892    |\n",
      "|    n_updates            | 5330       |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 1915  |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-0.68 +/- 0.28\n",
      "Episode length: 4.20 +/- 2.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 94000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08463746 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | 0.274      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 1962  |\n",
      "|    total_timesteps | 94208 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-0.54 +/- 0.32\n",
      "Episode length: 5.60 +/- 3.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.6        |\n",
      "|    mean_reward          | -0.54      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 96000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07950488 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.913     |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 5350       |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 47    |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 2006  |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-0.62 +/- 0.50\n",
      "Episode length: 4.80 +/- 4.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 98000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07301647 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.939     |\n",
      "|    explained_variance   | 0.0434     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0179     |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 48    |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 2044  |\n",
      "|    total_timesteps | 98304 |\n",
      "------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.88 +/- 0.15\n",
      "Episode length: 2.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.2         |\n",
      "|    mean_reward          | -0.88       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060018085 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.879      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 2080   |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 102000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06250611 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | 0.0945     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00613    |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    value_loss           | 0.299      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 50     |\n",
      "|    time_elapsed    | 2116   |\n",
      "|    total_timesteps | 102400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-0.94 +/- 0.05\n",
      "Episode length: 1.60 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.6         |\n",
      "|    mean_reward          | -0.94       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104945615 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000499   |\n",
      "|    n_updates            | 5390        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 51     |\n",
      "|    time_elapsed    | 2155   |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-0.34 +/- 0.80\n",
      "Episode length: 7.60 +/- 8.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.6        |\n",
      "|    mean_reward          | -0.34      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 106000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07145993 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.979     |\n",
      "|    explained_variance   | 0.275      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 2198   |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-2.28 +/- 2.86\n",
      "Episode length: 6.20 +/- 7.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -2.28      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 108000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08508283 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.973     |\n",
      "|    explained_variance   | 0.101      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0251     |\n",
      "|    n_updates            | 5410       |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.147      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 2243   |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-0.40 +/- 0.72\n",
      "Episode length: 7.00 +/- 7.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7          |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 110000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07677752 |\n",
      "|    clip_fraction        | 0.252      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.78      |\n",
      "|    explained_variance   | 0.00371    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 2283   |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-0.72 +/- 0.25\n",
      "Episode length: 3.80 +/- 2.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 112000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07383747 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | -0.00122   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 5430       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.34       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 55     |\n",
      "|    time_elapsed    | 2328   |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-0.44 +/- 0.77\n",
      "Episode length: 6.60 +/- 7.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | -0.44      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 114000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08099754 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.833     |\n",
      "|    explained_variance   | 0.188      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0053     |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 56     |\n",
      "|    time_elapsed    | 2371   |\n",
      "|    total_timesteps | 114688 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-0.52 +/- 0.40\n",
      "Episode length: 5.80 +/- 3.97\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 116000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10435447 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.885     |\n",
      "|    explained_variance   | 0.161      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 5450       |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 2412   |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=0.28 +/- 2.22\n",
      "Episode length: 13.80 +/- 22.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.8       |\n",
      "|    mean_reward          | 0.28       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 118000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07401813 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.825     |\n",
      "|    explained_variance   | -0.0254    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    value_loss           | 0.403      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 58     |\n",
      "|    time_elapsed    | 2458   |\n",
      "|    total_timesteps | 118784 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-0.92 +/- 0.07\n",
      "Episode length: 1.80 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 120000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06776708 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.871     |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00419   |\n",
      "|    n_updates            | 5470       |\n",
      "|    policy_gradient_loss | -0.0313    |\n",
      "|    value_loss           | 0.294      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 2505   |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-0.42 +/- 0.71\n",
      "Episode length: 6.80 +/- 7.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 122000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09194073 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | -0.0533    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0221    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 60     |\n",
      "|    time_elapsed    | 2545   |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-0.58 +/- 0.60\n",
      "Episode length: 5.20 +/- 5.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 124000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08331713 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.902     |\n",
      "|    explained_variance   | -0.203     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00161    |\n",
      "|    n_updates            | 5490       |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    value_loss           | 0.626      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 61     |\n",
      "|    time_elapsed    | 2585   |\n",
      "|    total_timesteps | 124928 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=0.92 +/- 1.67\n",
      "Episode length: 20.20 +/- 16.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | 0.92        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 126000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071532235 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | -0.311      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00386    |\n",
      "|    n_updates            | 5500        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.26        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 2629   |\n",
      "|    total_timesteps | 126976 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-0.82 +/- 0.15\n",
      "Episode length: 2.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 128000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07368089 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | -0.116     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.5        |\n",
      "|    n_updates            | 5510       |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    value_loss           | 0.407      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 2670   |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-0.80 +/- 0.17\n",
      "Episode length: 3.00 +/- 1.67\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3         |\n",
      "|    mean_reward          | -0.8      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 130000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0682967 |\n",
      "|    clip_fraction        | 0.252     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.974    |\n",
      "|    explained_variance   | 0.0488    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00458   |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | -0.0302   |\n",
      "|    value_loss           | 0.429     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 2712   |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-2.48 +/- 3.47\n",
      "Episode length: 4.20 +/- 2.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | -2.48       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077270895 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.0691      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 5530        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 0.303       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 65     |\n",
      "|    time_elapsed    | 2753   |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-0.56 +/- 0.73\n",
      "Episode length: 5.40 +/- 7.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 134000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08551276 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.92      |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    value_loss           | 0.413      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 2794   |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-0.16 +/- 0.57\n",
      "Episode length: 9.40 +/- 5.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.4        |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 136000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08017954 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.98      |\n",
      "|    explained_variance   | 0.144      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0278     |\n",
      "|    n_updates            | 5550       |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 67     |\n",
      "|    time_elapsed    | 2836   |\n",
      "|    total_timesteps | 137216 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 138000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08104113 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.927     |\n",
      "|    explained_variance   | 0.22       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00258   |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    value_loss           | 0.22       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 68     |\n",
      "|    time_elapsed    | 2877   |\n",
      "|    total_timesteps | 139264 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-0.30 +/- 0.70\n",
      "Episode length: 8.00 +/- 7.01\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09373152 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | 0.226      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 5570       |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 2915   |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-0.88 +/- 0.19\n",
      "Episode length: 2.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 142000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07906969 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.835     |\n",
      "|    explained_variance   | -0.00213   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.403      |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    value_loss           | 0.644      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 70     |\n",
      "|    time_elapsed    | 2954   |\n",
      "|    total_timesteps | 143360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 144000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06696971 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.896     |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00254   |\n",
      "|    n_updates            | 5590       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.4        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 2995   |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=0.10 +/- 2.00\n",
      "Episode length: 12.00 +/- 20.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 146000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086172536 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | -0.156      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 5600        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.386       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 72     |\n",
      "|    time_elapsed    | 3037   |\n",
      "|    total_timesteps | 147456 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-0.52 +/- 0.66\n",
      "Episode length: 5.80 +/- 6.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 148000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09844376 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.998     |\n",
      "|    explained_variance   | 0.12       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 5610       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 3075   |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-0.24 +/- 0.93\n",
      "Episode length: 8.60 +/- 9.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -0.24       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077756666 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.869      |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 5620        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 3117   |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-0.02 +/- 1.52\n",
      "Episode length: 10.80 +/- 15.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.8       |\n",
      "|    mean_reward          | -0.02      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 152000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07563406 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.209      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 5630       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 0.298      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 75     |\n",
      "|    time_elapsed    | 3160   |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-0.84 +/- 0.10\n",
      "Episode length: 2.60 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 154000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10627711 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | 0.131      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0148    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 0.299      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 3202   |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-0.92 +/- 0.16\n",
      "Episode length: 1.80 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 156000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08745633 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.171      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 5650       |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 77     |\n",
      "|    time_elapsed    | 3244   |\n",
      "|    total_timesteps | 157696 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=0.42 +/- 1.94\n",
      "Episode length: 15.20 +/- 19.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.2       |\n",
      "|    mean_reward          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 158000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07474124 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 78     |\n",
      "|    time_elapsed    | 3288   |\n",
      "|    total_timesteps | 159744 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-0.08 +/- 1.69\n",
      "Episode length: 10.20 +/- 16.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09435951 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.0852     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 5670       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 3328   |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-0.94 +/- 0.05\n",
      "Episode length: 1.60 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 162000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05639546 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.919     |\n",
      "|    explained_variance   | -0.0768    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    value_loss           | 0.525      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 80     |\n",
      "|    time_elapsed    | 3368   |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-0.78 +/- 0.19\n",
      "Episode length: 3.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 164000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08233106 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.845     |\n",
      "|    explained_variance   | 0.0387     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00993   |\n",
      "|    n_updates            | 5690       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    value_loss           | 0.474      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 3409   |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-0.64 +/- 4.64\n",
      "Episode length: 22.60 +/- 25.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 166000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08631556 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.905     |\n",
      "|    explained_variance   | -0.0598    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 82     |\n",
      "|    time_elapsed    | 3449   |\n",
      "|    total_timesteps | 167936 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-0.48 +/- 0.64\n",
      "Episode length: 6.20 +/- 6.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 168000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07656573 |\n",
      "|    clip_fraction        | 0.252      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.835     |\n",
      "|    explained_variance   | -0.0207    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00619   |\n",
      "|    n_updates            | 5710       |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    value_loss           | 0.413      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 3489   |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-0.52 +/- 0.71\n",
      "Episode length: 5.80 +/- 7.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5.8       |\n",
      "|    mean_reward          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 170000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0762824 |\n",
      "|    clip_fraction        | 0.257     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.848    |\n",
      "|    explained_variance   | 0.142     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00651   |\n",
      "|    n_updates            | 5720      |\n",
      "|    policy_gradient_loss | -0.0342   |\n",
      "|    value_loss           | 0.272     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=0.14 +/- 1.36\n",
      "Episode length: 12.40 +/- 13.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 0.14     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 172000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 3529   |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 174000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09325864 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.814     |\n",
      "|    explained_variance   | 0.125      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 5730       |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 85     |\n",
      "|    time_elapsed    | 3573   |\n",
      "|    total_timesteps | 174080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-0.82 +/- 0.15\n",
      "Episode length: 2.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 176000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08711049 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.0941     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0517    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.376      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 3613   |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-0.06 +/- 1.25\n",
      "Episode length: 10.40 +/- 12.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.4       |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 178000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07095325 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.818     |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 5750       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.319      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 87     |\n",
      "|    time_elapsed    | 3653   |\n",
      "|    total_timesteps | 178176 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=0.52 +/- 1.19\n",
      "Episode length: 16.20 +/- 11.94\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 16.2      |\n",
      "|    mean_reward          | 0.52      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 180000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0910554 |\n",
      "|    clip_fraction        | 0.28      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.858    |\n",
      "|    explained_variance   | -0.0244   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0326   |\n",
      "|    n_updates            | 5760      |\n",
      "|    policy_gradient_loss | -0.0441   |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 3694   |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-0.84 +/- 0.20\n",
      "Episode length: 2.60 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 182000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08996229 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.0832     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 5770       |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.322      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 89     |\n",
      "|    time_elapsed    | 3734   |\n",
      "|    total_timesteps | 182272 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-0.80 +/- 0.25\n",
      "Episode length: 3.00 +/- 2.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 184000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078745574 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.895      |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00107    |\n",
      "|    n_updates            | 5780        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.404       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 90     |\n",
      "|    time_elapsed    | 3775   |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-2.50 +/- 3.40\n",
      "Episode length: 4.00 +/- 2.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -2.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 186000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08539398 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.969     |\n",
      "|    explained_variance   | -0.0446    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000585  |\n",
      "|    n_updates            | 5790       |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 3814   |\n",
      "|    total_timesteps | 186368 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-0.82 +/- 0.10\n",
      "Episode length: 2.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | -0.82       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 188000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082952976 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.0954      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0323     |\n",
      "|    n_updates            | 5800        |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 92     |\n",
      "|    time_elapsed    | 3853   |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=0.10 +/- 1.56\n",
      "Episode length: 12.00 +/- 15.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12         |\n",
      "|    mean_reward          | 0.1        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 190000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07898917 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.905     |\n",
      "|    explained_variance   | 0.0176     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 5810       |\n",
      "|    policy_gradient_loss | -0.0383    |\n",
      "|    value_loss           | 0.391      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 3893   |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=0.06 +/- 1.34\n",
      "Episode length: 11.60 +/- 13.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.6       |\n",
      "|    mean_reward          | 0.06       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 192000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07380366 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.876     |\n",
      "|    explained_variance   | 0.174      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 94     |\n",
      "|    time_elapsed    | 3932   |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-0.90 +/- 0.09\n",
      "Episode length: 2.00 +/- 0.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 194000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070312664 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0167      |\n",
      "|    n_updates            | 5830        |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.411       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 95     |\n",
      "|    time_elapsed    | 3973   |\n",
      "|    total_timesteps | 194560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-0.64 +/- 0.48\n",
      "Episode length: 4.60 +/- 4.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | -0.64       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074598126 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 5840        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 4011   |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=0.42 +/- 2.79\n",
      "Episode length: 15.20 +/- 27.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.2        |\n",
      "|    mean_reward          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 198000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050457254 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.771      |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 5850        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.639       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 97     |\n",
      "|    time_elapsed    | 4050   |\n",
      "|    total_timesteps | 198656 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-0.46 +/- 0.93\n",
      "Episode length: 6.40 +/- 9.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 200000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04598801 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.659     |\n",
      "|    explained_variance   | 0.0389     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0551     |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.62       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 4086   |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-0.82 +/- 0.31\n",
      "Episode length: 2.80 +/- 3.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 202000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07738386 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | -0.0444    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0126    |\n",
      "|    n_updates            | 5870       |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 99     |\n",
      "|    time_elapsed    | 4124   |\n",
      "|    total_timesteps | 202752 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 204000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08474889 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.927     |\n",
      "|    explained_variance   | 0.0611     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00645   |\n",
      "|    n_updates            | 5880       |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 100    |\n",
      "|    time_elapsed    | 4159   |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-0.74 +/- 0.43\n",
      "Episode length: 3.60 +/- 4.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 206000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06703414 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.834     |\n",
      "|    explained_variance   | -0.0675    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.25       |\n",
      "|    n_updates            | 5890       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    value_loss           | 0.609      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 101    |\n",
      "|    time_elapsed    | 4198   |\n",
      "|    total_timesteps | 206848 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-0.18 +/- 1.44\n",
      "Episode length: 9.20 +/- 14.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.2        |\n",
      "|    mean_reward          | -0.18      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 208000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07894403 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0046    |\n",
      "|    n_updates            | 5900       |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 4241   |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=0.70 +/- 2.72\n",
      "Episode length: 18.00 +/- 27.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18         |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 210000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07254542 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.869     |\n",
      "|    explained_variance   | 0.0832     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 5910       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    value_loss           | 0.457      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 103    |\n",
      "|    time_elapsed    | 4282   |\n",
      "|    total_timesteps | 210944 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-0.42 +/- 0.82\n",
      "Episode length: 6.80 +/- 8.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | -0.42       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 212000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086325824 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | -0.0241     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 5920        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 0.29        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 104    |\n",
      "|    time_elapsed    | 4328   |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=0.38 +/- 2.06\n",
      "Episode length: 14.80 +/- 20.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 214000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059601363 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.0698      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 5930        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.534       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 105    |\n",
      "|    time_elapsed    | 4370   |\n",
      "|    total_timesteps | 215040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 216000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06496228 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.806     |\n",
      "|    explained_variance   | -0.0179    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 5940       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    value_loss           | 0.251      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 106    |\n",
      "|    time_elapsed    | 4414   |\n",
      "|    total_timesteps | 217088 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-0.84 +/- 0.19\n",
      "Episode length: 2.60 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 218000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10047037 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | -0.0125    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 5950       |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 107    |\n",
      "|    time_elapsed    | 4460   |\n",
      "|    total_timesteps | 219136 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-0.58 +/- 0.65\n",
      "Episode length: 5.20 +/- 6.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | -0.58       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088116944 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0563      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0178      |\n",
      "|    n_updates            | 5960        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.609       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 4506   |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-0.78 +/- 0.30\n",
      "Episode length: 3.20 +/- 2.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | -0.78       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 222000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075412825 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | -0.0452     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00153     |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 0.318       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 109    |\n",
      "|    time_elapsed    | 4550   |\n",
      "|    total_timesteps | 223232 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-0.28 +/- 1.10\n",
      "Episode length: 8.20 +/- 10.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.2        |\n",
      "|    mean_reward          | -0.28      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 224000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08929214 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.0764     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 5980       |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 4594   |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-0.74 +/- 0.47\n",
      "Episode length: 3.60 +/- 4.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 226000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09674671 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.989     |\n",
      "|    explained_variance   | 0.195      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0188     |\n",
      "|    n_updates            | 5990       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 111    |\n",
      "|    time_elapsed    | 4637   |\n",
      "|    total_timesteps | 227328 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-0.58 +/- 0.44\n",
      "Episode length: 5.20 +/- 4.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 228000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06194317 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.973     |\n",
      "|    explained_variance   | -0.0125    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0412     |\n",
      "|    n_updates            | 6000       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.61       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 49     |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 4680   |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-0.04 +/- 1.53\n",
      "Episode length: 10.60 +/- 15.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.6       |\n",
      "|    mean_reward          | -0.04      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 230000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07195729 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.997     |\n",
      "|    explained_variance   | 0.138      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 6010       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.314      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 113    |\n",
      "|    time_elapsed    | 4723   |\n",
      "|    total_timesteps | 231424 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-0.86 +/- 0.14\n",
      "Episode length: 2.40 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 232000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07214298 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.907     |\n",
      "|    explained_variance   | 0.387      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0145    |\n",
      "|    n_updates            | 6020       |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    value_loss           | 0.244      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 114    |\n",
      "|    time_elapsed    | 4764   |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 234000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06144949 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | -0.0584    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0207     |\n",
      "|    n_updates            | 6030       |\n",
      "|    policy_gradient_loss | -0.031     |\n",
      "|    value_loss           | 0.557      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 115    |\n",
      "|    time_elapsed    | 4808   |\n",
      "|    total_timesteps | 235520 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-0.80 +/- 0.11\n",
      "Episode length: 3.00 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 236000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08853197 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.963     |\n",
      "|    explained_variance   | -0.065     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00073   |\n",
      "|    n_updates            | 6040       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    value_loss           | 0.339      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 116    |\n",
      "|    time_elapsed    | 4851   |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-0.04 +/- 1.82\n",
      "Episode length: 10.60 +/- 18.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.6        |\n",
      "|    mean_reward          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 238000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094344854 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00848    |\n",
      "|    n_updates            | 6050        |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 117    |\n",
      "|    time_elapsed    | 4896   |\n",
      "|    total_timesteps | 239616 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-0.58 +/- 0.56\n",
      "Episode length: 5.20 +/- 5.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | -0.58       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100186095 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.099       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 6060        |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 4939   |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-0.02 +/- 1.86\n",
      "Episode length: 10.80 +/- 18.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 242000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072680645 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.873      |\n",
      "|    explained_variance   | 0.0574      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0247      |\n",
      "|    n_updates            | 6070        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.558       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 119    |\n",
      "|    time_elapsed    | 4983   |\n",
      "|    total_timesteps | 243712 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-2.24 +/- 3.92\n",
      "Episode length: 6.60 +/- 6.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | -2.24      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 244000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08058228 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.979     |\n",
      "|    explained_variance   | 0.0304     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00236   |\n",
      "|    n_updates            | 6080       |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.329      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 5026   |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 2.4       |\n",
      "|    mean_reward          | -0.86     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 246000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1125805 |\n",
      "|    clip_fraction        | 0.276     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.938    |\n",
      "|    explained_variance   | 0.237     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0148    |\n",
      "|    n_updates            | 6090      |\n",
      "|    policy_gradient_loss | -0.0414   |\n",
      "|    value_loss           | 0.324     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 121    |\n",
      "|    time_elapsed    | 5071   |\n",
      "|    total_timesteps | 247808 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-0.62 +/- 0.31\n",
      "Episode length: 4.80 +/- 3.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 248000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09095671 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.194      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00717   |\n",
      "|    n_updates            | 6100       |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    value_loss           | 0.268      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 5116   |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=0.24 +/- 1.67\n",
      "Episode length: 13.40 +/- 16.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.4       |\n",
      "|    mean_reward          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 250000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10441186 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 8.01e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 6110       |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.409      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 123    |\n",
      "|    time_elapsed    | 5160   |\n",
      "|    total_timesteps | 251904 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-0.60 +/- 0.37\n",
      "Episode length: 5.00 +/- 3.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 252000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09074287 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.965     |\n",
      "|    explained_variance   | -0.0778    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0272    |\n",
      "|    n_updates            | 6120       |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 124    |\n",
      "|    time_elapsed    | 5204   |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-0.66 +/- 0.38\n",
      "Episode length: 4.40 +/- 3.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | -0.66      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 254000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09177199 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.0996     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 6130       |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    value_loss           | 0.324      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.6      |\n",
      "|    mean_reward     | -0.94    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 256000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 5249   |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-0.88 +/- 0.19\n",
      "Episode length: 2.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 258000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07305891 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.998     |\n",
      "|    explained_variance   | -0.176     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0272     |\n",
      "|    n_updates            | 6140       |\n",
      "|    policy_gradient_loss | -0.0304    |\n",
      "|    value_loss           | 0.34       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 126    |\n",
      "|    time_elapsed    | 5291   |\n",
      "|    total_timesteps | 258048 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-0.80 +/- 0.17\n",
      "Episode length: 3.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099026814 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | -0.0971     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00962    |\n",
      "|    n_updates            | 6150        |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 5333   |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-0.84 +/- 0.14\n",
      "Episode length: 2.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 262000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089468665 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.00313     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 6160        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 128    |\n",
      "|    time_elapsed    | 5380   |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-0.82 +/- 0.12\n",
      "Episode length: 2.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | -0.82       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 264000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069847316 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.0589      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 6170        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 129    |\n",
      "|    time_elapsed    | 5422   |\n",
      "|    total_timesteps | 264192 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-0.34 +/- 0.84\n",
      "Episode length: 7.60 +/- 8.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | -0.34       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 266000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079714134 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | -0.0424     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 6180        |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 130    |\n",
      "|    time_elapsed    | 5465   |\n",
      "|    total_timesteps | 266240 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-0.62 +/- 0.50\n",
      "Episode length: 4.80 +/- 4.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 268000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07890092 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.949     |\n",
      "|    explained_variance   | 0.0721     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 6190       |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 131    |\n",
      "|    time_elapsed    | 5508   |\n",
      "|    total_timesteps | 268288 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-0.86 +/- 0.14\n",
      "Episode length: 2.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.4         |\n",
      "|    mean_reward          | -0.86       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086140096 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.0522      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 6200        |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 132    |\n",
      "|    time_elapsed    | 5551   |\n",
      "|    total_timesteps | 270336 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 272000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08829998 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.961     |\n",
      "|    explained_variance   | 0.0642     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 6210       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.383      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 133    |\n",
      "|    time_elapsed    | 5589   |\n",
      "|    total_timesteps | 272384 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=-0.30 +/- 1.12\n",
      "Episode length: 8.00 +/- 11.15\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 274000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08701063 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 6220       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 134    |\n",
      "|    time_elapsed    | 5631   |\n",
      "|    total_timesteps | 274432 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-0.74 +/- 0.39\n",
      "Episode length: 3.60 +/- 3.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 276000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10942836 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.0836     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 6230       |\n",
      "|    policy_gradient_loss | -0.0536    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 5673   |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=-0.80 +/- 0.26\n",
      "Episode length: 3.00 +/- 2.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 278000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084240936 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 6240        |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 136    |\n",
      "|    time_elapsed    | 5710   |\n",
      "|    total_timesteps | 278528 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-0.66 +/- 0.29\n",
      "Episode length: 4.40 +/- 2.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | -0.66       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089277215 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 6250        |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 5751   |\n",
      "|    total_timesteps | 280576 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-0.52 +/- 0.37\n",
      "Episode length: 5.80 +/- 3.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | -0.52       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 282000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080195695 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0334     |\n",
      "|    n_updates            | 6260        |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 138    |\n",
      "|    time_elapsed    | 5797   |\n",
      "|    total_timesteps | 282624 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 284000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07376438 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.227      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 6270       |\n",
      "|    policy_gradient_loss | -0.0472    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 139    |\n",
      "|    time_elapsed    | 5840   |\n",
      "|    total_timesteps | 284672 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-0.74 +/- 0.21\n",
      "Episode length: 3.60 +/- 2.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | -0.74       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 286000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071670175 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.978      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 6280        |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 140    |\n",
      "|    time_elapsed    | 5884   |\n",
      "|    total_timesteps | 286720 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-0.74 +/- 0.32\n",
      "Episode length: 3.60 +/- 3.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | -0.74       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053629965 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0467      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 6290        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.559       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 5928   |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-0.78 +/- 0.30\n",
      "Episode length: 3.20 +/- 2.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 290000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07914199 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.0451     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 6300       |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 142    |\n",
      "|    time_elapsed    | 5974   |\n",
      "|    total_timesteps | 290816 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-0.58 +/- 0.45\n",
      "Episode length: 5.20 +/- 4.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 292000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09396979 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.225      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 6310       |\n",
      "|    policy_gradient_loss | -0.0523    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 143    |\n",
      "|    time_elapsed    | 6018   |\n",
      "|    total_timesteps | 292864 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-0.84 +/- 0.22\n",
      "Episode length: 2.60 +/- 2.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 294000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11676368 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.0981     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 6320       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.322      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 144    |\n",
      "|    time_elapsed    | 6061   |\n",
      "|    total_timesteps | 294912 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-0.52 +/- 0.59\n",
      "Episode length: 5.80 +/- 5.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 296000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10385536 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.943     |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 6330       |\n",
      "|    policy_gradient_loss | -0.0507    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 6105   |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-0.42 +/- 0.68\n",
      "Episode length: 6.80 +/- 6.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 298000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07317017 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0209    |\n",
      "|    n_updates            | 6340       |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    value_loss           | 0.171      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 146    |\n",
      "|    time_elapsed    | 6151   |\n",
      "|    total_timesteps | 299008 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-0.90 +/- 0.15\n",
      "Episode length: 2.00 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 300000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07876653 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.993     |\n",
      "|    explained_variance   | 0.0644     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 6350       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.238      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 6194   |\n",
      "|    total_timesteps | 301056 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-0.82 +/- 0.26\n",
      "Episode length: 2.80 +/- 2.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 302000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07326731 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.911     |\n",
      "|    explained_variance   | 0.0879     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 6360       |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 148    |\n",
      "|    time_elapsed    | 6238   |\n",
      "|    total_timesteps | 303104 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-0.62 +/- 0.44\n",
      "Episode length: 4.80 +/- 4.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | -0.62       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 304000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076982684 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0467      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00524    |\n",
      "|    n_updates            | 6370        |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 6281   |\n",
      "|    total_timesteps | 305152 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=0.00 +/- 1.39\n",
      "Episode length: 11.00 +/- 13.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 1.49e-08   |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 306000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06624757 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.912     |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00458    |\n",
      "|    n_updates            | 6380       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    value_loss           | 0.355      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 150    |\n",
      "|    time_elapsed    | 6327   |\n",
      "|    total_timesteps | 307200 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-0.62 +/- 0.43\n",
      "Episode length: 4.80 +/- 4.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 308000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07735719 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.253      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00964   |\n",
      "|    n_updates            | 6390       |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    value_loss           | 0.191      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 6372   |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-0.70 +/- 0.35\n",
      "Episode length: 4.00 +/- 3.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 310000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07432534 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.19       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 6400       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 152    |\n",
      "|    time_elapsed    | 6419   |\n",
      "|    total_timesteps | 311296 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-0.94 +/- 0.05\n",
      "Episode length: 1.60 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.6         |\n",
      "|    mean_reward          | -0.94       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 312000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091976464 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 6410        |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 153    |\n",
      "|    time_elapsed    | 6463   |\n",
      "|    total_timesteps | 313344 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-0.86 +/- 0.19\n",
      "Episode length: 2.40 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 314000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09187459 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.987     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 6420       |\n",
      "|    policy_gradient_loss | -0.0482    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 154    |\n",
      "|    time_elapsed    | 6509   |\n",
      "|    total_timesteps | 315392 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-0.48 +/- 0.66\n",
      "Episode length: 6.20 +/- 6.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 316000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07128918 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 6430       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    value_loss           | 0.304      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 155    |\n",
      "|    time_elapsed    | 6553   |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=0.48 +/- 1.96\n",
      "Episode length: 15.80 +/- 19.57\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.8       |\n",
      "|    mean_reward          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 318000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08485636 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | 0.0682     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00246    |\n",
      "|    n_updates            | 6440       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.364      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 156    |\n",
      "|    time_elapsed    | 6600   |\n",
      "|    total_timesteps | 319488 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-0.44 +/- 0.69\n",
      "Episode length: 6.60 +/- 6.86\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | -0.44      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08080283 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0202    |\n",
      "|    n_updates            | 6450       |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 6644   |\n",
      "|    total_timesteps | 321536 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.2         |\n",
      "|    mean_reward          | -0.98       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 322000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074698664 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 6460        |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 158    |\n",
      "|    time_elapsed    | 6687   |\n",
      "|    total_timesteps | 323584 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-0.56 +/- 0.68\n",
      "Episode length: 5.40 +/- 6.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | -0.56       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 324000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069854125 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | -0.00553    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0012      |\n",
      "|    n_updates            | 6470        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.246       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 6734   |\n",
      "|    total_timesteps | 325632 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=1.14 +/- 1.71\n",
      "Episode length: 22.40 +/- 17.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.4        |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 326000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092420675 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.032      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 6480        |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 160    |\n",
      "|    time_elapsed    | 6784   |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 328000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07535796 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.13       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.641      |\n",
      "|    n_updates            | 6490       |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.268      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 6831   |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 330000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09400285 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | 0.0927     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 6500       |\n",
      "|    policy_gradient_loss | -0.0486    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 162    |\n",
      "|    time_elapsed    | 6877   |\n",
      "|    total_timesteps | 331776 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-0.86 +/- 0.19\n",
      "Episode length: 2.40 +/- 1.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.4         |\n",
      "|    mean_reward          | -0.86       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 332000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088661276 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 6510        |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 163    |\n",
      "|    time_elapsed    | 6924   |\n",
      "|    total_timesteps | 333824 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-0.36 +/- 1.23\n",
      "Episode length: 7.40 +/- 12.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 334000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07998733 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.0642     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 6520       |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 164    |\n",
      "|    time_elapsed    | 6975   |\n",
      "|    total_timesteps | 335872 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079536386 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -0.0396     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 6530        |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 165    |\n",
      "|    time_elapsed    | 7022   |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-0.22 +/- 0.97\n",
      "Episode length: 8.80 +/- 9.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.8        |\n",
      "|    mean_reward          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 338000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08987017 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | -0.106     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 6540       |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 166    |\n",
      "|    time_elapsed    | 7069   |\n",
      "|    total_timesteps | 339968 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-0.84 +/- 0.19\n",
      "Episode length: 2.60 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 340000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09480336 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 6550       |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.4      |\n",
      "|    mean_reward     | -0.96    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 342000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 7116   |\n",
      "|    total_timesteps | 342016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-0.52 +/- 0.53\n",
      "Episode length: 5.80 +/- 5.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | -0.52       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 344000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066290736 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.0045     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 6560        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.302       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 168    |\n",
      "|    time_elapsed    | 7164   |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-0.52 +/- 0.26\n",
      "Episode length: 5.80 +/- 2.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | -0.52       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 346000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058504995 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.005      |\n",
      "|    n_updates            | 6570        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 169    |\n",
      "|    time_elapsed    | 7212   |\n",
      "|    total_timesteps | 346112 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=0.04 +/- 1.55\n",
      "Episode length: 11.40 +/- 15.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.4       |\n",
      "|    mean_reward          | 0.04       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 348000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08751877 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.971     |\n",
      "|    explained_variance   | 0.0424     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 6580       |\n",
      "|    policy_gradient_loss | -0.0339    |\n",
      "|    value_loss           | 0.287      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 170    |\n",
      "|    time_elapsed    | 7256   |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-0.60 +/- 0.32\n",
      "Episode length: 5.00 +/- 3.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 350000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07987565 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.93      |\n",
      "|    explained_variance   | 0.219      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 6590       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 171    |\n",
      "|    time_elapsed    | 7299   |\n",
      "|    total_timesteps | 350208 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 6.60 +/- 9.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | -0.44      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 352000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09011987 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | -0.0926    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.048     |\n",
      "|    n_updates            | 6600       |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 172    |\n",
      "|    time_elapsed    | 7344   |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.6         |\n",
      "|    mean_reward          | -0.94       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 354000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054839116 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.0396      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 6610        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.537       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 173    |\n",
      "|    time_elapsed    | 7389   |\n",
      "|    total_timesteps | 354304 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-0.04 +/- 1.16\n",
      "Episode length: 10.60 +/- 11.57\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.6       |\n",
      "|    mean_reward          | -0.04      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 356000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07695303 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.955     |\n",
      "|    explained_variance   | -0.213     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0201    |\n",
      "|    n_updates            | 6620       |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.323      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 7433   |\n",
      "|    total_timesteps | 356352 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 358000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08405724 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.877     |\n",
      "|    explained_variance   | -0.332     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 6630       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 175    |\n",
      "|    time_elapsed    | 7475   |\n",
      "|    total_timesteps | 358400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-1.36 +/- 1.32\n",
      "Episode length: 15.40 +/- 23.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.4       |\n",
      "|    mean_reward          | -1.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 360000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06334054 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.889     |\n",
      "|    explained_variance   | -0.0454    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.005      |\n",
      "|    n_updates            | 6640       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.533      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 7523   |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=-0.52 +/- 0.58\n",
      "Episode length: 5.80 +/- 5.84\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 362000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09807141 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 6650       |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 177    |\n",
      "|    time_elapsed    | 7565   |\n",
      "|    total_timesteps | 362496 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-0.74 +/- 0.16\n",
      "Episode length: 3.60 +/- 1.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 364000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08156502 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 0.268      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 6660       |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    value_loss           | 0.208      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 178    |\n",
      "|    time_elapsed    | 7607   |\n",
      "|    total_timesteps | 364544 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-0.82 +/- 0.13\n",
      "Episode length: 2.80 +/- 1.33\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 2.8      |\n",
      "|    mean_reward          | -0.82    |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 366000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.099289 |\n",
      "|    clip_fraction        | 0.225    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.833   |\n",
      "|    explained_variance   | 0.0729   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.0239   |\n",
      "|    n_updates            | 6670     |\n",
      "|    policy_gradient_loss | -0.0298  |\n",
      "|    value_loss           | 0.368    |\n",
      "--------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 179    |\n",
      "|    time_elapsed    | 7653   |\n",
      "|    total_timesteps | 366592 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=0.06 +/- 1.54\n",
      "Episode length: 11.60 +/- 15.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.6       |\n",
      "|    mean_reward          | 0.06       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 368000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09680203 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.13       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00196    |\n",
      "|    n_updates            | 6680       |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    value_loss           | 0.193      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 7693   |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-0.74 +/- 0.47\n",
      "Episode length: 3.60 +/- 4.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 370000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07176341 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00353   |\n",
      "|    n_updates            | 6690       |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    value_loss           | 0.232      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 181    |\n",
      "|    time_elapsed    | 7730   |\n",
      "|    total_timesteps | 370688 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-0.62 +/- 0.76\n",
      "Episode length: 4.80 +/- 7.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 372000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07566887 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.944     |\n",
      "|    explained_variance   | -0.0205    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0315     |\n",
      "|    n_updates            | 6700       |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    value_loss           | 0.35       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 182    |\n",
      "|    time_elapsed    | 7769   |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 374000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07120019 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.965     |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0366    |\n",
      "|    n_updates            | 6710       |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 183    |\n",
      "|    time_elapsed    | 7806   |\n",
      "|    total_timesteps | 374784 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-0.84 +/- 0.23\n",
      "Episode length: 2.60 +/- 2.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 376000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099034846 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.889      |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.014      |\n",
      "|    n_updates            | 6720        |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 7845   |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-0.28 +/- 1.09\n",
      "Episode length: 8.20 +/- 10.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.2        |\n",
      "|    mean_reward          | -0.28      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 378000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09979148 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.999     |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 6730       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 185    |\n",
      "|    time_elapsed    | 7883   |\n",
      "|    total_timesteps | 378880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=0.10 +/- 2.00\n",
      "Episode length: 12.00 +/- 20.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072263606 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | -0.0874     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00744    |\n",
      "|    n_updates            | 6740        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.595       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 7922   |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-0.72 +/- 0.32\n",
      "Episode length: 3.80 +/- 3.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 382000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09940952 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | -0.0595    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 6750       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 187    |\n",
      "|    time_elapsed    | 7960   |\n",
      "|    total_timesteps | 382976 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-0.12 +/- 1.76\n",
      "Episode length: 9.80 +/- 17.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.8        |\n",
      "|    mean_reward          | -0.12      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 384000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10227491 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.94      |\n",
      "|    explained_variance   | 0.314      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 6760       |\n",
      "|    policy_gradient_loss | -0.0494    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 8000   |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-0.74 +/- 0.47\n",
      "Episode length: 3.60 +/- 4.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 386000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08326897 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 6770       |\n",
      "|    policy_gradient_loss | -0.0385    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 189    |\n",
      "|    time_elapsed    | 8036   |\n",
      "|    total_timesteps | 387072 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-0.64 +/- 0.46\n",
      "Episode length: 4.60 +/- 4.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | -0.64       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 388000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093912184 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.823      |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 6780        |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 8073   |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-0.82 +/- 0.19\n",
      "Episode length: 2.80 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 390000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08155617 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.833     |\n",
      "|    explained_variance   | 0.152      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 6790       |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    value_loss           | 0.19       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 191    |\n",
      "|    time_elapsed    | 8116   |\n",
      "|    total_timesteps | 391168 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-0.82 +/- 0.26\n",
      "Episode length: 2.80 +/- 2.64\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 2.8      |\n",
      "|    mean_reward          | -0.82    |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 392000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.080243 |\n",
      "|    clip_fraction        | 0.27     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.986   |\n",
      "|    explained_variance   | 0.125    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0154  |\n",
      "|    n_updates            | 6800     |\n",
      "|    policy_gradient_loss | -0.0357  |\n",
      "|    value_loss           | 0.251    |\n",
      "--------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 192    |\n",
      "|    time_elapsed    | 8163   |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-0.84 +/- 0.20\n",
      "Episode length: 2.60 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 394000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09175506 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.879     |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 6810       |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    value_loss           | 0.311      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 193    |\n",
      "|    time_elapsed    | 8205   |\n",
      "|    total_timesteps | 395264 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=0.00 +/- 1.51\n",
      "Episode length: 11.00 +/- 15.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11          |\n",
      "|    mean_reward          | 1.49e-08    |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 396000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082317166 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.817      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 6820        |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 194    |\n",
      "|    time_elapsed    | 8248   |\n",
      "|    total_timesteps | 397312 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-0.16 +/- 0.86\n",
      "Episode length: 9.40 +/- 8.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.4        |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 398000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08425422 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.777     |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00736   |\n",
      "|    n_updates            | 6830       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    value_loss           | 0.332      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 195    |\n",
      "|    time_elapsed    | 8292   |\n",
      "|    total_timesteps | 399360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-0.36 +/- 0.63\n",
      "Episode length: 7.40 +/- 6.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 400000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06772647 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.823     |\n",
      "|    explained_variance   | -0.186     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00625   |\n",
      "|    n_updates            | 6840       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    value_loss           | 0.279      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 8331   |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=0.00 +/- 1.21\n",
      "Episode length: 11.00 +/- 12.07\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 1.49e-08   |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 402000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07039475 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.685     |\n",
      "|    explained_variance   | 0.0641     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000229   |\n",
      "|    n_updates            | 6850       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 197    |\n",
      "|    time_elapsed    | 8375   |\n",
      "|    total_timesteps | 403456 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-0.30 +/- 0.60\n",
      "Episode length: 8.00 +/- 6.03\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 404000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08740288 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.879     |\n",
      "|    explained_variance   | 0.206      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0168     |\n",
      "|    n_updates            | 6860       |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 0.348      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 8420   |\n",
      "|    total_timesteps | 405504 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=-0.60 +/- 0.21\n",
      "Episode length: 5.00 +/- 2.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 406000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089875124 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | -0.0688     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 6870        |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 199    |\n",
      "|    time_elapsed    | 8463   |\n",
      "|    total_timesteps | 407552 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-0.54 +/- 0.30\n",
      "Episode length: 5.60 +/- 3.01\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5.6       |\n",
      "|    mean_reward          | -0.54     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 408000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1410329 |\n",
      "|    clip_fraction        | 0.262     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.849    |\n",
      "|    explained_variance   | 0.32      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00583  |\n",
      "|    n_updates            | 6880      |\n",
      "|    policy_gradient_loss | -0.0357   |\n",
      "|    value_loss           | 0.286     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 8507   |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 6.60 +/- 10.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | -0.44      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 410000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09824918 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 0.0693     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00156    |\n",
      "|    n_updates            | 6890       |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.24       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 201    |\n",
      "|    time_elapsed    | 8552   |\n",
      "|    total_timesteps | 411648 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-0.20 +/- 0.65\n",
      "Episode length: 9.00 +/- 6.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 412000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077466294 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.0119     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 6900        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.287       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 202    |\n",
      "|    time_elapsed    | 8597   |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=-0.72 +/- 0.56\n",
      "Episode length: 3.80 +/- 5.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | -0.72       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 414000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091249585 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.0893      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 6910        |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 203    |\n",
      "|    time_elapsed    | 8638   |\n",
      "|    total_timesteps | 415744 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 416000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07491781 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.881     |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 6920       |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    value_loss           | 0.303      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 204    |\n",
      "|    time_elapsed    | 8681   |\n",
      "|    total_timesteps | 417792 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=0.20 +/- 2.25\n",
      "Episode length: 13.00 +/- 22.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13         |\n",
      "|    mean_reward          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 418000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08573373 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.999     |\n",
      "|    explained_variance   | 0.176      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0031    |\n",
      "|    n_updates            | 6930       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 205    |\n",
      "|    time_elapsed    | 8723   |\n",
      "|    total_timesteps | 419840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 9.00 +/- 9.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 420000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07117167 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.883     |\n",
      "|    explained_variance   | 0.138      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0215     |\n",
      "|    n_updates            | 6940       |\n",
      "|    policy_gradient_loss | -0.0308    |\n",
      "|    value_loss           | 0.376      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 8765   |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=0.00 +/- 0.91\n",
      "Episode length: 11.00 +/- 9.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 1.49e-08   |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 422000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07699026 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.921     |\n",
      "|    explained_variance   | 0.0513     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.528      |\n",
      "|    n_updates            | 6950       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    value_loss           | 0.365      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 207    |\n",
      "|    time_elapsed    | 8805   |\n",
      "|    total_timesteps | 423936 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-0.74 +/- 0.31\n",
      "Episode length: 3.60 +/- 3.07\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.6       |\n",
      "|    mean_reward          | -0.74     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 424000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0900674 |\n",
      "|    clip_fraction        | 0.277     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.84     |\n",
      "|    explained_variance   | -0.0591   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0201   |\n",
      "|    n_updates            | 6960      |\n",
      "|    policy_gradient_loss | -0.0382   |\n",
      "|    value_loss           | 0.428     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 8846   |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=-0.52 +/- 0.42\n",
      "Episode length: 5.80 +/- 4.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | -0.52       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 426000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081386894 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.834      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00358     |\n",
      "|    n_updates            | 6970        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=428000, episode_reward=-0.72 +/- 0.20\n",
      "Episode length: 3.80 +/- 2.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | -0.72    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 428000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 209    |\n",
      "|    time_elapsed    | 8886   |\n",
      "|    total_timesteps | 428032 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-0.48 +/- 0.69\n",
      "Episode length: 6.20 +/- 6.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 430000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07383645 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.887     |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 6980       |\n",
      "|    policy_gradient_loss | -0.0344    |\n",
      "|    value_loss           | 0.227      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 210    |\n",
      "|    time_elapsed    | 8928   |\n",
      "|    total_timesteps | 430080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=0.28 +/- 1.85\n",
      "Episode length: 13.80 +/- 18.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 432000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064624794 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.0275      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00193     |\n",
      "|    n_updates            | 6990        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.399       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 211    |\n",
      "|    time_elapsed    | 8970   |\n",
      "|    total_timesteps | 432128 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=-0.80 +/- 0.19\n",
      "Episode length: 3.00 +/- 1.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 434000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09475024 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.913     |\n",
      "|    explained_variance   | 0.0154     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 7000       |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    value_loss           | 0.26       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 212    |\n",
      "|    time_elapsed    | 9012   |\n",
      "|    total_timesteps | 434176 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=0.68 +/- 3.11\n",
      "Episode length: 17.80 +/- 31.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.8       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 436000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07203083 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.934     |\n",
      "|    explained_variance   | 0.0843     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 7010       |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    value_loss           | 0.228      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 9053   |\n",
      "|    total_timesteps | 436224 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-0.78 +/- 0.22\n",
      "Episode length: 3.20 +/- 2.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 438000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08737834 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.918     |\n",
      "|    explained_variance   | 0.0752     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 7020       |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 214    |\n",
      "|    time_elapsed    | 9096   |\n",
      "|    total_timesteps | 438272 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-0.76 +/- 0.22\n",
      "Episode length: 3.40 +/- 2.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 440000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07236001 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.937     |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00481    |\n",
      "|    n_updates            | 7030       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.555      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 9140   |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=0.86 +/- 1.73\n",
      "Episode length: 19.60 +/- 17.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 19.6       |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 442000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10967497 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | -0.242     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 7040       |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 216    |\n",
      "|    time_elapsed    | 9182   |\n",
      "|    total_timesteps | 442368 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-0.48 +/- 0.84\n",
      "Episode length: 6.20 +/- 8.45\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.2       |\n",
      "|    mean_reward          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 444000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0979699 |\n",
      "|    clip_fraction        | 0.305     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.938    |\n",
      "|    explained_variance   | 0.215     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0393   |\n",
      "|    n_updates            | 7050      |\n",
      "|    policy_gradient_loss | -0.0454   |\n",
      "|    value_loss           | 0.152     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 217    |\n",
      "|    time_elapsed    | 9223   |\n",
      "|    total_timesteps | 444416 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=-0.76 +/- 0.29\n",
      "Episode length: 3.40 +/- 2.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 446000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08693397 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.917     |\n",
      "|    explained_variance   | 0.047      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 7060       |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 218    |\n",
      "|    time_elapsed    | 9264   |\n",
      "|    total_timesteps | 446464 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-0.78 +/- 0.27\n",
      "Episode length: 3.20 +/- 2.71\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.2       |\n",
      "|    mean_reward          | -0.78     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 448000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0805147 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.921    |\n",
      "|    explained_variance   | 0.0462    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0317   |\n",
      "|    n_updates            | 7070      |\n",
      "|    policy_gradient_loss | -0.0355   |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 219    |\n",
      "|    time_elapsed    | 9311   |\n",
      "|    total_timesteps | 448512 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-0.14 +/- 1.34\n",
      "Episode length: 9.60 +/- 13.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.6        |\n",
      "|    mean_reward          | -0.14      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 450000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10015459 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 7080       |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 220    |\n",
      "|    time_elapsed    | 9352   |\n",
      "|    total_timesteps | 450560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-0.46 +/- 0.69\n",
      "Episode length: 6.40 +/- 6.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 452000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08250238 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.959     |\n",
      "|    explained_variance   | 0.118      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00267   |\n",
      "|    n_updates            | 7090       |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 221    |\n",
      "|    time_elapsed    | 9395   |\n",
      "|    total_timesteps | 452608 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=-0.58 +/- 0.48\n",
      "Episode length: 5.20 +/- 4.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 454000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07741612 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.972     |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00353   |\n",
      "|    n_updates            | 7100       |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 222    |\n",
      "|    time_elapsed    | 9437   |\n",
      "|    total_timesteps | 454656 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-0.90 +/- 0.09\n",
      "Episode length: 2.00 +/- 0.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 456000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08232772 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.905     |\n",
      "|    explained_variance   | 0.131      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 7110       |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 9478   |\n",
      "|    total_timesteps | 456704 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 458000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09154186 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.899     |\n",
      "|    explained_variance   | 0.0968     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 7120       |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 224    |\n",
      "|    time_elapsed    | 9519   |\n",
      "|    total_timesteps | 458752 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-0.52 +/- 3.31\n",
      "Episode length: 23.80 +/- 25.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 23.8       |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08746661 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.913     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0309    |\n",
      "|    n_updates            | 7130       |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 9561   |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=-0.82 +/- 0.15\n",
      "Episode length: 2.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 462000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08333428 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | 0.144      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 7140       |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    value_loss           | 0.279      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 226    |\n",
      "|    time_elapsed    | 9602   |\n",
      "|    total_timesteps | 462848 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-0.86 +/- 0.08\n",
      "Episode length: 2.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 464000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08743649 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | -0.0195    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0274    |\n",
      "|    n_updates            | 7150       |\n",
      "|    policy_gradient_loss | -0.0484    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 227    |\n",
      "|    time_elapsed    | 9645   |\n",
      "|    total_timesteps | 464896 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=-0.50 +/- 0.55\n",
      "Episode length: 6.00 +/- 5.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 466000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078469835 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.031       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 7160        |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 228    |\n",
      "|    time_elapsed    | 9690   |\n",
      "|    total_timesteps | 466944 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-0.92 +/- 0.07\n",
      "Episode length: 1.80 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 468000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09624615 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.19       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 7170       |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 9732   |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-0.66 +/- 0.58\n",
      "Episode length: 4.40 +/- 5.82\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.4       |\n",
      "|    mean_reward          | -0.66     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 470000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1298975 |\n",
      "|    clip_fraction        | 0.307     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.962    |\n",
      "|    explained_variance   | -0.048    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.656     |\n",
      "|    n_updates            | 7180      |\n",
      "|    policy_gradient_loss | -0.0412   |\n",
      "|    value_loss           | 0.565     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 230    |\n",
      "|    time_elapsed    | 9775   |\n",
      "|    total_timesteps | 471040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-0.60 +/- 0.53\n",
      "Episode length: 5.00 +/- 5.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 472000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09093635 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.943     |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 7190       |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 231    |\n",
      "|    time_elapsed    | 9816   |\n",
      "|    total_timesteps | 473088 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-0.84 +/- 0.12\n",
      "Episode length: 2.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 474000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07739818 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.884     |\n",
      "|    explained_variance   | -0.0863    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0152    |\n",
      "|    n_updates            | 7200       |\n",
      "|    policy_gradient_loss | -0.0368    |\n",
      "|    value_loss           | 0.245      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 232    |\n",
      "|    time_elapsed    | 9859   |\n",
      "|    total_timesteps | 475136 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=0.84 +/- 1.80\n",
      "Episode length: 19.40 +/- 18.03\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 19.4       |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 476000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08074857 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.949     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000646  |\n",
      "|    n_updates            | 7210       |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 233    |\n",
      "|    time_elapsed    | 9903   |\n",
      "|    total_timesteps | 477184 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-0.68 +/- 0.20\n",
      "Episode length: 4.20 +/- 2.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 478000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07755646 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.928     |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 7220       |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    value_loss           | 0.209      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 234    |\n",
      "|    time_elapsed    | 9945   |\n",
      "|    total_timesteps | 479232 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-0.64 +/- 0.43\n",
      "Episode length: 4.60 +/- 4.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08586793 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.867     |\n",
      "|    explained_variance   | 0.0827     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 7230       |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 9987   |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-0.76 +/- 0.34\n",
      "Episode length: 3.40 +/- 3.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | -0.76       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 482000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092481986 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 7240        |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 236    |\n",
      "|    time_elapsed    | 10030  |\n",
      "|    total_timesteps | 483328 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-0.70 +/- 0.41\n",
      "Episode length: 4.00 +/- 4.15\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 484000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10208702 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.929     |\n",
      "|    explained_variance   | 0.0877     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 7250       |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 10076  |\n",
      "|    total_timesteps | 485376 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=-0.62 +/- 0.32\n",
      "Episode length: 4.80 +/- 3.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 486000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09178124 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.942     |\n",
      "|    explained_variance   | 0.0276     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 7260       |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.261      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 238    |\n",
      "|    time_elapsed    | 10120  |\n",
      "|    total_timesteps | 487424 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-0.06 +/- 1.34\n",
      "Episode length: 10.40 +/- 13.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.4       |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 488000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08517208 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | -0.115     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 7270       |\n",
      "|    policy_gradient_loss | -0.038     |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 10166  |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-0.84 +/- 0.17\n",
      "Episode length: 2.60 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 490000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086993635 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | 0.0664      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 7280        |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 240    |\n",
      "|    time_elapsed    | 10210  |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-0.86 +/- 0.14\n",
      "Episode length: 2.40 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 492000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06651777 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.847     |\n",
      "|    explained_variance   | 0.197      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 7290       |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 241    |\n",
      "|    time_elapsed    | 10254  |\n",
      "|    total_timesteps | 493568 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 3.00 +/- 4.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 494000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08009293 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.862     |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 7300       |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 242    |\n",
      "|    time_elapsed    | 10296  |\n",
      "|    total_timesteps | 495616 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-0.90 +/- 0.20\n",
      "Episode length: 2.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072884485 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.808      |\n",
      "|    explained_variance   | 0.0502      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.247       |\n",
      "|    n_updates            | 7310        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.37        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 243    |\n",
      "|    time_elapsed    | 10339  |\n",
      "|    total_timesteps | 497664 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-0.60 +/- 0.70\n",
      "Episode length: 5.00 +/- 7.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 498000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07785439 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.837     |\n",
      "|    explained_variance   | 0.0144     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00972   |\n",
      "|    n_updates            | 7320       |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 244    |\n",
      "|    time_elapsed    | 10382  |\n",
      "|    total_timesteps | 499712 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-0.48 +/- 0.75\n",
      "Episode length: 6.20 +/- 7.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -0.48       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092310086 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.867      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 7330        |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 245    |\n",
      "|    time_elapsed    | 10425  |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 502000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0774195 |\n",
      "|    clip_fraction        | 0.283     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.845    |\n",
      "|    explained_variance   | -0.0311   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0313   |\n",
      "|    n_updates            | 7340      |\n",
      "|    policy_gradient_loss | -0.0399   |\n",
      "|    value_loss           | 0.221     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 246    |\n",
      "|    time_elapsed    | 10469  |\n",
      "|    total_timesteps | 503808 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=0.74 +/- 1.81\n",
      "Episode length: 18.40 +/- 18.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090517566 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.873      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 7350        |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 247    |\n",
      "|    time_elapsed    | 10514  |\n",
      "|    total_timesteps | 505856 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=-0.06 +/- 1.30\n",
      "Episode length: 10.40 +/- 12.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.4       |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 506000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06290163 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.812     |\n",
      "|    explained_variance   | 0.0158     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000133   |\n",
      "|    n_updates            | 7360       |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    value_loss           | 0.515      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 248    |\n",
      "|    time_elapsed    | 10558  |\n",
      "|    total_timesteps | 507904 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-0.32 +/- 1.36\n",
      "Episode length: 7.80 +/- 13.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | -0.32       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 508000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073183656 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.0489      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0089     |\n",
      "|    n_updates            | 7370        |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    value_loss           | 0.21        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 10600  |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-0.94 +/- 0.12\n",
      "Episode length: 1.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 510000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10543433 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.797     |\n",
      "|    explained_variance   | -0.147     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00135    |\n",
      "|    n_updates            | 7380       |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    value_loss           | 0.437      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.8      |\n",
      "|    mean_reward     | -0.92    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 512000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 250    |\n",
      "|    time_elapsed    | 10644  |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=-0.74 +/- 0.29\n",
      "Episode length: 3.60 +/- 2.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 514000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07358605 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.843     |\n",
      "|    explained_variance   | 0.061      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 7390       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    value_loss           | 0.282      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 251    |\n",
      "|    time_elapsed    | 10684  |\n",
      "|    total_timesteps | 514048 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-0.34 +/- 1.07\n",
      "Episode length: 7.60 +/- 10.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.6        |\n",
      "|    mean_reward          | -0.34      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 516000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09377742 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.719     |\n",
      "|    explained_variance   | 0.087      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0425    |\n",
      "|    n_updates            | 7400       |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 252    |\n",
      "|    time_elapsed    | 10730  |\n",
      "|    total_timesteps | 516096 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=-0.24 +/- 1.52\n",
      "Episode length: 8.60 +/- 15.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -0.24       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 518000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105819374 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.854      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 7410        |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 253    |\n",
      "|    time_elapsed    | 10771  |\n",
      "|    total_timesteps | 518144 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=0.02 +/- 1.64\n",
      "Episode length: 11.20 +/- 16.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086911336 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.779      |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 7420        |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 10817  |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=-0.48 +/- 0.71\n",
      "Episode length: 6.20 +/- 7.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -0.48       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 522000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085362464 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.875      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000629    |\n",
      "|    n_updates            | 7430        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.436       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 255    |\n",
      "|    time_elapsed    | 10863  |\n",
      "|    total_timesteps | 522240 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-0.22 +/- 0.94\n",
      "Episode length: 8.80 +/- 9.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.8        |\n",
      "|    mean_reward          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 524000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07161203 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | -0.119     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 7440       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 256    |\n",
      "|    time_elapsed    | 10908  |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=-0.48 +/- 0.39\n",
      "Episode length: 6.20 +/- 3.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 526000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10328337 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.817     |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 7450       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 0.319      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 257    |\n",
      "|    time_elapsed    | 10955  |\n",
      "|    total_timesteps | 526336 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-0.56 +/- 0.68\n",
      "Episode length: 5.40 +/- 6.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 528000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08039919 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.875     |\n",
      "|    explained_variance   | 0.262      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00956    |\n",
      "|    n_updates            | 7460       |\n",
      "|    policy_gradient_loss | -0.0344    |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 258    |\n",
      "|    time_elapsed    | 10999  |\n",
      "|    total_timesteps | 528384 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-0.76 +/- 0.39\n",
      "Episode length: 3.40 +/- 3.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | -0.76       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094196275 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 0.091       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 7470        |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 259    |\n",
      "|    time_elapsed    | 11045  |\n",
      "|    total_timesteps | 530432 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-0.34 +/- 1.22\n",
      "Episode length: 7.60 +/- 12.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.6        |\n",
      "|    mean_reward          | -0.34      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 532000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06486966 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.925     |\n",
      "|    explained_variance   | -0.107     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 7480       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 0.452      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 260    |\n",
      "|    time_elapsed    | 11088  |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-0.42 +/- 0.62\n",
      "Episode length: 6.80 +/- 6.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | -0.42       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 534000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087733075 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.831      |\n",
      "|    explained_variance   | -0.0972     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0489     |\n",
      "|    n_updates            | 7490        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 261    |\n",
      "|    time_elapsed    | 11133  |\n",
      "|    total_timesteps | 534528 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-0.14 +/- 1.62\n",
      "Episode length: 9.60 +/- 16.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.6        |\n",
      "|    mean_reward          | -0.14      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 536000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08329453 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.89      |\n",
      "|    explained_variance   | -0.0796    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 7500       |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 262    |\n",
      "|    time_elapsed    | 11177  |\n",
      "|    total_timesteps | 536576 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=-0.90 +/- 0.09\n",
      "Episode length: 2.00 +/- 0.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 538000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10261743 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.892     |\n",
      "|    explained_variance   | -0.0831    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.129      |\n",
      "|    n_updates            | 7510       |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    value_loss           | 0.349      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 48     |\n",
      "|    iterations      | 263    |\n",
      "|    time_elapsed    | 11221  |\n",
      "|    total_timesteps | 538624 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=0.70 +/- 1.64\n",
      "Episode length: 18.00 +/- 16.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18         |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 540000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08026139 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.85      |\n",
      "|    explained_variance   | 0.0248     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 7520       |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 0.33       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 264    |\n",
      "|    time_elapsed    | 11266  |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=-0.90 +/- 0.11\n",
      "Episode length: 2.00 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 542000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10725538 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | -0.0767    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0246     |\n",
      "|    n_updates            | 7530       |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 265    |\n",
      "|    time_elapsed    | 11313  |\n",
      "|    total_timesteps | 542720 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-0.82 +/- 0.22\n",
      "Episode length: 2.80 +/- 2.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 544000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12279686 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.985     |\n",
      "|    explained_variance   | -0.00273   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 7540       |\n",
      "|    policy_gradient_loss | -0.0385    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 266    |\n",
      "|    time_elapsed    | 11360  |\n",
      "|    total_timesteps | 544768 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=-0.88 +/- 0.16\n",
      "Episode length: 2.20 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 546000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09530623 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.94      |\n",
      "|    explained_variance   | -0.0171    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 7550       |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 267    |\n",
      "|    time_elapsed    | 11408  |\n",
      "|    total_timesteps | 546816 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-0.70 +/- 0.36\n",
      "Episode length: 4.00 +/- 3.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 548000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07845332 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.969     |\n",
      "|    explained_variance   | 0.0341     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 7560       |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    value_loss           | 0.246      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 268    |\n",
      "|    time_elapsed    | 11455  |\n",
      "|    total_timesteps | 548864 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-0.08 +/- 1.41\n",
      "Episode length: 10.20 +/- 14.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 550000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09011564 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.971     |\n",
      "|    explained_variance   | 0.0387     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0285    |\n",
      "|    n_updates            | 7570       |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 269    |\n",
      "|    time_elapsed    | 11501  |\n",
      "|    total_timesteps | 550912 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-0.82 +/- 0.27\n",
      "Episode length: 2.80 +/- 2.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 552000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08011794 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.875     |\n",
      "|    explained_variance   | -0.0165    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 7580       |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 270    |\n",
      "|    time_elapsed    | 11546  |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=-0.88 +/- 0.04\n",
      "Episode length: 2.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 554000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10836036 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.948     |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0152    |\n",
      "|    n_updates            | 7590       |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.167      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 271    |\n",
      "|    time_elapsed    | 11590  |\n",
      "|    total_timesteps | 555008 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-0.54 +/- 0.59\n",
      "Episode length: 5.60 +/- 5.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | -0.54       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 556000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088881835 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.908      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 7600        |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.292       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 272    |\n",
      "|    time_elapsed    | 11635  |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=0.20 +/- 0.84\n",
      "Episode length: 13.00 +/- 8.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13         |\n",
      "|    mean_reward          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 558000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08855532 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.85      |\n",
      "|    explained_variance   | 0.0591     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 7610       |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 273    |\n",
      "|    time_elapsed    | 11681  |\n",
      "|    total_timesteps | 559104 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=0.70 +/- 2.72\n",
      "Episode length: 18.00 +/- 27.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077938564 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | -0.00609    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 7620        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 274    |\n",
      "|    time_elapsed    | 11728  |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=-0.08 +/- 1.32\n",
      "Episode length: 10.20 +/- 13.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 562000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06697895 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.909     |\n",
      "|    explained_variance   | -0.213     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0105     |\n",
      "|    n_updates            | 7630       |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 275    |\n",
      "|    time_elapsed    | 11773  |\n",
      "|    total_timesteps | 563200 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-0.84 +/- 0.22\n",
      "Episode length: 2.60 +/- 2.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 564000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09629127 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | -0.00662   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 7640       |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 276    |\n",
      "|    time_elapsed    | 11819  |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=-0.64 +/- 0.62\n",
      "Episode length: 4.60 +/- 6.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 566000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09186933 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 7650       |\n",
      "|    policy_gradient_loss | -0.0506    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 277    |\n",
      "|    time_elapsed    | 11866  |\n",
      "|    total_timesteps | 567296 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-0.50 +/- 0.77\n",
      "Episode length: 6.00 +/- 7.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 568000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09459054 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.963     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 7660       |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 11912  |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=0.94 +/- 2.53\n",
      "Episode length: 20.40 +/- 25.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.4       |\n",
      "|    mean_reward          | 0.94       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 570000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06683032 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.941     |\n",
      "|    explained_variance   | 0.0383     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 7670       |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    value_loss           | 0.246      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 279    |\n",
      "|    time_elapsed    | 11959  |\n",
      "|    total_timesteps | 571392 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=0.38 +/- 2.04\n",
      "Episode length: 14.80 +/- 20.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 14.8       |\n",
      "|    mean_reward          | 0.38       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 572000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09291926 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.938     |\n",
      "|    explained_variance   | 0.0548     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 7680       |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 280    |\n",
      "|    time_elapsed    | 12005  |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=-0.80 +/- 0.26\n",
      "Episode length: 3.00 +/- 2.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 574000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060420502 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 7690        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.467       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 281    |\n",
      "|    time_elapsed    | 12049  |\n",
      "|    total_timesteps | 575488 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 7.00 +/- 9.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7          |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 576000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09121944 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.91      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0546    |\n",
      "|    n_updates            | 7700       |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 282    |\n",
      "|    time_elapsed    | 12095  |\n",
      "|    total_timesteps | 577536 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=-0.48 +/- 0.90\n",
      "Episode length: 6.20 +/- 8.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -0.48       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 578000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086867034 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.0619      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 7710        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 0.391       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 283    |\n",
      "|    time_elapsed    | 12140  |\n",
      "|    total_timesteps | 579584 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 580000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08552323 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | -0.0943    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 7720       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.276      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 284    |\n",
      "|    time_elapsed    | 12186  |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=-0.20 +/- 0.80\n",
      "Episode length: 9.00 +/- 8.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 582000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11292412 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.0918     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 7730       |\n",
      "|    policy_gradient_loss | -0.0524    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 285    |\n",
      "|    time_elapsed    | 12232  |\n",
      "|    total_timesteps | 583680 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-0.70 +/- 0.28\n",
      "Episode length: 4.00 +/- 2.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 584000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08983117 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 7740       |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 286    |\n",
      "|    time_elapsed    | 12280  |\n",
      "|    total_timesteps | 585728 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=-0.02 +/- 1.76\n",
      "Episode length: 10.80 +/- 17.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 586000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070758596 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.0592     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0302     |\n",
      "|    n_updates            | 7750        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.335       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 287    |\n",
      "|    time_elapsed    | 12324  |\n",
      "|    total_timesteps | 587776 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-0.68 +/- 0.32\n",
      "Episode length: 4.20 +/- 3.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 588000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06680924 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.81      |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 7760       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    value_loss           | 0.294      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 12369  |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 590000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11064414 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.902     |\n",
      "|    explained_variance   | 0.00165    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 7770       |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 289    |\n",
      "|    time_elapsed    | 12419  |\n",
      "|    total_timesteps | 591872 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=0.70 +/- 2.53\n",
      "Episode length: 18.00 +/- 25.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18         |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 592000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11475143 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.982     |\n",
      "|    explained_variance   | 0.0637     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00681   |\n",
      "|    n_updates            | 7780       |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 290    |\n",
      "|    time_elapsed    | 12464  |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=-0.66 +/- 0.31\n",
      "Episode length: 4.40 +/- 3.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | -0.66       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 594000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058749147 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.848      |\n",
      "|    explained_variance   | 0.0441      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 7790        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.379       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 291    |\n",
      "|    time_elapsed    | 12511  |\n",
      "|    total_timesteps | 595968 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 596000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08554734 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.973     |\n",
      "|    explained_variance   | 0.083      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0272    |\n",
      "|    n_updates            | 7800       |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.226      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.2      |\n",
      "|    mean_reward     | -0.98    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 598000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 292    |\n",
      "|    time_elapsed    | 12560  |\n",
      "|    total_timesteps | 598016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-0.88 +/- 0.04\n",
      "Episode length: 2.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 600000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10272615 |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | -0.00874   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0223    |\n",
      "|    n_updates            | 7810       |\n",
      "|    policy_gradient_loss | -0.0458    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 12606  |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.4         |\n",
      "|    mean_reward          | -0.96       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 602000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064597815 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0881      |\n",
      "|    n_updates            | 7820        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.319       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 294    |\n",
      "|    time_elapsed    | 12651  |\n",
      "|    total_timesteps | 602112 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-0.56 +/- 0.46\n",
      "Episode length: 5.40 +/- 4.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 604000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10605582 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.901     |\n",
      "|    explained_variance   | 0.0349     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.151      |\n",
      "|    n_updates            | 7830       |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.402      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 295    |\n",
      "|    time_elapsed    | 12700  |\n",
      "|    total_timesteps | 604160 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=-0.08 +/- 1.69\n",
      "Episode length: 10.20 +/- 16.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 606000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08697881 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.279      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0145    |\n",
      "|    n_updates            | 7840       |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 296    |\n",
      "|    time_elapsed    | 12750  |\n",
      "|    total_timesteps | 606208 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-0.74 +/- 0.23\n",
      "Episode length: 3.60 +/- 2.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 608000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07992158 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.0551     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0211    |\n",
      "|    n_updates            | 7850       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.399      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 297    |\n",
      "|    time_elapsed    | 12795  |\n",
      "|    total_timesteps | 608256 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-0.28 +/- 0.56\n",
      "Episode length: 8.20 +/- 5.56\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.2        |\n",
      "|    mean_reward          | -0.28      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 610000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09662576 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | -0.012     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 7860       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 298    |\n",
      "|    time_elapsed    | 12839  |\n",
      "|    total_timesteps | 610304 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-0.52 +/- 0.51\n",
      "Episode length: 5.80 +/- 5.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 612000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08582861 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.835     |\n",
      "|    explained_variance   | 0.219      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0169    |\n",
      "|    n_updates            | 7870       |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 299    |\n",
      "|    time_elapsed    | 12883  |\n",
      "|    total_timesteps | 612352 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=-0.78 +/- 0.25\n",
      "Episode length: 3.20 +/- 2.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | -0.78       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 614000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063213676 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | 0.0534      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000994    |\n",
      "|    n_updates            | 7880        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.412       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 300    |\n",
      "|    time_elapsed    | 12925  |\n",
      "|    total_timesteps | 614400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-0.58 +/- 0.38\n",
      "Episode length: 5.20 +/- 3.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.2        |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 616000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08350105 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.764     |\n",
      "|    explained_variance   | 0.0138     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 7890       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 301    |\n",
      "|    time_elapsed    | 12972  |\n",
      "|    total_timesteps | 616448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=-0.82 +/- 0.18\n",
      "Episode length: 2.80 +/- 1.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 618000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09305147 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.917     |\n",
      "|    explained_variance   | -0.0115    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00234   |\n",
      "|    n_updates            | 7900       |\n",
      "|    policy_gradient_loss | -0.038     |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 302    |\n",
      "|    time_elapsed    | 13020  |\n",
      "|    total_timesteps | 618496 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 620000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08916646 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.905     |\n",
      "|    explained_variance   | 0.241      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0368    |\n",
      "|    n_updates            | 7910       |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 303    |\n",
      "|    time_elapsed    | 13064  |\n",
      "|    total_timesteps | 620544 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=0.08 +/- 1.87\n",
      "Episode length: 11.80 +/- 18.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 622000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086156234 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.784      |\n",
      "|    explained_variance   | 0.0718      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00192     |\n",
      "|    n_updates            | 7920        |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 304    |\n",
      "|    time_elapsed    | 13107  |\n",
      "|    total_timesteps | 622592 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-0.88 +/- 0.19\n",
      "Episode length: 2.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 624000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09401724 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.749     |\n",
      "|    explained_variance   | 0.152      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 7930       |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    value_loss           | 0.218      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 305    |\n",
      "|    time_elapsed    | 13149  |\n",
      "|    total_timesteps | 624640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=0.26 +/- 1.52\n",
      "Episode length: 13.60 +/- 15.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.6       |\n",
      "|    mean_reward          | 0.26       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 626000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07181747 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.788     |\n",
      "|    explained_variance   | 0.237      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00595    |\n",
      "|    n_updates            | 7940       |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 0.231      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 306    |\n",
      "|    time_elapsed    | 13197  |\n",
      "|    total_timesteps | 626688 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.4         |\n",
      "|    mean_reward          | -0.86       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 628000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092594065 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.0761      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 7950        |\n",
      "|    policy_gradient_loss | -0.0398     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 307    |\n",
      "|    time_elapsed    | 13241  |\n",
      "|    total_timesteps | 628736 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-0.56 +/- 0.88\n",
      "Episode length: 5.40 +/- 8.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | -0.56       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 630000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091566585 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 7960        |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 308    |\n",
      "|    time_elapsed    | 13283  |\n",
      "|    total_timesteps | 630784 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=-0.70 +/- 0.31\n",
      "Episode length: 4.00 +/- 3.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 632000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06862968 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.706     |\n",
      "|    explained_variance   | -0.177     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00664    |\n",
      "|    n_updates            | 7970       |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 309    |\n",
      "|    time_elapsed    | 13327  |\n",
      "|    total_timesteps | 632832 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=-0.88 +/- 0.16\n",
      "Episode length: 2.20 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 634000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09136641 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.836     |\n",
      "|    explained_variance   | 0.141      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00862   |\n",
      "|    n_updates            | 7980       |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 310    |\n",
      "|    time_elapsed    | 13373  |\n",
      "|    total_timesteps | 634880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-0.46 +/- 0.76\n",
      "Episode length: 6.40 +/- 7.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 636000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08810202 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.852     |\n",
      "|    explained_variance   | 0.153      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 7990       |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 311    |\n",
      "|    time_elapsed    | 13415  |\n",
      "|    total_timesteps | 636928 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=-0.76 +/- 0.30\n",
      "Episode length: 3.40 +/- 3.01\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 638000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09711606 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.734     |\n",
      "|    explained_variance   | 0.00346    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0187    |\n",
      "|    n_updates            | 8000       |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 312    |\n",
      "|    time_elapsed    | 13460  |\n",
      "|    total_timesteps | 638976 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-0.72 +/- 0.15\n",
      "Episode length: 3.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 640000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06909968 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.913     |\n",
      "|    explained_variance   | 0.0243     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0155    |\n",
      "|    n_updates            | 8010       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.533      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 313    |\n",
      "|    time_elapsed    | 13505  |\n",
      "|    total_timesteps | 641024 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 6.40 +/- 8.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 642000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08497919 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.889     |\n",
      "|    explained_variance   | 0.0481     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0201    |\n",
      "|    n_updates            | 8020       |\n",
      "|    policy_gradient_loss | -0.0355    |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 314    |\n",
      "|    time_elapsed    | 13549  |\n",
      "|    total_timesteps | 643072 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=-0.92 +/- 0.10\n",
      "Episode length: 1.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 644000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078621216 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | -0.0553     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00621    |\n",
      "|    n_updates            | 8030        |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 0.376       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 315    |\n",
      "|    time_elapsed    | 13594  |\n",
      "|    total_timesteps | 645120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 646000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07430903 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.894     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 8040       |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    value_loss           | 0.35       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 316    |\n",
      "|    time_elapsed    | 13637  |\n",
      "|    total_timesteps | 647168 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-0.84 +/- 0.15\n",
      "Episode length: 2.60 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 648000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14997198 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.811     |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00527    |\n",
      "|    n_updates            | 8050       |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.335      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 317    |\n",
      "|    time_elapsed    | 13682  |\n",
      "|    total_timesteps | 649216 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-0.88 +/- 0.15\n",
      "Episode length: 2.20 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 650000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12796108 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.887     |\n",
      "|    explained_variance   | 0.00399    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 8060       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 318    |\n",
      "|    time_elapsed    | 13727  |\n",
      "|    total_timesteps | 651264 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-0.22 +/- 0.84\n",
      "Episode length: 8.80 +/- 8.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.8        |\n",
      "|    mean_reward          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 652000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09606523 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.88      |\n",
      "|    explained_variance   | 0.0427     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 8070       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 319    |\n",
      "|    time_elapsed    | 13773  |\n",
      "|    total_timesteps | 653312 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=-0.64 +/- 0.33\n",
      "Episode length: 4.60 +/- 3.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 654000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11218934 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.935     |\n",
      "|    explained_variance   | 0.239      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0424    |\n",
      "|    n_updates            | 8080       |\n",
      "|    policy_gradient_loss | -0.0536    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 320    |\n",
      "|    time_elapsed    | 13817  |\n",
      "|    total_timesteps | 655360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -1          |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 656000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069603086 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.795      |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00833    |\n",
      "|    n_updates            | 8090        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.331       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 321    |\n",
      "|    time_elapsed    | 13864  |\n",
      "|    total_timesteps | 657408 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=-0.42 +/- 0.97\n",
      "Episode length: 6.80 +/- 9.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 658000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10856491 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.868     |\n",
      "|    explained_variance   | -0.0474    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00378   |\n",
      "|    n_updates            | 8100       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    value_loss           | 0.383      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 322    |\n",
      "|    time_elapsed    | 13911  |\n",
      "|    total_timesteps | 659456 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-0.76 +/- 0.43\n",
      "Episode length: 3.40 +/- 4.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 660000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09897801 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.825     |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 8110       |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 0.26       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 323    |\n",
      "|    time_elapsed    | 13958  |\n",
      "|    total_timesteps | 661504 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 6.40 +/- 10.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | -0.46       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 662000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080064505 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.86       |\n",
      "|    explained_variance   | 0.0586      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 8120        |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.427       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 324    |\n",
      "|    time_elapsed    | 14004  |\n",
      "|    total_timesteps | 663552 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-0.56 +/- 0.88\n",
      "Episode length: 5.40 +/- 8.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 664000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08125682 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.849     |\n",
      "|    explained_variance   | 0.0565     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 8130       |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 0.259      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 325    |\n",
      "|    time_elapsed    | 14049  |\n",
      "|    total_timesteps | 665600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=-0.16 +/- 1.28\n",
      "Episode length: 9.40 +/- 12.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.4        |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 666000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07268049 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.124      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 8140       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    value_loss           | 0.421      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 326    |\n",
      "|    time_elapsed    | 14092  |\n",
      "|    total_timesteps | 667648 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-0.82 +/- 0.22\n",
      "Episode length: 2.80 +/- 2.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 668000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09411461 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.777     |\n",
      "|    explained_variance   | -0.0151    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00675   |\n",
      "|    n_updates            | 8150       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 0.292      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 327    |\n",
      "|    time_elapsed    | 14137  |\n",
      "|    total_timesteps | 669696 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102475695 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | -0.0106     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 8160        |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 0.375       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 328    |\n",
      "|    time_elapsed    | 14179  |\n",
      "|    total_timesteps | 671744 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-0.90 +/- 0.15\n",
      "Episode length: 2.00 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 672000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08248913 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.794     |\n",
      "|    explained_variance   | -0.0836    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0179     |\n",
      "|    n_updates            | 8170       |\n",
      "|    policy_gradient_loss | -0.0323    |\n",
      "|    value_loss           | 0.236      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 329    |\n",
      "|    time_elapsed    | 14226  |\n",
      "|    total_timesteps | 673792 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=-0.52 +/- 0.78\n",
      "Episode length: 5.80 +/- 7.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 674000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10249695 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.962     |\n",
      "|    explained_variance   | 0.0985     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 8180       |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 330    |\n",
      "|    time_elapsed    | 14269  |\n",
      "|    total_timesteps | 675840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-0.76 +/- 0.30\n",
      "Episode length: 3.40 +/- 3.01\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.4       |\n",
      "|    mean_reward          | -0.76     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 676000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0863535 |\n",
      "|    clip_fraction        | 0.272     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.776    |\n",
      "|    explained_variance   | -0.000347 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.106     |\n",
      "|    n_updates            | 8190      |\n",
      "|    policy_gradient_loss | -0.0372   |\n",
      "|    value_loss           | 0.329     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 331    |\n",
      "|    time_elapsed    | 14313  |\n",
      "|    total_timesteps | 677888 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-0.88 +/- 0.15\n",
      "Episode length: 2.20 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 678000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06395019 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.771     |\n",
      "|    explained_variance   | -0.256     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.291      |\n",
      "|    n_updates            | 8200       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.382      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 332    |\n",
      "|    time_elapsed    | 14360  |\n",
      "|    total_timesteps | 679936 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-0.62 +/- 0.31\n",
      "Episode length: 4.80 +/- 3.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 680000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11375966 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.858     |\n",
      "|    explained_variance   | -1.07      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 8210       |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 333    |\n",
      "|    time_elapsed    | 14405  |\n",
      "|    total_timesteps | 681984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 682000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10199253 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.862     |\n",
      "|    explained_variance   | 0.0546     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0215    |\n",
      "|    n_updates            | 8220       |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.362      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-0.54 +/- 0.77\n",
      "Episode length: 5.60 +/- 7.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -0.54    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 684000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 334    |\n",
      "|    time_elapsed    | 14449  |\n",
      "|    total_timesteps | 684032 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=-0.74 +/- 0.19\n",
      "Episode length: 3.60 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 686000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08470078 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.869     |\n",
      "|    explained_variance   | 0.118      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 8230       |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 335    |\n",
      "|    time_elapsed    | 14492  |\n",
      "|    total_timesteps | 686080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-0.20 +/- 0.90\n",
      "Episode length: 9.00 +/- 9.03\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 688000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12843052 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.797     |\n",
      "|    explained_variance   | -0.108     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 8240       |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 336    |\n",
      "|    time_elapsed    | 14539  |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-0.40 +/- 0.91\n",
      "Episode length: 7.00 +/- 9.12\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 7         |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 690000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0933007 |\n",
      "|    clip_fraction        | 0.335     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.972    |\n",
      "|    explained_variance   | 0.199     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.032    |\n",
      "|    n_updates            | 8250      |\n",
      "|    policy_gradient_loss | -0.0474   |\n",
      "|    value_loss           | 0.157     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 337    |\n",
      "|    time_elapsed    | 14583  |\n",
      "|    total_timesteps | 690176 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.4       |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 692000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0762334 |\n",
      "|    clip_fraction        | 0.289     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.798    |\n",
      "|    explained_variance   | 0.285     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0261   |\n",
      "|    n_updates            | 8260      |\n",
      "|    policy_gradient_loss | -0.0423   |\n",
      "|    value_loss           | 0.197     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 338    |\n",
      "|    time_elapsed    | 14625  |\n",
      "|    total_timesteps | 692224 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=-0.50 +/- 0.52\n",
      "Episode length: 6.00 +/- 5.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 694000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08403555 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.772     |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 8270       |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 0.311      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 339    |\n",
      "|    time_elapsed    | 14669  |\n",
      "|    total_timesteps | 694272 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-0.94 +/- 0.05\n",
      "Episode length: 1.60 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.6       |\n",
      "|    mean_reward          | -0.94     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 696000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0804638 |\n",
      "|    clip_fraction        | 0.3       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.824    |\n",
      "|    explained_variance   | 0.16      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -4.08e-05 |\n",
      "|    n_updates            | 8280      |\n",
      "|    policy_gradient_loss | -0.0444   |\n",
      "|    value_loss           | 0.194     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 340    |\n",
      "|    time_elapsed    | 14715  |\n",
      "|    total_timesteps | 696320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=-0.64 +/- 0.30\n",
      "Episode length: 4.60 +/- 3.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | -0.64       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 698000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092214175 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.869      |\n",
      "|    explained_variance   | 0.0412      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 8290        |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.427       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 341    |\n",
      "|    time_elapsed    | 14758  |\n",
      "|    total_timesteps | 698368 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-0.56 +/- 0.37\n",
      "Episode length: 5.40 +/- 3.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 700000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07370553 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.754     |\n",
      "|    explained_variance   | 0.192      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 8300       |\n",
      "|    policy_gradient_loss | -0.0344    |\n",
      "|    value_loss           | 0.367      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 342    |\n",
      "|    time_elapsed    | 14802  |\n",
      "|    total_timesteps | 700416 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=-0.76 +/- 0.29\n",
      "Episode length: 3.40 +/- 2.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 702000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06898282 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.82      |\n",
      "|    explained_variance   | -0.104     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0248     |\n",
      "|    n_updates            | 8310       |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    value_loss           | 0.589      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 343    |\n",
      "|    time_elapsed    | 14846  |\n",
      "|    total_timesteps | 702464 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-0.50 +/- 0.80\n",
      "Episode length: 6.00 +/- 8.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 704000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09345009 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.841     |\n",
      "|    explained_variance   | -0.343     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0266    |\n",
      "|    n_updates            | 8320       |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 344    |\n",
      "|    time_elapsed    | 14891  |\n",
      "|    total_timesteps | 704512 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 706000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09781935 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.875     |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0235    |\n",
      "|    n_updates            | 8330       |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 345    |\n",
      "|    time_elapsed    | 14937  |\n",
      "|    total_timesteps | 706560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 3.00 +/- 4.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 708000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086732104 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.95       |\n",
      "|    explained_variance   | 0.0647      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 8340        |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 0.408       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 346    |\n",
      "|    time_elapsed    | 14982  |\n",
      "|    total_timesteps | 708608 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-0.80 +/- 0.13\n",
      "Episode length: 3.00 +/- 1.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 710000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11236881 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.858     |\n",
      "|    explained_variance   | -0.115     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 8350       |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 347    |\n",
      "|    time_elapsed    | 15025  |\n",
      "|    total_timesteps | 710656 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=-0.72 +/- 0.37\n",
      "Episode length: 3.80 +/- 3.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 712000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06957668 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.787     |\n",
      "|    explained_variance   | 0.0429     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 8360       |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    value_loss           | 0.434      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 348    |\n",
      "|    time_elapsed    | 15070  |\n",
      "|    total_timesteps | 712704 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=-0.88 +/- 0.15\n",
      "Episode length: 2.20 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 714000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10565449 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | -0.0658    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0316    |\n",
      "|    n_updates            | 8370       |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 349    |\n",
      "|    time_elapsed    | 15118  |\n",
      "|    total_timesteps | 714752 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=1.36 +/- 3.48\n",
      "Episode length: 24.60 +/- 34.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 24.6       |\n",
      "|    mean_reward          | 1.36       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 716000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09028996 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.995     |\n",
      "|    explained_variance   | 0.0209     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 8380       |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.182      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 350    |\n",
      "|    time_elapsed    | 15165  |\n",
      "|    total_timesteps | 716800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=-0.92 +/- 0.07\n",
      "Episode length: 1.80 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 718000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13833787 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.941     |\n",
      "|    explained_variance   | 0.141      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 8390       |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 351    |\n",
      "|    time_elapsed    | 15212  |\n",
      "|    total_timesteps | 718848 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-0.36 +/- 0.69\n",
      "Episode length: 7.40 +/- 6.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 720000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08405151 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.87      |\n",
      "|    explained_variance   | 0.0216     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00436   |\n",
      "|    n_updates            | 8400       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 0.227      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 352    |\n",
      "|    time_elapsed    | 15253  |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=-0.70 +/- 0.46\n",
      "Episode length: 4.00 +/- 4.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 722000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077233955 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 8410        |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 353    |\n",
      "|    time_elapsed    | 15292  |\n",
      "|    total_timesteps | 722944 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-1.44 +/- 1.32\n",
      "Episode length: 14.60 +/- 23.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 14.6       |\n",
      "|    mean_reward          | -1.44      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 724000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09522839 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.891     |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 8420       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 354    |\n",
      "|    time_elapsed    | 15332  |\n",
      "|    total_timesteps | 724992 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=-0.68 +/- 0.31\n",
      "Episode length: 4.20 +/- 3.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 726000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08648501 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.93      |\n",
      "|    explained_variance   | 0.149      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00691   |\n",
      "|    n_updates            | 8430       |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 355    |\n",
      "|    time_elapsed    | 15366  |\n",
      "|    total_timesteps | 727040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-0.94 +/- 0.12\n",
      "Episode length: 1.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 728000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09397164 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.665     |\n",
      "|    explained_variance   | 0.195      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 8440       |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 0.231      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 356    |\n",
      "|    time_elapsed    | 15403  |\n",
      "|    total_timesteps | 729088 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-0.24 +/- 0.96\n",
      "Episode length: 8.60 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -0.24       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080260634 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.868      |\n",
      "|    explained_variance   | 0.0517      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0297      |\n",
      "|    n_updates            | 8450        |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.444       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 357    |\n",
      "|    time_elapsed    | 15447  |\n",
      "|    total_timesteps | 731136 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-0.90 +/- 0.11\n",
      "Episode length: 2.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 732000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084098175 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.898      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 8460        |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 358    |\n",
      "|    time_elapsed    | 15493  |\n",
      "|    total_timesteps | 733184 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=-0.02 +/- 0.98\n",
      "Episode length: 10.80 +/- 9.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 734000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094647676 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.843      |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 8470        |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 359    |\n",
      "|    time_elapsed    | 15533  |\n",
      "|    total_timesteps | 735232 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-0.48 +/- 0.90\n",
      "Episode length: 6.20 +/- 8.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 736000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10789199 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.678     |\n",
      "|    explained_variance   | 0.208      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0238    |\n",
      "|    n_updates            | 8480       |\n",
      "|    policy_gradient_loss | -0.0511    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 360    |\n",
      "|    time_elapsed    | 15577  |\n",
      "|    total_timesteps | 737280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=-0.90 +/- 0.20\n",
      "Episode length: 2.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 738000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09088076 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.828     |\n",
      "|    explained_variance   | 0.27       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00379   |\n",
      "|    n_updates            | 8490       |\n",
      "|    policy_gradient_loss | -0.0504    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 361    |\n",
      "|    time_elapsed    | 15623  |\n",
      "|    total_timesteps | 739328 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-0.70 +/- 0.18\n",
      "Episode length: 4.00 +/- 1.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07741226 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.879     |\n",
      "|    explained_variance   | 0.0738     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0236     |\n",
      "|    n_updates            | 8500       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    value_loss           | 0.393      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 362    |\n",
      "|    time_elapsed    | 15667  |\n",
      "|    total_timesteps | 741376 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=-0.68 +/- 0.30\n",
      "Episode length: 4.20 +/- 2.99\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.2       |\n",
      "|    mean_reward          | -0.68     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 742000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0978616 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.742    |\n",
      "|    explained_variance   | 0.21      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00722  |\n",
      "|    n_updates            | 8510      |\n",
      "|    policy_gradient_loss | -0.0321   |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 363    |\n",
      "|    time_elapsed    | 15713  |\n",
      "|    total_timesteps | 743424 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=0.44 +/- 1.68\n",
      "Episode length: 15.40 +/- 16.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.4       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 744000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08659725 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.882     |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 8520       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 364    |\n",
      "|    time_elapsed    | 15761  |\n",
      "|    total_timesteps | 745472 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=-0.36 +/- 0.76\n",
      "Episode length: 7.40 +/- 7.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 7.4       |\n",
      "|    mean_reward          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 746000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1003137 |\n",
      "|    clip_fraction        | 0.322     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.915    |\n",
      "|    explained_variance   | -0.00837  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0219   |\n",
      "|    n_updates            | 8530      |\n",
      "|    policy_gradient_loss | -0.0436   |\n",
      "|    value_loss           | 0.255     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 365    |\n",
      "|    time_elapsed    | 15808  |\n",
      "|    total_timesteps | 747520 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.4       |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 748000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0958273 |\n",
      "|    clip_fraction        | 0.321     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.874    |\n",
      "|    explained_variance   | 0.0545    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0371   |\n",
      "|    n_updates            | 8540      |\n",
      "|    policy_gradient_loss | -0.0426   |\n",
      "|    value_loss           | 0.326     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 366    |\n",
      "|    time_elapsed    | 15852  |\n",
      "|    total_timesteps | 749568 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-0.28 +/- 1.19\n",
      "Episode length: 8.20 +/- 11.92\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 8.2       |\n",
      "|    mean_reward          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 750000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0792792 |\n",
      "|    clip_fraction        | 0.297     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.848    |\n",
      "|    explained_variance   | 0.0641    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.000947  |\n",
      "|    n_updates            | 8550      |\n",
      "|    policy_gradient_loss | -0.0365   |\n",
      "|    value_loss           | 0.233     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 367    |\n",
      "|    time_elapsed    | 15895  |\n",
      "|    total_timesteps | 751616 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-0.36 +/- 0.63\n",
      "Episode length: 7.40 +/- 6.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 752000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11033704 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.786     |\n",
      "|    explained_variance   | -0.295     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 8560       |\n",
      "|    policy_gradient_loss | -0.0489    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 368    |\n",
      "|    time_elapsed    | 15937  |\n",
      "|    total_timesteps | 753664 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 754000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10300463 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.78      |\n",
      "|    explained_variance   | 0.118      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 8570       |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 369    |\n",
      "|    time_elapsed    | 15984  |\n",
      "|    total_timesteps | 755712 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 756000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07233767 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | 0.127      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 8580       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.425      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 370    |\n",
      "|    time_elapsed    | 16028  |\n",
      "|    total_timesteps | 757760 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=0.08 +/- 1.58\n",
      "Episode length: 11.80 +/- 15.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.8       |\n",
      "|    mean_reward          | 0.08       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 758000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09777466 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.854     |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 8590       |\n",
      "|    policy_gradient_loss | -0.0494    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 371    |\n",
      "|    time_elapsed    | 16073  |\n",
      "|    total_timesteps | 759808 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=0.44 +/- 1.28\n",
      "Episode length: 15.40 +/- 12.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.4       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 760000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06283639 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.712     |\n",
      "|    explained_variance   | -0.126     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00278   |\n",
      "|    n_updates            | 8600       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    value_loss           | 0.68       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 372    |\n",
      "|    time_elapsed    | 16116  |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=1.08 +/- 3.18\n",
      "Episode length: 21.80 +/- 31.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.8        |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 762000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098998174 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.76       |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00787    |\n",
      "|    n_updates            | 8610        |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 373    |\n",
      "|    time_elapsed    | 16160  |\n",
      "|    total_timesteps | 763904 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-0.74 +/- 0.33\n",
      "Episode length: 3.60 +/- 3.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 764000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08943343 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.803     |\n",
      "|    explained_variance   | -0.0634    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 8620       |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 374    |\n",
      "|    time_elapsed    | 16208  |\n",
      "|    total_timesteps | 765952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=-0.88 +/- 0.12\n",
      "Episode length: 2.20 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 766000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08321397 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.866     |\n",
      "|    explained_variance   | 0.178      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00306   |\n",
      "|    n_updates            | 8630       |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    value_loss           | 0.218      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-0.46 +/- 0.62\n",
      "Episode length: 6.40 +/- 6.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -0.46    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 768000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 375    |\n",
      "|    time_elapsed    | 16255  |\n",
      "|    total_timesteps | 768000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-2.22 +/- 2.69\n",
      "Episode length: 6.80 +/- 9.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -2.22      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 770000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10217784 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.842     |\n",
      "|    explained_variance   | -0.0271    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00956   |\n",
      "|    n_updates            | 8640       |\n",
      "|    policy_gradient_loss | -0.0494    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 376    |\n",
      "|    time_elapsed    | 16304  |\n",
      "|    total_timesteps | 770048 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-0.86 +/- 0.14\n",
      "Episode length: 2.40 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 772000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06108964 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.843     |\n",
      "|    explained_variance   | 0.0736     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 8650       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.45       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 377    |\n",
      "|    time_elapsed    | 16349  |\n",
      "|    total_timesteps | 772096 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=-0.84 +/- 0.22\n",
      "Episode length: 2.60 +/- 2.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 774000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10124996 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.834     |\n",
      "|    explained_variance   | -0.00793   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00837   |\n",
      "|    n_updates            | 8660       |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 378    |\n",
      "|    time_elapsed    | 16394  |\n",
      "|    total_timesteps | 774144 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-0.78 +/- 0.30\n",
      "Episode length: 3.20 +/- 2.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 776000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09666066 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.821     |\n",
      "|    explained_variance   | -0.402     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 8670       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 379    |\n",
      "|    time_elapsed    | 16441  |\n",
      "|    total_timesteps | 776192 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=-0.74 +/- 0.24\n",
      "Episode length: 3.60 +/- 2.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | -0.74       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 778000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090811506 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 8680        |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 380    |\n",
      "|    time_elapsed    | 16486  |\n",
      "|    total_timesteps | 778240 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-0.08 +/- 1.03\n",
      "Episode length: 10.20 +/- 10.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 780000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08179353 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.82      |\n",
      "|    explained_variance   | -0.149     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 8690       |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.355      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 381    |\n",
      "|    time_elapsed    | 16530  |\n",
      "|    total_timesteps | 780288 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=0.22 +/- 2.44\n",
      "Episode length: 13.20 +/- 24.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.2        |\n",
      "|    mean_reward          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 782000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098789304 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.817      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 8700        |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 382    |\n",
      "|    time_elapsed    | 16575  |\n",
      "|    total_timesteps | 782336 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=0.30 +/- 2.50\n",
      "Episode length: 14.00 +/- 25.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 784000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079571426 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.816      |\n",
      "|    explained_variance   | 0.0258      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 8710        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.521       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 383    |\n",
      "|    time_elapsed    | 16621  |\n",
      "|    total_timesteps | 784384 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=-0.72 +/- 0.15\n",
      "Episode length: 3.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 786000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09505859 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.942     |\n",
      "|    explained_variance   | 0.122      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 8720       |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 384    |\n",
      "|    time_elapsed    | 16668  |\n",
      "|    total_timesteps | 786432 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-0.22 +/- 1.06\n",
      "Episode length: 8.80 +/- 10.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.8        |\n",
      "|    mean_reward          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 788000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10286712 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.871     |\n",
      "|    explained_variance   | 0.0264     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 8730       |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 385    |\n",
      "|    time_elapsed    | 16715  |\n",
      "|    total_timesteps | 788480 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-0.84 +/- 0.20\n",
      "Episode length: 2.60 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 790000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08912174 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.941     |\n",
      "|    explained_variance   | 0.0421     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 8740       |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    value_loss           | 0.237      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 386    |\n",
      "|    time_elapsed    | 16758  |\n",
      "|    total_timesteps | 790528 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=-0.74 +/- 0.26\n",
      "Episode length: 3.60 +/- 2.58\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 792000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08937764 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.789     |\n",
      "|    explained_variance   | 0.0188     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00794   |\n",
      "|    n_updates            | 8750       |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    value_loss           | 0.261      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 387    |\n",
      "|    time_elapsed    | 16802  |\n",
      "|    total_timesteps | 792576 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=-0.40 +/- 0.97\n",
      "Episode length: 7.00 +/- 9.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 794000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093792826 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.791      |\n",
      "|    explained_variance   | 0.087       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 8760        |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 388    |\n",
      "|    time_elapsed    | 16847  |\n",
      "|    total_timesteps | 794624 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=-0.80 +/- 0.24\n",
      "Episode length: 3.00 +/- 2.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 796000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08061668 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.904     |\n",
      "|    explained_variance   | 0.0283     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 8770       |\n",
      "|    policy_gradient_loss | -0.0375    |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 389    |\n",
      "|    time_elapsed    | 16898  |\n",
      "|    total_timesteps | 796672 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=0.48 +/- 2.41\n",
      "Episode length: 33.80 +/- 60.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33.8       |\n",
      "|    mean_reward          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 798000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08263408 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.882     |\n",
      "|    explained_variance   | 0.0395     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.532      |\n",
      "|    n_updates            | 8780       |\n",
      "|    policy_gradient_loss | -0.031     |\n",
      "|    value_loss           | 0.458      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 390    |\n",
      "|    time_elapsed    | 16948  |\n",
      "|    total_timesteps | 798720 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-0.74 +/- 0.39\n",
      "Episode length: 3.60 +/- 3.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 800000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08953214 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | 0.0884     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00871   |\n",
      "|    n_updates            | 8790       |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 391    |\n",
      "|    time_elapsed    | 16993  |\n",
      "|    total_timesteps | 800768 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=-0.30 +/- 1.35\n",
      "Episode length: 8.00 +/- 13.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 802000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07430545 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.802     |\n",
      "|    explained_variance   | 0.0597     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.109      |\n",
      "|    n_updates            | 8800       |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.432      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 392    |\n",
      "|    time_elapsed    | 17034  |\n",
      "|    total_timesteps | 802816 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-0.16 +/- 0.99\n",
      "Episode length: 9.40 +/- 9.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.4        |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 804000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07927129 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.753     |\n",
      "|    explained_variance   | -0.24      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00513   |\n",
      "|    n_updates            | 8810       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.366      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 393    |\n",
      "|    time_elapsed    | 17079  |\n",
      "|    total_timesteps | 804864 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=-0.48 +/- 0.89\n",
      "Episode length: 6.20 +/- 8.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 806000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10922337 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.851     |\n",
      "|    explained_variance   | -0.146     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 8820       |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 394    |\n",
      "|    time_elapsed    | 17127  |\n",
      "|    total_timesteps | 806912 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=-0.86 +/- 0.17\n",
      "Episode length: 2.40 +/- 1.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 808000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08244045 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.94      |\n",
      "|    explained_variance   | -0.00551   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 8830       |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.324      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 395    |\n",
      "|    time_elapsed    | 17173  |\n",
      "|    total_timesteps | 808960 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 810000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09092227 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.833     |\n",
      "|    explained_variance   | -0.0346    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0166     |\n",
      "|    n_updates            | 8840       |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    value_loss           | 0.512      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 396    |\n",
      "|    time_elapsed    | 17219  |\n",
      "|    total_timesteps | 811008 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=-0.24 +/- 1.03\n",
      "Episode length: 8.60 +/- 10.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.6        |\n",
      "|    mean_reward          | -0.24      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 812000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09344275 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.366      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 8850       |\n",
      "|    policy_gradient_loss | -0.0396    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 397    |\n",
      "|    time_elapsed    | 17273  |\n",
      "|    total_timesteps | 813056 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=-0.62 +/- 0.52\n",
      "Episode length: 4.80 +/- 5.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | -0.62       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 814000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092244476 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | -0.00999    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 8860        |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 0.298       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 398    |\n",
      "|    time_elapsed    | 17315  |\n",
      "|    total_timesteps | 815104 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=0.70 +/- 1.87\n",
      "Episode length: 18.00 +/- 18.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111154735 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.859      |\n",
      "|    explained_variance   | 0.0491      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 8870        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.357       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 399    |\n",
      "|    time_elapsed    | 17359  |\n",
      "|    total_timesteps | 817152 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=-0.86 +/- 0.23\n",
      "Episode length: 2.40 +/- 2.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.4         |\n",
      "|    mean_reward          | -0.86       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 818000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086678624 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00529    |\n",
      "|    n_updates            | 8880        |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 400    |\n",
      "|    time_elapsed    | 17402  |\n",
      "|    total_timesteps | 819200 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-0.64 +/- 0.31\n",
      "Episode length: 4.60 +/- 3.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 820000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08506861 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.855     |\n",
      "|    explained_variance   | 0.123      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 8890       |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.33       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 401    |\n",
      "|    time_elapsed    | 17442  |\n",
      "|    total_timesteps | 821248 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=-0.78 +/- 0.24\n",
      "Episode length: 3.20 +/- 2.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 822000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09134034 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.804     |\n",
      "|    explained_variance   | -0.529     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0211    |\n",
      "|    n_updates            | 8900       |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 402    |\n",
      "|    time_elapsed    | 17486  |\n",
      "|    total_timesteps | 823296 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=-0.48 +/- 0.56\n",
      "Episode length: 6.20 +/- 5.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | -0.48      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 824000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07217045 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.774     |\n",
      "|    explained_variance   | 0.106      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 8910       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.361      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 403    |\n",
      "|    time_elapsed    | 17530  |\n",
      "|    total_timesteps | 825344 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=0.60 +/- 1.02\n",
      "Episode length: 17.00 +/- 10.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17         |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 826000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07036676 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.864     |\n",
      "|    explained_variance   | 0.137      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 8920       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.361      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 404    |\n",
      "|    time_elapsed    | 17578  |\n",
      "|    total_timesteps | 827392 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=-0.72 +/- 0.37\n",
      "Episode length: 3.80 +/- 3.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 828000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08750239 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.771     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0231    |\n",
      "|    n_updates            | 8930       |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    value_loss           | 0.239      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 405    |\n",
      "|    time_elapsed    | 17622  |\n",
      "|    total_timesteps | 829440 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.2       |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 830000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1097112 |\n",
      "|    clip_fraction        | 0.292     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.822    |\n",
      "|    explained_variance   | -0.0228   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0176   |\n",
      "|    n_updates            | 8940      |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    value_loss           | 0.296     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 406    |\n",
      "|    time_elapsed    | 17668  |\n",
      "|    total_timesteps | 831488 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=-0.62 +/- 0.61\n",
      "Episode length: 4.80 +/- 6.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | -0.62       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 832000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076658756 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0198      |\n",
      "|    n_updates            | 8950        |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 0.438       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 407    |\n",
      "|    time_elapsed    | 17715  |\n",
      "|    total_timesteps | 833536 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=-0.90 +/- 0.20\n",
      "Episode length: 2.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 834000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10099957 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.85      |\n",
      "|    explained_variance   | 0.045      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0102    |\n",
      "|    n_updates            | 8960       |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 408    |\n",
      "|    time_elapsed    | 17758  |\n",
      "|    total_timesteps | 835584 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 836000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08648249 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.776     |\n",
      "|    explained_variance   | -0.0132    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0148    |\n",
      "|    n_updates            | 8970       |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.351      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 409    |\n",
      "|    time_elapsed    | 17810  |\n",
      "|    total_timesteps | 837632 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=-0.40 +/- 0.78\n",
      "Episode length: 7.00 +/- 7.77\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 7         |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 838000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0958052 |\n",
      "|    clip_fraction        | 0.301     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.871    |\n",
      "|    explained_variance   | 0.214     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0239   |\n",
      "|    n_updates            | 8980      |\n",
      "|    policy_gradient_loss | -0.0453   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 410    |\n",
      "|    time_elapsed    | 17858  |\n",
      "|    total_timesteps | 839680 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.8       |\n",
      "|    mean_reward          | -0.92     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 840000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0794875 |\n",
      "|    clip_fraction        | 0.297     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.835    |\n",
      "|    explained_variance   | -0.204    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00618  |\n",
      "|    n_updates            | 8990      |\n",
      "|    policy_gradient_loss | -0.0368   |\n",
      "|    value_loss           | 0.249     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 47     |\n",
      "|    iterations      | 411    |\n",
      "|    time_elapsed    | 17903  |\n",
      "|    total_timesteps | 841728 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-0.38 +/- 0.68\n",
      "Episode length: 7.20 +/- 6.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.2        |\n",
      "|    mean_reward          | -0.38      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 842000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07780968 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.778     |\n",
      "|    explained_variance   | 0.0713     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00769    |\n",
      "|    n_updates            | 9000       |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 412    |\n",
      "|    time_elapsed    | 17956  |\n",
      "|    total_timesteps | 843776 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=-0.56 +/- 0.28\n",
      "Episode length: 5.40 +/- 2.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 844000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09924501 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | -0.132     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.531      |\n",
      "|    n_updates            | 9010       |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.4        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 413    |\n",
      "|    time_elapsed    | 18003  |\n",
      "|    total_timesteps | 845824 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-0.88 +/- 0.12\n",
      "Episode length: 2.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.2         |\n",
      "|    mean_reward          | -0.88       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 846000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098448314 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 9020        |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 414    |\n",
      "|    time_elapsed    | 18050  |\n",
      "|    total_timesteps | 847872 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=-0.34 +/- 0.67\n",
      "Episode length: 7.60 +/- 6.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | -0.34       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 848000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061527506 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.774      |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 9030        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.455       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 415    |\n",
      "|    time_elapsed    | 18098  |\n",
      "|    total_timesteps | 849920 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-0.96 +/- 0.08\n",
      "Episode length: 1.40 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 850000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10229446 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.727     |\n",
      "|    explained_variance   | 0.0929     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00332   |\n",
      "|    n_updates            | 9040       |\n",
      "|    policy_gradient_loss | -0.0474    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 416    |\n",
      "|    time_elapsed    | 18143  |\n",
      "|    total_timesteps | 851968 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 852000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08736123 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.78      |\n",
      "|    explained_variance   | 0.0168     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0187    |\n",
      "|    n_updates            | 9050       |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=854000, episode_reward=-0.74 +/- 0.43\n",
      "Episode length: 3.60 +/- 4.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | -0.74    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 854000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 417    |\n",
      "|    time_elapsed    | 18197  |\n",
      "|    total_timesteps | 854016 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=-0.08 +/- 1.09\n",
      "Episode length: 10.20 +/- 10.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.2       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 856000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09819612 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.783     |\n",
      "|    explained_variance   | 0.121      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.016     |\n",
      "|    n_updates            | 9060       |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 418    |\n",
      "|    time_elapsed    | 18243  |\n",
      "|    total_timesteps | 856064 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=-0.80 +/- 0.28\n",
      "Episode length: 3.00 +/- 2.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 858000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078942925 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.808      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.417       |\n",
      "|    n_updates            | 9070        |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 419    |\n",
      "|    time_elapsed    | 18294  |\n",
      "|    total_timesteps | 858112 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-0.90 +/- 0.20\n",
      "Episode length: 2.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 860000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08759467 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 9080       |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    value_loss           | 0.355      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 420    |\n",
      "|    time_elapsed    | 18339  |\n",
      "|    total_timesteps | 860160 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=-0.90 +/- 0.20\n",
      "Episode length: 2.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 862000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07682942 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.588     |\n",
      "|    explained_variance   | 0.0112     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 9090       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    value_loss           | 0.365      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 421    |\n",
      "|    time_elapsed    | 18388  |\n",
      "|    total_timesteps | 862208 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=0.60 +/- 1.74\n",
      "Episode length: 17.00 +/- 17.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17         |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 864000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08291051 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.774     |\n",
      "|    explained_variance   | 0.145      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00579    |\n",
      "|    n_updates            | 9100       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.357      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 422    |\n",
      "|    time_elapsed    | 18441  |\n",
      "|    total_timesteps | 864256 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=-0.92 +/- 0.07\n",
      "Episode length: 1.80 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 866000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08962533 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.793     |\n",
      "|    explained_variance   | -0.0773    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.02       |\n",
      "|    n_updates            | 9110       |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 423    |\n",
      "|    time_elapsed    | 18496  |\n",
      "|    total_timesteps | 866304 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 868000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086453274 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 9120        |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 424    |\n",
      "|    time_elapsed    | 18561  |\n",
      "|    total_timesteps | 868352 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-0.90 +/- 0.06\n",
      "Episode length: 2.00 +/- 0.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 870000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11600369 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.841     |\n",
      "|    explained_variance   | 0.0722     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00888    |\n",
      "|    n_updates            | 9130       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.414      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 425    |\n",
      "|    time_elapsed    | 18608  |\n",
      "|    total_timesteps | 870400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=-0.48 +/- 0.80\n",
      "Episode length: 6.20 +/- 7.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -0.48       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 872000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056633577 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | -0.0848     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00406    |\n",
      "|    n_updates            | 9140        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.448       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 426    |\n",
      "|    time_elapsed    | 18656  |\n",
      "|    total_timesteps | 872448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=-0.84 +/- 0.16\n",
      "Episode length: 2.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 874000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093331486 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.787      |\n",
      "|    explained_variance   | 0.0334      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 9150        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.463       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 427    |\n",
      "|    time_elapsed    | 18701  |\n",
      "|    total_timesteps | 874496 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=-0.62 +/- 0.61\n",
      "Episode length: 4.80 +/- 6.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 876000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10176114 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.85      |\n",
      "|    explained_variance   | -0.419     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 9160       |\n",
      "|    policy_gradient_loss | -0.0465    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 428    |\n",
      "|    time_elapsed    | 18750  |\n",
      "|    total_timesteps | 876544 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=-0.16 +/- 0.97\n",
      "Episode length: 9.40 +/- 9.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.4        |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 878000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10175631 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000919  |\n",
      "|    n_updates            | 9170       |\n",
      "|    policy_gradient_loss | -0.0504    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 429    |\n",
      "|    time_elapsed    | 18793  |\n",
      "|    total_timesteps | 878592 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-0.92 +/- 0.16\n",
      "Episode length: 1.80 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 880000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08997982 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.0715     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 9180       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 430    |\n",
      "|    time_elapsed    | 18839  |\n",
      "|    total_timesteps | 880640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=-0.20 +/- 1.55\n",
      "Episode length: 9.00 +/- 15.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 882000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100005165 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | -0.212      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0546     |\n",
      "|    n_updates            | 9190        |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 431    |\n",
      "|    time_elapsed    | 18885  |\n",
      "|    total_timesteps | 882688 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 884000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10269588 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.955     |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 9200       |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 432    |\n",
      "|    time_elapsed    | 18938  |\n",
      "|    total_timesteps | 884736 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.2        |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 886000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07872599 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.975     |\n",
      "|    explained_variance   | 0.0637     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 9210       |\n",
      "|    policy_gradient_loss | -0.0383    |\n",
      "|    value_loss           | 0.234      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 433    |\n",
      "|    time_elapsed    | 18987  |\n",
      "|    total_timesteps | 886784 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 6.80 +/- 10.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 888000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08917995 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.884     |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 9220       |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.395      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 434    |\n",
      "|    time_elapsed    | 19035  |\n",
      "|    total_timesteps | 888832 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 890000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09032707 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.98      |\n",
      "|    explained_variance   | -0.152     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0349    |\n",
      "|    n_updates            | 9230       |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 435    |\n",
      "|    time_elapsed    | 19079  |\n",
      "|    total_timesteps | 890880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=-0.90 +/- 0.11\n",
      "Episode length: 2.00 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 892000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06168007 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.841     |\n",
      "|    explained_variance   | 0.0556     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 9240       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    value_loss           | 0.36       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 436    |\n",
      "|    time_elapsed    | 19123  |\n",
      "|    total_timesteps | 892928 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=-0.52 +/- 0.96\n",
      "Episode length: 5.80 +/- 9.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 894000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09661479 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.781     |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00489    |\n",
      "|    n_updates            | 9250       |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 437    |\n",
      "|    time_elapsed    | 19168  |\n",
      "|    total_timesteps | 894976 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=0.08 +/- 1.20\n",
      "Episode length: 11.80 +/- 11.96\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 11.8      |\n",
      "|    mean_reward          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 896000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0978144 |\n",
      "|    clip_fraction        | 0.304     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.826    |\n",
      "|    explained_variance   | 0.177     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0388   |\n",
      "|    n_updates            | 9260      |\n",
      "|    policy_gradient_loss | -0.0503   |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 438    |\n",
      "|    time_elapsed    | 19215  |\n",
      "|    total_timesteps | 897024 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=-0.20 +/- 1.50\n",
      "Episode length: 9.00 +/- 15.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 898000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08176336 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.836     |\n",
      "|    explained_variance   | 0.142      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00948    |\n",
      "|    n_updates            | 9270       |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    value_loss           | 0.363      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 439    |\n",
      "|    time_elapsed    | 19260  |\n",
      "|    total_timesteps | 899072 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-0.92 +/- 0.10\n",
      "Episode length: 1.80 +/- 0.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 900000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09369579 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.826     |\n",
      "|    explained_variance   | 0.016      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 9280       |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 440    |\n",
      "|    time_elapsed    | 19306  |\n",
      "|    total_timesteps | 901120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=0.14 +/- 1.24\n",
      "Episode length: 12.40 +/- 12.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.4       |\n",
      "|    mean_reward          | 0.14       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 902000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09471507 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.84      |\n",
      "|    explained_variance   | 0.0975     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 9290       |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 441    |\n",
      "|    time_elapsed    | 19352  |\n",
      "|    total_timesteps | 903168 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=-0.58 +/- 0.39\n",
      "Episode length: 5.20 +/- 3.92\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5.2       |\n",
      "|    mean_reward          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 904000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1020544 |\n",
      "|    clip_fraction        | 0.337     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.901    |\n",
      "|    explained_variance   | 0.28      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0226   |\n",
      "|    n_updates            | 9300      |\n",
      "|    policy_gradient_loss | -0.0489   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 442    |\n",
      "|    time_elapsed    | 19397  |\n",
      "|    total_timesteps | 905216 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=-0.14 +/- 1.11\n",
      "Episode length: 9.60 +/- 11.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.6        |\n",
      "|    mean_reward          | -0.14      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 906000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10268345 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.852     |\n",
      "|    explained_variance   | 0.226      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0531    |\n",
      "|    n_updates            | 9310       |\n",
      "|    policy_gradient_loss | -0.0538    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 443    |\n",
      "|    time_elapsed    | 19443  |\n",
      "|    total_timesteps | 907264 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=-0.88 +/- 0.15\n",
      "Episode length: 2.20 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 908000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11410025 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.794     |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00431   |\n",
      "|    n_updates            | 9320       |\n",
      "|    policy_gradient_loss | -0.0504    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 444    |\n",
      "|    time_elapsed    | 19487  |\n",
      "|    total_timesteps | 909312 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-0.74 +/- 0.24\n",
      "Episode length: 3.60 +/- 2.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 910000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08760586 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.804     |\n",
      "|    explained_variance   | 0.049      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 9330       |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.413      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 445    |\n",
      "|    time_elapsed    | 19530  |\n",
      "|    total_timesteps | 911360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.2         |\n",
      "|    mean_reward          | -0.98       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 912000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065601334 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.71       |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00398     |\n",
      "|    n_updates            | 9340        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.431       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 446    |\n",
      "|    time_elapsed    | 19574  |\n",
      "|    total_timesteps | 913408 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=0.64 +/- 1.31\n",
      "Episode length: 17.40 +/- 13.09\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.4       |\n",
      "|    mean_reward          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 914000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08481711 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.777     |\n",
      "|    explained_variance   | -0.167     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 9350       |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.326      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 447    |\n",
      "|    time_elapsed    | 19620  |\n",
      "|    total_timesteps | 915456 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 916000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09371312 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.811     |\n",
      "|    explained_variance   | 0.174      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 9360       |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    value_loss           | 0.241      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 448    |\n",
      "|    time_elapsed    | 19664  |\n",
      "|    total_timesteps | 917504 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=-0.88 +/- 0.07\n",
      "Episode length: 2.20 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 918000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07831049 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.811     |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 9370       |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    value_loss           | 0.32       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 449    |\n",
      "|    time_elapsed    | 19708  |\n",
      "|    total_timesteps | 919552 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=0.02 +/- 1.46\n",
      "Episode length: 11.20 +/- 14.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.2       |\n",
      "|    mean_reward          | 0.02       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 920000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10221057 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.782     |\n",
      "|    explained_variance   | 0.00259    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 9380       |\n",
      "|    policy_gradient_loss | -0.0355    |\n",
      "|    value_loss           | 0.391      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 450    |\n",
      "|    time_elapsed    | 19752  |\n",
      "|    total_timesteps | 921600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=-0.80 +/- 0.17\n",
      "Episode length: 3.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 922000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062718205 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.781      |\n",
      "|    explained_variance   | 0.056       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00929     |\n",
      "|    n_updates            | 9390        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.573       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 451    |\n",
      "|    time_elapsed    | 19795  |\n",
      "|    total_timesteps | 923648 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-0.74 +/- 0.39\n",
      "Episode length: 3.60 +/- 3.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 924000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10742442 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.778     |\n",
      "|    explained_variance   | 0.000707   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00547    |\n",
      "|    n_updates            | 9400       |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 452    |\n",
      "|    time_elapsed    | 19839  |\n",
      "|    total_timesteps | 925696 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=-0.72 +/- 0.56\n",
      "Episode length: 3.80 +/- 5.60\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.8       |\n",
      "|    mean_reward          | -0.72     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 926000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0967772 |\n",
      "|    clip_fraction        | 0.297     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.779    |\n",
      "|    explained_variance   | 0.0734    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00551  |\n",
      "|    n_updates            | 9410      |\n",
      "|    policy_gradient_loss | -0.0437   |\n",
      "|    value_loss           | 0.292     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 453    |\n",
      "|    time_elapsed    | 19884  |\n",
      "|    total_timesteps | 927744 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=-0.84 +/- 0.19\n",
      "Episode length: 2.60 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 928000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09184135 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.813     |\n",
      "|    explained_variance   | 0.081      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00211    |\n",
      "|    n_updates            | 9420       |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    value_loss           | 0.303      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 454    |\n",
      "|    time_elapsed    | 19928  |\n",
      "|    total_timesteps | 929792 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-0.68 +/- 0.42\n",
      "Episode length: 4.20 +/- 4.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | -0.68       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100290895 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | -0.187      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00555    |\n",
      "|    n_updates            | 9430        |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 455    |\n",
      "|    time_elapsed    | 19972  |\n",
      "|    total_timesteps | 931840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=-0.92 +/- 0.04\n",
      "Episode length: 1.80 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 932000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08650981 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.78      |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0215     |\n",
      "|    n_updates            | 9440       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.389      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 456    |\n",
      "|    time_elapsed    | 20015  |\n",
      "|    total_timesteps | 933888 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -1          |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 934000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086342424 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | -0.264      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 9450        |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 457    |\n",
      "|    time_elapsed    | 20058  |\n",
      "|    total_timesteps | 935936 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 936000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08330257 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.758     |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0232    |\n",
      "|    n_updates            | 9460       |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.208      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 458    |\n",
      "|    time_elapsed    | 20106  |\n",
      "|    total_timesteps | 937984 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=-0.66 +/- 0.63\n",
      "Episode length: 4.40 +/- 6.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | -0.66      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 938000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09525664 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.817     |\n",
      "|    explained_variance   | -0.0404    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 9470       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-0.66 +/- 0.30\n",
      "Episode length: 4.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | -0.66    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 940000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 459    |\n",
      "|    time_elapsed    | 20156  |\n",
      "|    total_timesteps | 940032 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=-0.66 +/- 0.68\n",
      "Episode length: 4.40 +/- 6.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | -0.66      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 942000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10613586 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.844     |\n",
      "|    explained_variance   | -0.0808    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 9480       |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 460    |\n",
      "|    time_elapsed    | 20198  |\n",
      "|    total_timesteps | 942080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=-0.86 +/- 0.15\n",
      "Episode length: 2.40 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 944000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08236966 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.757     |\n",
      "|    explained_variance   | -0.0471    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 9490       |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 461    |\n",
      "|    time_elapsed    | 20242  |\n",
      "|    total_timesteps | 944128 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=-0.76 +/- 0.34\n",
      "Episode length: 3.40 +/- 3.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 946000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09614109 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.753     |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00624   |\n",
      "|    n_updates            | 9500       |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.31       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 462    |\n",
      "|    time_elapsed    | 20288  |\n",
      "|    total_timesteps | 946176 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=-0.96 +/- 0.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.4        |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 948000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08557686 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.831     |\n",
      "|    explained_variance   | 0.131      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 9510       |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.294      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 463    |\n",
      "|    time_elapsed    | 20334  |\n",
      "|    total_timesteps | 948224 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-0.52 +/- 0.45\n",
      "Episode length: 5.80 +/- 4.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | -0.52       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 950000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095192134 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 0.0172      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 9520        |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.294       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 464    |\n",
      "|    time_elapsed    | 20376  |\n",
      "|    total_timesteps | 950272 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=-0.62 +/- 0.42\n",
      "Episode length: 4.80 +/- 4.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 952000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07778442 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.732     |\n",
      "|    explained_variance   | 0.148      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.717      |\n",
      "|    n_updates            | 9530       |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    value_loss           | 0.488      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 465    |\n",
      "|    time_elapsed    | 20422  |\n",
      "|    total_timesteps | 952320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=-0.92 +/- 0.16\n",
      "Episode length: 1.80 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.8        |\n",
      "|    mean_reward          | -0.92      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 954000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09444274 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.853     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 9540       |\n",
      "|    policy_gradient_loss | -0.0488    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 466    |\n",
      "|    time_elapsed    | 20465  |\n",
      "|    total_timesteps | 954368 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=-0.92 +/- 0.12\n",
      "Episode length: 1.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.8         |\n",
      "|    mean_reward          | -0.92       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 956000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098106325 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | 0.0721      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 9550        |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.21        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 467    |\n",
      "|    time_elapsed    | 20510  |\n",
      "|    total_timesteps | 956416 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=-0.88 +/- 0.19\n",
      "Episode length: 2.20 +/- 1.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 958000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06063608 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.83      |\n",
      "|    explained_variance   | -0.0619    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 9560       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    value_loss           | 0.433      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 468    |\n",
      "|    time_elapsed    | 20554  |\n",
      "|    total_timesteps | 958464 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-0.74 +/- 0.52\n",
      "Episode length: 3.60 +/- 5.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | -0.74      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 960000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06885194 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.807     |\n",
      "|    explained_variance   | 0.0688     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00447   |\n",
      "|    n_updates            | 9570       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    value_loss           | 0.323      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 469    |\n",
      "|    time_elapsed    | 20598  |\n",
      "|    total_timesteps | 960512 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=-0.86 +/- 0.28\n",
      "Episode length: 2.40 +/- 2.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.4        |\n",
      "|    mean_reward          | -0.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 962000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10786488 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.843     |\n",
      "|    explained_variance   | -0.134     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 9580       |\n",
      "|    policy_gradient_loss | -0.0544    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 470    |\n",
      "|    time_elapsed    | 20644  |\n",
      "|    total_timesteps | 962560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=0.04 +/- 1.27\n",
      "Episode length: 11.40 +/- 12.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.4        |\n",
      "|    mean_reward          | 0.04        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 964000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108331494 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.847      |\n",
      "|    explained_variance   | -0.0877     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 9590        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 0.288       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 471    |\n",
      "|    time_elapsed    | 20691  |\n",
      "|    total_timesteps | 964608 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-0.56 +/- 0.64\n",
      "Episode length: 5.40 +/- 6.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 966000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08199237 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.777     |\n",
      "|    explained_variance   | 0.0193     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 9600       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    value_loss           | 0.371      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 472    |\n",
      "|    time_elapsed    | 20734  |\n",
      "|    total_timesteps | 966656 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=-0.10 +/- 1.19\n",
      "Episode length: 10.00 +/- 11.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10         |\n",
      "|    mean_reward          | -0.1       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 968000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07381205 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.709     |\n",
      "|    explained_variance   | -0.0543    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00577   |\n",
      "|    n_updates            | 9610       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    value_loss           | 0.542      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 473    |\n",
      "|    time_elapsed    | 20779  |\n",
      "|    total_timesteps | 968704 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-0.42 +/- 0.82\n",
      "Episode length: 6.80 +/- 8.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | -0.42       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 970000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068945035 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | -0.0124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 9620        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.448       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 474    |\n",
      "|    time_elapsed    | 20823  |\n",
      "|    total_timesteps | 970752 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-0.94 +/- 0.08\n",
      "Episode length: 1.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 972000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09639258 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.697     |\n",
      "|    explained_variance   | 0.0409     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0224    |\n",
      "|    n_updates            | 9630       |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    value_loss           | 0.259      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 475    |\n",
      "|    time_elapsed    | 20866  |\n",
      "|    total_timesteps | 972800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=-0.76 +/- 0.20\n",
      "Episode length: 3.40 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 974000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07775389 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.753     |\n",
      "|    explained_variance   | 0.583      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 9640       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.326      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 476    |\n",
      "|    time_elapsed    | 20910  |\n",
      "|    total_timesteps | 974848 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=0.10 +/- 1.63\n",
      "Episode length: 12.00 +/- 16.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12         |\n",
      "|    mean_reward          | 0.1        |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 976000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08424111 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.756     |\n",
      "|    explained_variance   | -0.099     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0159    |\n",
      "|    n_updates            | 9650       |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 0.461      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 477    |\n",
      "|    time_elapsed    | 20958  |\n",
      "|    total_timesteps | 976896 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-1.86 +/- 2.29\n",
      "Episode length: 10.40 +/- 13.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.4       |\n",
      "|    mean_reward          | -1.86      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 978000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07486118 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.84      |\n",
      "|    explained_variance   | -0.145     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.49       |\n",
      "|    n_updates            | 9660       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.546      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 478    |\n",
      "|    time_elapsed    | 21001  |\n",
      "|    total_timesteps | 978944 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=0.06 +/- 1.41\n",
      "Episode length: 11.60 +/- 14.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090258375 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.0661      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000874    |\n",
      "|    n_updates            | 9670        |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.354       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 479    |\n",
      "|    time_elapsed    | 21047  |\n",
      "|    total_timesteps | 980992 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=0.60 +/- 1.89\n",
      "Episode length: 17.00 +/- 18.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 982000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095082596 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.818      |\n",
      "|    explained_variance   | 0.0408      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.027       |\n",
      "|    n_updates            | 9680        |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.249       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 480    |\n",
      "|    time_elapsed    | 21094  |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-0.88 +/- 0.16\n",
      "Episode length: 2.20 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.2        |\n",
      "|    mean_reward          | -0.88      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 984000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08527358 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.755     |\n",
      "|    explained_variance   | 0.0175     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 9690       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.284      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 481    |\n",
      "|    time_elapsed    | 21136  |\n",
      "|    total_timesteps | 985088 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=-0.94 +/- 0.12\n",
      "Episode length: 1.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6        |\n",
      "|    mean_reward          | -0.94      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 986000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11377078 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.765     |\n",
      "|    explained_variance   | 0.129      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0186    |\n",
      "|    n_updates            | 9700       |\n",
      "|    policy_gradient_loss | -0.0506    |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 482    |\n",
      "|    time_elapsed    | 21178  |\n",
      "|    total_timesteps | 987136 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=-0.30 +/- 1.20\n",
      "Episode length: 8.00 +/- 12.03\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8          |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 988000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08829144 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 0.273      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 9710       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 483    |\n",
      "|    time_elapsed    | 21223  |\n",
      "|    total_timesteps | 989184 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-0.76 +/- 0.29\n",
      "Episode length: 3.40 +/- 2.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -0.76      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 990000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06268541 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.842     |\n",
      "|    explained_variance   | -0.0795    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0389     |\n",
      "|    n_updates            | 9720       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    value_loss           | 0.608      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 484    |\n",
      "|    time_elapsed    | 21265  |\n",
      "|    total_timesteps | 991232 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=-0.84 +/- 0.32\n",
      "Episode length: 2.60 +/- 3.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 992000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07223201 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.767     |\n",
      "|    explained_variance   | 0.0809     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.032      |\n",
      "|    n_updates            | 9730       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.628      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 485    |\n",
      "|    time_elapsed    | 21314  |\n",
      "|    total_timesteps | 993280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=-0.80 +/- 0.31\n",
      "Episode length: 3.00 +/- 3.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | -0.8       |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 994000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08846004 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.836     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00374   |\n",
      "|    n_updates            | 9740       |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 486    |\n",
      "|    time_elapsed    | 21364  |\n",
      "|    total_timesteps | 995328 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-0.68 +/- 0.59\n",
      "Episode length: 4.20 +/- 5.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | -0.68       |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 996000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088879704 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.756      |\n",
      "|    explained_variance   | -0.00435    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00878     |\n",
      "|    n_updates            | 9750        |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.381       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 487    |\n",
      "|    time_elapsed    | 21417  |\n",
      "|    total_timesteps | 997376 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=-0.98 +/- 0.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.2       |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 998000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1078694 |\n",
      "|    clip_fraction        | 0.29      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.774    |\n",
      "|    explained_variance   | -0.0877   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0029    |\n",
      "|    n_updates            | 9760      |\n",
      "|    policy_gradient_loss | -0.0405   |\n",
      "|    value_loss           | 0.297     |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 46     |\n",
      "|    iterations      | 488    |\n",
      "|    time_elapsed    | 21468  |\n",
      "|    total_timesteps | 999424 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-0.28 +/- 0.66\n",
      "Episode length: 8.20 +/- 6.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.2        |\n",
      "|    mean_reward          | -0.28      |\n",
      "| time/                   |            |\n",
      "|    total timesteps      | 1000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08244377 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.714     |\n",
      "|    explained_variance   | -0.121     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0193     |\n",
      "|    n_updates            | 9770       |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    value_loss           | 0.317      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 46      |\n",
      "|    iterations      | 489     |\n",
      "|    time_elapsed    | 21512   |\n",
      "|    total_timesteps | 1001472 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x158f2c850>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345e24c",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d2217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Path = os.path.join('Training', 'SavedModels', 'PPO_Racing_Kings_1m_win+step_reward_random_start_after_reset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8b0db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(Model_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c82d6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9cf2199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(Model_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3960df2",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa29ae",
   "metadata": {},
   "source": [
    "# Storing the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_weights/100k_dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_player(board):\n",
    "    move = random.choice(list(board.legal_moves))\n",
    "    return move.uci()\n",
    "for i in range(10):\n",
    "    print(random_player(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(random_player, random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b402fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {None: 0, True: 0, False: 0}\n",
    "for i in range(10):\n",
    "    result, msg, board = play_game(random_player, random_player, visual=None)\n",
    "    counts[result] += 1\n",
    "    print(counts)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858bf7f0",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a1470e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:2 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:3 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:4 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:5 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:6 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:7 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:8 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:9 Score:-1 Info:{'msg': 'Action is not a valid move'}\n",
      "Episode:10 Score:-1 Info:{'msg': 'Action is not a valid move'}\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render(mode=None)\n",
    "        action = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Info:{}'.format(episode, score, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a3ac2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Move 31 Black:</b><br/><svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><defs><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light lastmove g2\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light lastmove b7\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-queen\" transform=\"translate(330, 330)\" /><use xlink:href=\"#black-king\" transform=\"translate(15, 285)\" /><use xlink:href=\"#black-bishop\" transform=\"translate(105, 285)\" /><use xlink:href=\"#white-king\" transform=\"translate(330, 285)\" /><use xlink:href=\"#black-rook\" transform=\"translate(150, 240)\" /><use xlink:href=\"#white-rook\" transform=\"translate(195, 240)\" /><use xlink:href=\"#black-knight\" transform=\"translate(330, 240)\" /><use xlink:href=\"#black-rook\" transform=\"translate(240, 150)\" /><use xlink:href=\"#white-rook\" transform=\"translate(60, 105)\" /><use xlink:href=\"#black-bishop\" transform=\"translate(330, 105)\" /><use xlink:href=\"#white-bishop\" transform=\"translate(15, 60)\" /><use xlink:href=\"#white-bishop\" transform=\"translate(60, 60)\" /><use xlink:href=\"#white-knight\" transform=\"translate(285, 60)\" /></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-0.5499999932944775, 0.854692940049581)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c07dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "636cf5762cc7d752356f6c503ef4faeef5e3c22281f38effb63ed56f12463915"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
